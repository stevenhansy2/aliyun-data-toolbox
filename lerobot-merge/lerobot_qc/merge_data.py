import os
import shutil
import json
from util import *
from pathlib import Path
import subprocess
from datetime import datetime
from uuid import uuid4

PARAMETERS = "parameters"
META = "meta"
MASK = "mask/chunk-000"


def deepest_dirs(root, absolute=False):
    root = os.path.abspath(root)
    deepest = []
    for dirpath, dirnames, _ in os.walk(root, topdown=False):
        if not dirnames:
            if absolute:
                deepest.append(dirpath)
            else:
                rel = os.path.relpath(dirpath, root)
                deepest.append(rel)
    return [
        dirname for dirname in deepest
        if dirname not in [PARAMETERS, META] and MASK + "/episode" not in dirname
    ]


class LeRobotMetadata:
    def __init__(self, root: str | Path | None = None):
        self.root = Path(root)
        self.load_metadata()

    def load_metadata(self):
        self.info = load_info(self.root)
        self.tasks, self.task_to_task_index = load_tasks(self.root)
        self.episodes = load_episodes(self.root)
        self.episodes_stats = load_episodes_stats(self.root)

    def get_info(self):
        return self.info

    def get_tasks(self):
        return self.tasks

    def get_episodes(self):
        return self.episodes

    def get_episodes_stats(self):
        return self.episodes_stats


def _human_readable_size(size_bytes):
    if size_bytes == 0:
        return "0 B"
    units = ["B", "KB", "MB", "GB", "TB"]
    i = 0
    while size_bytes >= 1024 and i < len(units) - 1:
        size_bytes /= 1024.0
        i += 1
    return f"{size_bytes:.2f} {units[i]}"


def validate_episode_structure(root: Path) -> None:
    """
    éªŒè¯ root ç›®å½•æ˜¯å¦ç¬¦åˆæ ‡å‡† LeRobot episode ç»“æ„ã€‚
    è‹¥ä¸ç¬¦åˆï¼ŒæŠ›å‡º ValueErrorã€‚
    """
    # 1. å¿…é¡»å­˜åœ¨ meta/
    meta_dir = root / "meta"
    if not meta_dir.exists() or not meta_dir.is_dir():
        raise ValueError("ç¼ºå¤± meta/ ç›®å½•")


    required_meta_files = ["info.json", "episodes.jsonl", "episodes_stats.jsonl", "tasks.jsonl"]
    missing_meta = [f for f in required_meta_files if not (meta_dir / f).exists()]
    if missing_meta:
        raise ValueError(f"meta/ ç›®å½•ç¼ºå¤±å¿…è¦æ–‡ä»¶: {missing_meta}")

    # æ£€æŸ¥ jsonl æ–‡ä»¶æ˜¯å¦ä¸ºç©º
    jsonl_files = ["episodes.jsonl", "episodes_stats.jsonl", "tasks.jsonl"]
    empty_jsonl = []
    for jf in jsonl_files:
        file_path = meta_dir / jf
        if file_path.exists():
            try:
                if file_path.stat().st_size == 0:
                    empty_jsonl.append(jf)
                else:
                    # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦å†…å®¹å…¨ä¸ºç©ºè¡Œ
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = [line.strip() for line in f.readlines()]
                        if not any(lines):
                            empty_jsonl.append(jf)
            except Exception as e:
                raise ValueError(f"æ£€æŸ¥ {jf} æ˜¯å¦ä¸ºç©ºæ—¶å‡ºé”™: {e}")
    if empty_jsonl:
        raise ValueError(f"meta/ ç›®å½•ä¸‹å­˜åœ¨ç©ºçš„ jsonl æ–‡ä»¶: {empty_jsonl}")

    # 2. å¿…é¡»å­˜åœ¨ data/chunk-000/ ä¸”åŒ…å«è‡³å°‘ä¸€ä¸ª episode_*.parquet
    data_chunk = root / "data" / "chunk-000"
    if not data_chunk.exists() or not data_chunk.is_dir():
        raise ValueError("ç¼ºå¤± data/chunk-000/ ç›®å½•")
    parquet_files = list(data_chunk.glob("episode_*.parquet"))
    if not parquet_files:
        raise ValueError("data/chunk-000/ ä¸­æ—  episode_*.parquet æ–‡ä»¶")

    # 3. å¿…é¡»å­˜åœ¨ videos/chunk-000/observation.images.camera_*/*.mp4
    videos_base = root / "videos" / "chunk-000"
    expected_cameras = [
        "observation.images.camera_top",
        "observation.images.camera_wrist_left",
        "observation.images.camera_wrist_right"
    ]
    for cam in expected_cameras:
        cam_dir = videos_base / cam
        if not cam_dir.exists() or not cam_dir.is_dir():
            raise ValueError(f"ç¼ºå¤±è§†é¢‘ç›®å½•: {cam_dir}")
        mp4_files = list(cam_dir.glob("episode_*.mp4"))
        if not mp4_files:
            raise ValueError(f"è§†é¢‘ç›®å½• {cam_dir} ä¸­æ—  episode_*.mp4 æ–‡ä»¶")

    # 4. parameters/ å¿…é¡»åŒ…å«æ‰€æœ‰ç›¸æœºå‚æ•°æ–‡ä»¶
    param_dir = root / "parameters"
    if not param_dir.exists() or not param_dir.is_dir():
        raise ValueError("ç¼ºå¤± parameters/ ç›®å½•")
    required_param_files = [
        "camera_top_extrinsic.json",
        "camera_top_intrinsic.json",
        "camera_wrist_left_extrinsic.json",
        "camera_wrist_left_intrinsic.json",
        "camera_wrist_right_extrinsic.json",
        "camera_wrist_right_intrinsic.json"
    ]
    missing_params = [f for f in required_param_files if not (param_dir / f).exists()]
    if missing_params:
        raise ValueError(f"parameters/ ç¼ºå¤±å¿…è¦æ–‡ä»¶: {missing_params}")

    #print(f"[validate] âœ… ç»“æ„éªŒè¯é€šè¿‡: {root}")


def merge_meta(src_path, tgt_path, summary_output_dir=None):
    src_path = Path(src_path)
    if not src_path.exists():
        raise ValueError(f"æºç›®å½•ä¸å­˜åœ¨: {src_path}")

    # è·å–æ‰€æœ‰å­ç›®å½•ï¼ˆä»…ç›®å½•ï¼‰
    srcs = [p for p in src_path.iterdir() if p.is_dir()]
    #print(f"[merge_meta] éå†æºç›®å½•: {src_path}, å‘ç° {len(srcs)} ä¸ªå­ç›®å½•")

    new_info = None
    total_size_bytes = 0
    success_count = 0
    skipped_dirs = []
    merged_metadata = {}  # ç”¨äºåˆå¹¶ metadata.json

    if summary_output_dir is None:
        summary_output_dir = tgt_path
    summary_output_dir = Path(summary_output_dir)
    summary_output_dir.mkdir(parents=True, exist_ok=True)

    tgt_path = Path(tgt_path)

    # æŒ‰åç§°æ’åºï¼Œç¡®ä¿é¡ºåºä¸€è‡´
    for local_path in sorted(srcs):
        try:
            #print(f"[merge_meta] å¤„ç†å­ç›®å½•: {local_path}")

            # === ã€å¢å¼ºã€‘ç»“æ„å®Œæ•´æ€§æ£€æŸ¥ ===
            validate_episode_structure(local_path)

            # === ã€æ–°å¢ã€‘è·³è¿‡æ€»å¸§æ•° < 30 çš„æ•°æ®é›† ===
            info_json_path = local_path / "meta" / "info.json"
            with open(info_json_path, 'r', encoding='utf-8') as f:
                tmp_info_early = json.load(f)

            if tmp_info_early.get("total_frames", 0) < 95:
                #print(f"[merge_meta] è·³è¿‡ {local_path}ï¼šæ€»å¸§æ•° {tmp_info_early['total_frames']} < 95")
                skipped_dirs.append((str(local_path), "total_frames < 95"))
                continue

            # è®¡ç®—å¤§å°
            size_bytes = sum(f.stat().st_size for f in local_path.rglob('*') if f.is_file())
            total_size_bytes += size_bytes

            # åŠ è½½å…ƒæ•°æ®ï¼ˆæ­¤æ—¶å¯å®‰å…¨è°ƒç”¨ï¼‰
            metas = LeRobotMetadata(local_path)
            tmp_info = metas.get_info()
            tmp_episodes = metas.get_episodes()
            tmp_episodes_stats = metas.get_episodes_stats()

            #print(f"[merge_meta] è¯»å– meta æˆåŠŸ: {local_path}")

            if success_count == 0:
                # åˆå§‹åŒ– new_infoï¼ˆä»…ç¬¬ä¸€ä¸ªæˆåŠŸå­é›†ï¼‰
                new_info = tmp_info.copy()
                new_tasks = metas.get_tasks()
                #print(f"[merge_meta] åˆå§‹åŒ–å…ƒæ•°æ®ï¼Œå†™å…¥ idx=0 åˆ° {tgt_path}")
                append_ep_idx(tmp_episodes, 0, tgt_path)
                append_ep_sts_idx(tmp_episodes_stats, 0, tgt_path)
                append_tasks_idx(new_tasks, tgt_path)

                # æ£€æŸ¥å¹¶åˆå¹¶ metadata.json
                metadata_path = local_path / "metadata.json"
                if metadata_path.exists():
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        # æ›´æ–° episode_id ä¸ºæ•°å­—ç´¢å¼•
                        metadata['episode_id'] = 0
                        merged_metadata["0"] = metadata
                    except Exception as e:
                        print(f"[WARNING] è¯»å– metadata.json å¤±è´¥ ({local_path}): {e}")

                dirs = deepest_dirs(local_path)
                #print(f"[merge_meta] æœ€æ·±å±‚æ•°æ®ç›®å½•: {dirs}")
                if (local_path / PARAMETERS).exists():
                    #print(f"[merge_meta] æ‹·è´ PARAMETERS: {local_path / PARAMETERS} -> {tgt_path / PARAMETERS}")
                    mv_data(local_path / PARAMETERS, tgt_path / PARAMETERS)
                if (local_path / MASK).exists():
                    #print(f"[merge_meta] æ‹·è´ MASK: {local_path / MASK} -> {tgt_path / MASK}")
                    mv_data_dir_idx(local_path / MASK, tgt_path / MASK, 0)
                for path in dirs:
                    #print(f"[merge_meta] æ‹·è´æ•°æ®ç›®å½•: {local_path / path} -> {tgt_path / path}")
                    mv_data_idx(local_path / path, tgt_path / path, 0)
            else:
                assert new_info is not None, "new_info æœªåˆå§‹åŒ–"
                ep_idx_delta = new_info['total_episodes']
                #print(f"[merge_meta] åˆå¹¶ç¬¬ {success_count} ä¸ªå­é›†ï¼Œep_idx_delta={ep_idx_delta}")
                append_ep_idx(tmp_episodes, ep_idx_delta, tgt_path)
                append_ep_sts_idx(tmp_episodes_stats, ep_idx_delta, tgt_path)

                # æ£€æŸ¥å¹¶åˆå¹¶ metadata.json
                metadata_path = local_path / "metadata.json"
                if metadata_path.exists():
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        # æ›´æ–° episode_id ä¸ºæ•°å­—ç´¢å¼•
                        metadata['episode_id'] = ep_idx_delta
                        merged_metadata[str(ep_idx_delta)] = metadata
                    except Exception as e:
                        print(f"[WARNING] è¯»å– metadata.json å¤±è´¥ ({local_path}): {e}")

                # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
                new_info['total_episodes'] += tmp_info['total_episodes']
                new_info['total_frames'] += tmp_info['total_frames']
                new_info['total_videos'] += tmp_info['total_videos']

                if (local_path / MASK).exists():
                    #print(f"[merge_meta] æ‹·è´ MASK: {local_path / MASK} -> {tgt_path / MASK}")
                    mv_data_dir_idx(local_path / MASK, tgt_path / MASK, success_count)
                dirs = deepest_dirs(local_path)
                for path in dirs:
                    #print(f"[merge_meta] æ‹·è´æ•°æ®ç›®å½•: {local_path / path} -> {tgt_path / path}")
                    mv_data_idx(local_path / path, tgt_path / path, ep_idx_delta)

            success_count += 1

        except Exception as e:
            error_msg = str(e)
            #print(f"[ERROR] è·³è¿‡å­ç›®å½• {local_path}: {error_msg}")
            skipped_dirs.append((str(local_path), error_msg))
            continue

    # === æ ¡éªŒæ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæˆåŠŸ ===
    if new_info is None:
        raise RuntimeError("âŒ æ‰€æœ‰å­ç›®å½•å‡åˆå¹¶å¤±è´¥ï¼æ— æ³•ç”Ÿæˆæœ‰æ•ˆ LeRobot æ•°æ®é›†ã€‚")

    # === è‡ªåŠ¨è®¡ç®— total_chunks å’Œ chunks_size ===
    import math
    chunk_size = 1000
    total_episodes = new_info.get("total_episodes", 0)
    total_chunks = math.ceil(total_episodes / chunk_size) if total_episodes > 0 else 1
    new_info["chunks_size"] = chunk_size
    new_info["total_chunks"] = total_chunks

    # å†™å…¥æœ€ç»ˆ info.json
    write_info(new_info, tgt_path)

    # å†™å…¥åˆå¹¶åçš„ metadata.jsonï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if merged_metadata:
        metadata_output_path = tgt_path / "metadata.json"
        with open(metadata_output_path, 'w', encoding='utf-8') as f:
            json.dump(merged_metadata, f, indent=4, ensure_ascii=False)
        #print(f"[merge_meta] âœ… å·²åˆå¹¶å¹¶å†™å…¥ metadata.json: {metadata_output_path}")

    # === è®¡ç®—æ€»æ—¶é•¿ ===
    if 'fps' in new_info and new_info['fps'] is not None and new_info['fps'] > 0:
        total_duration_sec_val = new_info['total_frames'] / new_info['fps']
        total_duration_sec_str = f"{total_duration_sec_val:.3f} sec"
        total_duration_hour_str = f"{total_duration_sec_val / 3600:.3f} hour"
    else:
        total_duration_sec_str = None
        total_duration_hour_str = None
        #print(f"[è­¦å‘Š] åˆå¹¶åçš„ info.json ç¼ºå°‘æœ‰æ•ˆ 'fps'ï¼ˆå½“å‰å€¼: {new_info.get('fps')}ï¼‰ï¼Œæ— æ³•è®¡ç®—æ€»æ—¶é•¿")

    # === æ„å»ºæœ€ç»ˆ summary ===
    total_size_mb = total_size_bytes / (1024 ** 2)
    total_size_tb = total_size_bytes / (1024 ** 4)

    summary = {
        "input_subdirs_total": len(srcs),
        "successful_merges": success_count,
        "skipped_subdirs_count": len(skipped_dirs),
        # "skipped_details": skipped_dirs,
        "skipped_details": str(src_path),
        "final_total_episodes": new_info["total_episodes"],
        "final_total_frames": new_info["total_frames"],
        "final_total_videos": new_info["total_videos"],
        "total_duration_sec": total_duration_sec_str,
        "total_duration_hour": total_duration_hour_str,
        "total_size_bytes": total_size_bytes,
        "total_size_mb": round(total_size_mb, 2),
        "total_size_tb": round(total_size_tb, 4),
    }

    summary_path = summary_output_dir / "dataset_summary.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    #print(f"\n[merge_meta] âœ… å·²ç”Ÿæˆæ•°æ®é›†æ‘˜è¦: {summary_path}")
    #print(f"[merge_meta] ğŸ“Š æˆåŠŸåˆå¹¶ {success_count} / {len(srcs)} ä¸ªå­ç›®å½•")
    if skipped_dirs:
        #print("[merge_meta] âš ï¸ è·³è¿‡çš„ç›®å½•ï¼ˆå‰5ä¸ªï¼‰:")
        for path, reason in skipped_dirs[:5]:
            print(f"  - {path}: {reason}")
        if len(skipped_dirs) > 5:
            print(f"  ... è¿˜æœ‰ {len(skipped_dirs) - 5} ä¸ªæœªåˆ—å‡º")

    #print(f"[merge_meta] ğŸ”š åˆå¹¶å®Œæˆã€‚")


def parallel_tar_compress(src_dir, out_path):
    src_dir = Path(src_dir)
    out_path = Path(out_path)
    items = [f.name for f in src_dir.iterdir() if f.name != out_path.name]
    if not items:
        raise RuntimeError("æ²¡æœ‰å¯å‹ç¼©çš„å†…å®¹")

    cmd = ["tar", "-cf", str(out_path), "-C", str(src_dir)] + items
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        raise RuntimeError(f"tar æ‰“åŒ…å¤±è´¥: {result.stderr}")


def get_tar(src_dir, out_path, max_tar=2):
    for tar_attempt in range(1, max_tar + 1):
        try:
            parallel_tar_compress(src_dir, out_path)
            #print(f"ğŸ“¦ æ•°æ®é›†å·²æ‰“åŒ…è‡³: {out_path}")
            break
        except Exception as e:
            #print(f"âŒ ç¬¬{tar_attempt}æ¬¡æ‰“åŒ…å¤±è´¥: {e}")
            if os.path.exists(out_path):
                try:
                    os.remove(out_path)
                    print(f"âš ï¸ å·²åˆ é™¤æ®‹ä½™å‹ç¼©åŒ…: {out_path}")
                except Exception as del_e:
                    print(f"âš ï¸ åˆ é™¤æ®‹ä½™å‹ç¼©åŒ…å¤±è´¥: {del_e}")
            if tar_attempt == max_tar:
                print(f"ğŸ’¥ æ‰“åŒ…æœ€ç»ˆå¤±è´¥: {e}")


def reorganize_chunks(tgt_path):
    """
    åªå°†è¶…è¿‡1000çš„éƒ¨åˆ†ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªchunkæ–‡ä»¶å¤¹ï¼Œchunk-000ä¿ç•™å‰1000æ¡ï¼Œchunk-001æ”¾1001~2000æ¡ï¼Œä»¥æ­¤ç±»æ¨ã€‚
    æ–‡ä»¶å‘½åä¸º episode_000000, episode_000001, ...ï¼Œç¼–å·å…¨å±€é€’å¢ã€‚
    videos åŒæ­¥ç§»åŠ¨ï¼Œchunk-000åŠå…¶å­ç›®å½•ä¿ç•™ã€‚
    """
    tgt_path = Path(tgt_path)
    data_dir = tgt_path / "data"
    chunk0_dir = data_dir / "chunk-000"
    if not chunk0_dir.exists():
        print(f"âŒ æœªæ‰¾åˆ° {chunk0_dir}")
        return

    # 1. ç»Ÿè®¡æ‰€æœ‰ episode_*.parquet
    parquet_files = sorted(chunk0_dir.glob("episode_*.parquet"))
    total = len(parquet_files)
    chunk_size = 1000
    num_chunks = (total + chunk_size - 1) // chunk_size

    # 2. åˆ›å»º chunk-xxx æ–‡ä»¶å¤¹ï¼ˆé™¤chunk-000å¤–ï¼‰
    for i in range(1, num_chunks):
        chunk_name = f"chunk-{i:03d}"
        chunk_dir = data_dir / chunk_name
        chunk_dir.mkdir(parents=True, exist_ok=True)

    # 3. åªç§»åŠ¨ chunk-000 ç›®å½•ä¸‹è¶…è¿‡1000çš„ parquet æ–‡ä»¶åˆ°ä¸‹ä¸€ä¸ªchunk
    for idx, file in enumerate(parquet_files):
        if idx < chunk_size:
            # å‰1000æ¡ç•™åœ¨chunk-000
            new_name = f"episode_{idx:06d}.parquet"
            target = chunk0_dir / new_name
            if file.name != new_name:
                shutil.move(str(file), target)
        else:
            chunk_idx = idx // chunk_size
            chunk_dir = data_dir / f"chunk-{chunk_idx:03d}"
            new_name = f"episode_{idx:06d}.parquet"
            target = chunk_dir / new_name
            shutil.move(str(file), target)

    # 4. videos åªç§»åŠ¨ chunk-000/observation.images.camera_* ä¸‹è¶…è¿‡1000çš„ mp4 åˆ°ä¸‹ä¸€ä¸ªchunk
    videos_dir = tgt_path / "videos"
    cam_base_dir = videos_dir / "chunk-000"
    for cam_dir in cam_base_dir.glob("observation.images.camera_*"):
        if not cam_dir.is_dir():
            continue
        mp4_files = sorted(cam_dir.glob("episode_*.mp4"))
        for i in range(1, num_chunks):
            chunk_cam_dir = videos_dir / f"chunk-{i:03d}" / cam_dir.name
            chunk_cam_dir.mkdir(parents=True, exist_ok=True)
        for idx, mp4 in enumerate(mp4_files):
            if idx < chunk_size:
                # å‰1000æ¡ç•™åœ¨chunk-000
                new_name = f"episode_{idx:06d}.mp4"
                target = cam_dir / new_name
                if mp4.name != new_name:
                    shutil.move(str(mp4), target)
            else:
                chunk_idx = idx // chunk_size
                chunk_cam_dir = videos_dir / f"chunk-{chunk_idx:03d}" / cam_dir.name
                new_name = f"episode_{idx:06d}.mp4"
                target = chunk_cam_dir / new_name
                shutil.move(str(mp4), target)

    # 5. ä¸æ¸…ç† chunk-000 åŠå…¶ observation.images.camera_* ç›®å½•ï¼Œåªç§»åŠ¨å¤šä½™éƒ¨åˆ†

    # 6. æ›´æ–° info.json çš„ total_chunks
    info_path = tgt_path / "meta" / "info.json"
    if info_path.exists():
        with open(info_path, "r", encoding="utf-8") as f:
            info = json.load(f)
        info["total_chunks"] = num_chunks
        with open(info_path, "w", encoding="utf-8") as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        print(f"âœ… å·²æ›´æ–° info.json total_chunks={num_chunks}")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="é‡ç»„ testoutput ç›®å½•ä¸‹çš„ LeRobot æ•°æ®é›†")
    parser.add_argument("--src_dir", type=str, required=True, help="å¾…é‡ç»„çš„æºçˆ¶ç›®å½•ï¼ˆåŒ…å«å¤šä¸ªå­æ•°æ®é›†ï¼‰")
    parser.add_argument("--tgt_dir", type=str, required=True, help="åˆå¹¶åçš„ç›®æ ‡ç›®å½•")
    parser.add_argument("--summary_dir", type=str, default=None,
                        help="ç»Ÿè®¡ JSON è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸º tgt_dirï¼‰")
    parser.add_argument("--save", action="store_true",
                        help="æ˜¯å¦ä¿ç•™ tgt_dirï¼ˆè‹¥æœªæŒ‡å®šï¼Œåˆ™åˆå¹¶åè‡ªåŠ¨åˆ é™¤ï¼‰")
    parser.add_argument("--max_tar", default=2, type=int, help="æœ€å¤§é‡è¯•æ‰“åŒ…æ¬¡æ•°ï¼ˆå½“å‰æœªä½¿ç”¨ï¼‰")
    args = parser.parse_args()

    merge_meta(args.src_dir, args.tgt_dir, summary_output_dir=args.summary_dir)

    # æ–°å¢ï¼šè‡ªåŠ¨åˆ†å—
    reorganize_chunks(args.tgt_dir)

    if not args.save:
        if Path(args.tgt_dir).exists():
            print(f"[cleanup] åˆ é™¤ä¸´æ—¶åˆå¹¶ç›®å½•: {args.tgt_dir}")
            shutil.rmtree(args.tgt_dir)


if __name__ == '__main__':
    main()