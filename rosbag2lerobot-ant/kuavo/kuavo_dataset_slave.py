#!/usr/bin/env python3
"""
ä¼˜åŒ–äº†æ•°æ®é›†å¯¹é½é€»è¾‘ã€‚

"""
import numpy as np
import cv2
import rosbag
from pprint import pprint
import os
import glob
from collections import defaultdict
import yaml
import time
from std_msgs.msg import Float64MultiArray
from config_dataset_slave import Config, load_config_from_json
from kuavo_pose_calculator import (
    extract_camera_poses_from_bag,
    extract_camera_poses_from_bag_with_time,
)


# ================ æœºå™¨äººå…³èŠ‚ä¿¡æ¯å®šä¹‰ ================
class TimestampStuckError(Exception):
    """æ—¶é—´æˆ³å¡é¡¿å¼‚å¸¸"""

    def __init__(
        self,
        message,
        topic=None,
        stuck_timestamp=None,
        stuck_duration=None,
        stuck_frame_count=None,
        threshold=None,
    ):
        super().__init__(message)
        self.topic = topic
        self.stuck_timestamp = stuck_timestamp
        self.stuck_duration = stuck_duration
        self.stuck_frame_count = stuck_frame_count
        self.threshold = threshold

    def __str__(self):
        return f"TimestampStuckError: {super().__str__()}"


USE_JOINT_CURRENT_STATE = True

DEFAULT_LEG_JOINT_NAMES = [
    "l_leg_roll",
    "l_leg_yaw",
    "l_leg_pitch",
    "l_knee",
    "l_foot_pitch",
    "l_foot_roll",
    "r_leg_roll",
    "r_leg_yaw",
    "r_leg_pitch",
    "r_knee",
    "r_foot_pitch",
    "r_foot_roll",
]
DEFAULT_ARM_JOINT_NAMES = [
    "zarm_l1_link",
    "zarm_l2_link",
    "zarm_l3_link",
    "zarm_l4_link",
    "zarm_l5_link",
    "zarm_l6_link",
    "zarm_l7_link",
    "zarm_r1_link",
    "zarm_r2_link",
    "zarm_r3_link",
    "zarm_r4_link",
    "zarm_r5_link",
    "zarm_r6_link",
    "zarm_r7_link",
]
DEFAULT_HEAD_JOINT_NAMES = ["head_yaw", "head_pitch"]
DEFAULT_DEXHAND_JOINT_NAMES = [
    "left_qiangnao_1",
    "left_qiangnao_2",
    "left_qiangnao_3",
    "left_qiangnao_4",
    "left_qiangnao_5",
    "left_qiangnao_6",
    "right_qiangnao_1",
    "right_qiangnao_2",
    "right_qiangnao_3",
    "right_qiangnao_4",
    "right_qiangnao_5",
    "right_qiangnao_6",
]
DEFAULT_LEJUCLAW_JOINT_NAMES = [
    "left_claw",
    "right_claw",
]

DEFAULT_JOINT_NAMES_LIST = (
    DEFAULT_LEG_JOINT_NAMES + DEFAULT_ARM_JOINT_NAMES + DEFAULT_HEAD_JOINT_NAMES
)

DEFAULT_JOINT_NAMES = {
    "full_joint_names": DEFAULT_LEG_JOINT_NAMES
    + DEFAULT_ARM_JOINT_NAMES
    + DEFAULT_HEAD_JOINT_NAMES,
    "leg_joint_names": DEFAULT_LEG_JOINT_NAMES,
    "arm_joint_names": DEFAULT_ARM_JOINT_NAMES,
    "head_joint_names": DEFAULT_HEAD_JOINT_NAMES,
}


class KuavoMsgProcesser:
    def __init__(self, resize_w, resize_h):
        self.RESIZE_W = resize_w
        self.RESIZE_H = resize_h

    @staticmethod
    def process_joint_q_state(msg):
        joint_q = msg.joint_data.joint_q
        return {"data": joint_q, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_v_state(msg):
        joint_v = msg.joint_data.joint_v
        return {"data": joint_v, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_vd_state(msg):
        joint_vd = msg.joint_data.joint_vd
        return {"data": joint_vd, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_current_state(msg):
        joint_current = msg.joint_data.joint_current
        return {"data": joint_current, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_torque_state(msg):
        joint_torque = msg.joint_data.joint_torque
        return {"data": joint_torque, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_sensors_data_raw_extract_imu(msg):
        imu_data = msg.imu_data
        imu = np.array(
            [
                imu_data.gyro.x,
                imu_data.gyro.y,
                imu_data.gyro.z,
                imu_data.acc.x,
                imu_data.acc.y,
                imu_data.acc.z,
                imu_data.free_acc.x,
                imu_data.free_acc.y,
                imu_data.free_acc.z,
                imu_data.quat.x,
                imu_data.quat.y,
                imu_data.quat.z,
                imu_data.quat.w,
            ]
        )
        return {"data": imu, "timestamp": msg.header.stamp.to_sec()}

    # /kuavo_arm_traj

    @staticmethod
    def process_kuavo_arm_traj(msg):
        return {
            "data": np.deg2rad(msg.position),
            "timestamp": msg.header.stamp.to_sec(),
        }

    # /joint_cmd
    @staticmethod
    def process_joint_cmd_joint_q(msg):
        return {"data": msg.joint_q, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_cmd_joint_v(msg):
        return {"data": msg.joint_v, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_cmd_tau(msg):
        return {"data": msg.tau, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_cmd_tau_max(msg):
        return {"data": msg.tau_max, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_cmd_tau_ratio(msg):
        return {"data": msg.tau_ratio, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_cmd_joint_kp(msg):
        return {"data": msg.joint_kp, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_cmd_joint_kd(msg):
        return {"data": msg.joint_kd, "timestamp": msg.header.stamp.to_sec()}

    @staticmethod
    def process_joint_cmd_control_modes(msg):
        return {"data": msg.control_modes, "timestamp": msg.header.stamp.to_sec()}

    # /control_robot_hand_position_state

    @staticmethod
    def process_qiangnao_state(msg):
        state = list(msg.left_hand_position) + list(msg.right_hand_position)
        return {"data": state, "timestamp": msg.header.stamp.to_sec()}

    # /control_robot_hand_position
    @staticmethod
    def process_qiangnao_cmd(msg):
        position = list(msg.left_hand_position) + list(msg.right_hand_position)
        return {"data": position, "timestamp": msg.header.stamp.to_sec()}

    # /leju_claw_state
    @staticmethod
    def process_claw_state(msg):
        state = msg.data.position
        return {"data": state, "timestamp": msg.header.stamp.to_sec()}

    # /leju_claw_command
    @staticmethod
    def process_claw_cmd(msg):
        return {"data": msg.data.position, "timestamp": msg.header.stamp.to_sec()}

    def process_color_image(self, msg):
        # åªè¿”å›åŸå§‹bytes
        img_bytes = bytes(msg.data)
        return {"data": img_bytes, "timestamp": msg.header.stamp.to_sec()}

    def process_depth_image(self, msg):
        img_bytes = bytes(msg.data)
        return {
            "data": img_bytes,
            "timestamp": msg.header.stamp.to_sec(),
            "compressed": True,
        }

    def process_depth_image_16U(self, msg):
        img_bytes = bytes(msg.data)
        return {
            "data": img_bytes,
            "timestamp": msg.header.stamp.to_sec(),
            "compressed": False,
        }

    def process_depth(self, msg):
        if not (hasattr(msg, "format") and hasattr(msg, "data")):
            print(f"Skipping invalid message")

        # print(f"message format: {msg.format}")

        png_magic = bytes([137, 80, 78, 71, 13, 10, 26, 10])
        idx = msg.data.find(png_magic)
        if idx == -1:
            print("PNG header not found, unable to decode.")
            return None

        png_data = msg.data[idx:]
        np_arr = np.frombuffer(png_data, np.uint8)
        image = cv2.imdecode(np_arr, cv2.IMREAD_UNCHANGED)
        if image is None:
            print("cv2.imdecode also failed")
            return None

        if image.dtype != np.uint16:
            print(
                "Warning: The decoded image is not a 16-bit image, actual dtype: ",
                image.dtype,
            )
        depth_image = cv2.resize(
            image, (self.RESIZE_W, self.RESIZE_H), interpolation=cv2.INTER_NEAREST
        )

        return {
            "data": depth_image[np.newaxis, ...],
            "timestamp": msg.header.stamp.to_sec(),
        }

    @staticmethod
    def process_camera_metadata(msg):
        return {"data": msg.json_data, "timestamp": msg.header.stamp.to_sec()}

        # /color/metadata

    @staticmethod
    def process_camera_info(msg):
        distortion_model = msg.distortion_model
        D = np.array(msg.D)  # ç•¸å˜å‚æ•°
        K = np.array(msg.K)  # ç›¸æœºå†…å‚çŸ©é˜µ
        R = np.array(msg.R)  # æ—‹è½¬çŸ©é˜µ
        P = np.array(msg.P)  # æŠ•å½±çŸ©é˜µ

        # æ‹¼æ¥æˆä¸€ä¸ªå‘é‡
        # é¡ºåº: ç•¸å˜å‚æ•°D + å†…å‚Kï¼ˆå±•å¹³ï¼‰ + æ—‹è½¬çŸ©é˜µRï¼ˆå±•å¹³ï¼‰ + æŠ•å½±çŸ©é˜µPï¼ˆå±•å¹³ï¼‰
        camera_vec = np.concatenate(
            [
                D.ravel(),  # å±•å¹³ç•¸å˜å‚æ•°æ•°ç»„
                K.ravel(),  # å±•å¹³å†…å‚çŸ©é˜µ
                R.ravel(),  # å±•å¹³æ—‹è½¬çŸ©é˜µ
                P.ravel(),  # å±•å¹³æŠ•å½±çŸ©é˜µ
            ]
        )
        # print("+" * 20,camera_vec.shape, "camera_vec")

        return {
            "data": camera_vec,
            "distortion_model": distortion_model,
            "timestamp": msg.header.stamp.to_sec(),
        }


class KuavoRosbagReader:
    def __init__(self, config):
        self.DEFAULT_CAMERA_NAMES = config.default_camera_names
        self.TRAIN_HZ = config.train_hz
        self.MAIN_TIMELINE_FPS = config.main_timeline_fps
        self.SAMPLE_DROP = config.sample_drop
        self._msg_processer = KuavoMsgProcesser(
            config.resize.width, config.resize.height
        )
        self.TOPICS = config.topics
        self.EEF_TYPE = config.eef_type
        self.HAND_STATE_TOPICS = [
            "/control_robot_hand_position_state",
            "/dexhand/state",
        ]
        self.MAIN_TIMESTAMP_TOPIC = "head_cam_h"
        self.TIME_TOLERANCE = 180
        # å…ˆä¸æ„å»ºmain_topic_mapï¼Œç­‰åˆ°process_rosbagæ—¶å†æ„å»º
        self.main_topic_map = None

        # åŠ¨æ€æ„å»ºtopicå¤„ç†æ˜ å°„
        self._topic_process_map = {}

        for camera in self.DEFAULT_CAMERA_NAMES:
            # å½©è‰²å›¾åƒï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            color_topic = f"/{camera[-5:]}/color/image_raw/compressed"
            if color_topic in self.TOPICS:
                self._topic_process_map[f"{camera}"] = {
                    "topic": color_topic,
                    "msg_process_fn": self._msg_processer.process_color_image,
                }
                camera_info_topic = f"/{camera[-5:]}/color/camera_info"
                if camera_info_topic in self.TOPICS:
                    self._topic_process_map[f"{camera}_camera_info"] = {
                        "topic": camera_info_topic,
                        "msg_process_fn": self._msg_processer.process_camera_info,
                    }

            # æ·±åº¦å›¾åƒï¼ˆä¼˜å…ˆæœªå‹ç¼©ï¼‰
            if "wrist" in camera:
                depth_topic_uncompressed = (
                    f"/{camera[-5:]}/depth/image_rect_raw/compressedDepth"
                )
                depth_topic_compressed = (
                    f"/{camera[-5:]}/depth/image_rect_raw/compressed"
                )
            else:  # head_cam_h
                depth_topic_uncompressed = (
                    f"/{camera[-5:]}/depth/image_raw/compressedDepth"
                )
                depth_topic_compressed = f"/{camera[-5:]}/depth/image_raw/compressed"

            if depth_topic_uncompressed in self.TOPICS:
                print(f"[INFO] {camera}: é€‰æ‹©æœªå‹ç¼©æ·±åº¦è¯é¢˜ {depth_topic_uncompressed}")
                self._topic_process_map[f"{camera}_depth"] = {
                    "topic": depth_topic_uncompressed,
                    "msg_process_fn": self._msg_processer.process_depth_image_16U,
                    "fallback_topic": depth_topic_compressed,
                    "fallback_fn": self._msg_processer.process_depth_image,
                }
            elif depth_topic_compressed in self.TOPICS:
                print(f"[INFO] {camera}: ä»…æ‰¾åˆ°å‹ç¼©æ·±åº¦è¯é¢˜ {depth_topic_compressed}")
                self._topic_process_map[f"{camera}_depth"] = {
                    "topic": depth_topic_compressed,
                    "msg_process_fn": self._msg_processer.process_depth_image,
                }
            else:
                print(f"[WARN] {camera} æœªæ‰¾åˆ°æ·±åº¦è¯é¢˜ï¼ˆæœªå‹ç¼©æˆ–å‹ç¼©ï¼‰")

    def _find_actual_hand_state_topic(self, bag_file):
        """è‡ªåŠ¨æ£€æµ‹å®é™…å­˜åœ¨çš„æ‰‹çŠ¶æ€è¯é¢˜"""
        import rosbag

        bag = rosbag.Bag(bag_file)
        bag_topics = set([t for t in bag.get_type_and_topic_info().topics])
        bag.close()
        for t in self.HAND_STATE_TOPICS:
            if t in bag_topics:
                return t
        return None

    def _test_joint_current_availability(self, bag_file):
        """æµ‹è¯•bagæ–‡ä»¶ä¸­çš„/sensors_data_rawæ¶ˆæ¯æ˜¯å¦æœ‰joint_currentå­—æ®µ"""
        try:
            import rosbag

            bag = rosbag.Bag(bag_file)

            # è¯»å–ç¬¬ä¸€æ¡/sensors_data_rawæ¶ˆæ¯è¿›è¡Œæµ‹è¯•
            for topic, msg, t in bag.read_messages(topics=["/sensors_data_raw"]):
                try:
                    # å°è¯•è®¿é—®joint_currentå­—æ®µ
                    _ = msg.joint_data.joint_current
                    bag.close()
                    return True  # æœ‰joint_currentå­—æ®µ
                except AttributeError:
                    try:
                        # å°è¯•è®¿é—®joint_torqueå­—æ®µ
                        _ = msg.joint_data.joint_torque
                        bag.close()
                        return False  # æ²¡æœ‰joint_currentï¼Œä½†æœ‰joint_torque
                    except AttributeError:
                        bag.close()
                        return False  # éƒ½æ²¡æœ‰ï¼Œé»˜è®¤ä½¿ç”¨joint_torque
                # åªæµ‹è¯•ç¬¬ä¸€æ¡æ¶ˆæ¯
                break

            bag.close()
            return False  # æ²¡æœ‰æ‰¾åˆ°æ¶ˆæ¯ï¼Œé»˜è®¤ä½¿ç”¨joint_torque

        except Exception as e:
            print(f"æµ‹è¯•joint_currentå¯ç”¨æ€§æ—¶å‡ºé”™: {e}")
            return False  # å‡ºé”™æ—¶é»˜è®¤ä½¿ç”¨joint_torque

    def extract_and_format_camera_extrinsics(
        self, bag_file, abs_start=None, abs_end=None
    ):
        """æå–å¹¶æ ¼å¼åŒ–ç›¸æœºå¤–å‚ï¼Œæ”¯æŒæ—¶é—´è£å‰ª"""
        urdf_path = getattr(self, "urdf_path", None)
        if (
            urdf_path is None
            and hasattr(self, "config")
            and hasattr(self.config, "urdf_path")
        ):
            urdf_path = self.config.urdf_path
        if urdf_path is None:
            urdf_path = "./kuavo/biped_s45.urdf"  # é»˜è®¤è·¯å¾„

        # æ£€æŸ¥è¯é¢˜
        bag = self.load_raw_rosbag(bag_file)
        bag_topics = set([t for t in bag.get_type_and_topic_info().topics])
        bag.close()
        if "/sensors_data_raw" not in bag_topics:
            return {}

        # æ–°å¢ï¼šå¸¦æ—¶é—´è£å‰ªçš„å¤–å‚æå–
        camera_poses = extract_camera_poses_from_bag_with_time(
            bag_file, urdf_path, abs_start, abs_end
        )

        def rigid_to_dict(rigid, ts):
            return {
                "rotation_matrix": rigid.rotation().matrix().tolist(),
                "translation_vector": rigid.translation().tolist(),
                "timestamp": ts,
            }

        return {
            "head_camera_extrinsics": [
                rigid_to_dict(pose, ts)
                for pose, ts in zip(
                    camera_poses["head_camera_poses"], camera_poses["timestamps"]
                )
            ],
            "left_hand_camera_extrinsics": [
                rigid_to_dict(pose, ts)
                for pose, ts in zip(
                    camera_poses["left_hand_camera_poses"], camera_poses["timestamps"]
                )
            ],
            "right_hand_camera_extrinsics": [
                rigid_to_dict(pose, ts)
                for pose, ts in zip(
                    camera_poses["right_hand_camera_poses"], camera_poses["timestamps"]
                )
            ],
        }

    def _build_main_topic_map(self, bag_file):
        # ...existing code...
        use_joint_current = self._test_joint_current_availability(bag_file)
        if use_joint_current:
            joint_current_processor = self._msg_processer.process_joint_current_state
            print("ä½¿ç”¨ joint_current è¯é¢˜")
        else:
            joint_current_processor = self._msg_processer.process_joint_torque_state
            print("ä½¿ç”¨ joint_torque è¯é¢˜")

        # è‡ªåŠ¨é€‚é…æ‰‹çŠ¶æ€è¯é¢˜
        actual_hand_state_topic = self._find_actual_hand_state_topic(bag_file)
        if actual_hand_state_topic:
            print(f"ä½¿ç”¨æ‰‹éƒ¨çŠ¶æ€è¯é¢˜: {actual_hand_state_topic}")
        else:
            print("[WARN] æœªæ‰¾åˆ°æ‰‹éƒ¨çŠ¶æ€è¯é¢˜ï¼Œå°†ä¸ä¼šè¯»å–æ‰‹éƒ¨çŠ¶æ€æ•°æ®ã€‚")
        main_topic_map = {
            "/sensors_data_raw": [
                (
                    "observation.sensorsData.joint_q",
                    self._msg_processer.process_joint_q_state,
                ),
                (
                    "observation.sensorsData.joint_v",
                    self._msg_processer.process_joint_v_state,
                ),
                (
                    "observation.sensorsData.joint_vd",
                    self._msg_processer.process_joint_vd_state,
                ),
                ("observation.sensorsData.joint_current", joint_current_processor),
                (
                    "observation.sensorsData.imu",
                    self._msg_processer.process_sensors_data_raw_extract_imu,
                ),
            ],
            "/kuavo_arm_traj": [
                ("action.kuavo_arm_traj", self._msg_processer.process_kuavo_arm_traj),
            ],
            "/joint_cmd": [
                (
                    "action.joint_cmd.joint_q",
                    self._msg_processer.process_joint_cmd_joint_q,
                ),
                (
                    "action.joint_cmd.joint_v",
                    self._msg_processer.process_joint_cmd_joint_v,
                ),
                ("action.joint_cmd.tau", self._msg_processer.process_joint_cmd_tau),
                (
                    "action.joint_cmd.tau_max",
                    self._msg_processer.process_joint_cmd_tau_max,
                ),
                (
                    "action.joint_cmd.tau_ratio",
                    self._msg_processer.process_joint_cmd_tau_ratio,
                ),
                (
                    "action.joint_cmd.tau_joint_kp",
                    self._msg_processer.process_joint_cmd_joint_kp,
                ),
                (
                    "action.joint_cmd.tau_joint_kd",
                    self._msg_processer.process_joint_cmd_joint_kd,
                ),
                (
                    "action.joint_cmd.control_modes",
                    self._msg_processer.process_joint_cmd_control_modes,
                ),
            ],
            # "/control_robot_hand_position_state": [
            #     ("observation.qiangnao", self._msg_processer.process_qiangnao_state),
            # ],
            "/control_robot_hand_position": [
                ("action.qiangnao", self._msg_processer.process_qiangnao_cmd),
            ],
            "/leju_claw_state": [
                ("observation.claw", self._msg_processer.process_claw_state),
            ],
            "/leju_claw_command": [
                ("action.claw", self._msg_processer.process_claw_cmd),
            ],
        }

        # åŠ¨æ€æ·»åŠ æ‰‹çŠ¶æ€è¯é¢˜
        if actual_hand_state_topic == "/control_robot_hand_position_state":
            main_topic_map[actual_hand_state_topic] = [
                ("observation.qiangnao", self._msg_processer.process_qiangnao_state),
            ]
        elif actual_hand_state_topic == "/dexhand/state":
            # æ–°æ ¼å¼ï¼Œç›´æ¥è¯»å–12ç»´æ•°ç»„
            def process_dexhand_state(msg):
                # å‡è®¾msg.positionä¸º12ç»´æ•°ç»„
                return {
                    "data": list(msg.position),
                    "timestamp": msg.header.stamp.to_sec(),
                }

            main_topic_map[actual_hand_state_topic] = [
                ("observation.qiangnao", process_dexhand_state),
            ]
        return main_topic_map

    def load_raw_rosbag(self, bag_file: str):
        return rosbag.Bag(bag_file)

    def print_bag_info(self, bag: rosbag.Bag):
        pprint(bag.get_type_and_topic_info().topics)

    def process_rosbag(
        self,
        bag_file: str,
        start_time: float = 0,
        end_time: float = 1,
        action_config=None,
    ):
        import gc
        import rospy

        # å¦‚æœè¿˜æ²¡æœ‰æ„å»ºmain_topic_mapï¼Œå…ˆæ„å»ºå®ƒ
        if self.main_topic_map is None:
            self.main_topic_map = self._build_main_topic_map(bag_file)
            # é€‚é…æ‰‹çŠ¶æ€è¯é¢˜
            actual_hand_state_topic = None
            for t in self.HAND_STATE_TOPICS:
                if t in self.main_topic_map:
                    actual_hand_state_topic = t
                    break
            for topic in self.TOPICS:
                # é€‚é…æ‰‹çŠ¶æ€è¯é¢˜
                if topic in self.HAND_STATE_TOPICS:
                    if (
                        actual_hand_state_topic
                        and actual_hand_state_topic in self.main_topic_map
                    ):
                        for key, fn in self.main_topic_map[actual_hand_state_topic]:
                            self._topic_process_map[key] = {
                                "topic": actual_hand_state_topic,
                                "msg_process_fn": fn,
                            }
                elif topic in self.main_topic_map:
                    for key, fn in self.main_topic_map[topic]:
                        self._topic_process_map[key] = {
                            "topic": topic,
                            "msg_process_fn": fn,
                        }

        bag = self.load_raw_rosbag(bag_file)
        data = {}

        # Get bag start time and duration
        bag_start = bag.get_start_time()
        bag_end = bag.get_end_time()
        bag_duration = bag_end - bag_start

        # Calculate absolute start/end times
        abs_start = bag_start + start_time * bag_duration
        abs_end = bag_start + end_time * bag_duration

        print(f"å¼€å§‹å¤„ç† bag æ–‡ä»¶: {bag_file}")
        print(f"è¯é¢˜æ•°é‡: {len(self._topic_process_map)}")

        # åˆ†è¯é¢˜å¤„ç†ï¼Œæ¯å¤„ç†å®Œä¸€ä¸ªè¯é¢˜ç«‹å³å›æ”¶å†…å­˜
        processed_topics = 0
        total_topics = len(self._topic_process_map)

        for key, topic_info in self._topic_process_map.items():
            topic = topic_info["topic"]
            msg_process_fn = topic_info["msg_process_fn"]
            data[key] = []

            print(f"[{processed_topics+1}/{total_topics}] å¤„ç†è¯é¢˜: {topic} -> {key}")

            # ä¸´æ—¶å­˜å‚¨æ¶ˆæ¯æ•°æ®
            temp_messages = []

            # å…ˆå°è¯•è¯»å–ä¸» topic
            frame_count = 0
            for _, msg, t in bag.read_messages(
                topics=[topic],  # åªè¯»å–å½“å‰è¯é¢˜
                start_time=rospy.Time.from_sec(abs_start),
                end_time=rospy.Time.from_sec(abs_end),
            ):
                msg_data = msg_process_fn(msg)
                correct_timestamp = t.to_sec()
                msg_data["timestamp"] = correct_timestamp
                temp_messages.append(msg_data)
                frame_count += 1

            # å°†ä¸´æ—¶æ¶ˆæ¯æ•°æ®è½¬ç§»åˆ°dataä¸­
            data[key] = temp_messages
            # ç«‹å³åˆ é™¤ä¸´æ—¶å˜é‡
            del temp_messages

            # å¦‚æœæ˜¯æ·±åº¦è¯é¢˜ä¸”æ²¡è¯»åˆ°æ¶ˆæ¯ï¼Œä¸”æœ‰ fallbackï¼Œå°è¯•é™çº§
            if (
                len(data[key]) == 0
                and "fallback_topic" in topic_info
                and "fallback_fn" in topic_info
            ):
                print(
                    f"  [WARN] {topic} æœªè¯»å–åˆ°æ•°æ®ï¼Œå°è¯•é™çº§åˆ° {topic_info['fallback_topic']}"
                )
                fallback_messages = []
                fallback_count = 0

                for _, msg, t in bag.read_messages(
                    topics=[topic_info["fallback_topic"]],  # åªè¯»å–fallbackè¯é¢˜
                    start_time=rospy.Time.from_sec(abs_start),
                    end_time=rospy.Time.from_sec(abs_end),
                ):
                    msg_data = topic_info["fallback_fn"](msg)
                    correct_timestamp = t.to_sec()
                    msg_data["timestamp"] = correct_timestamp
                    fallback_messages.append(msg_data)
                    fallback_count += 1

                # ä½¿ç”¨fallbackæ•°æ®
                data[key] = fallback_messages
                del fallback_messages

            processed_topics += 1
            print(f"  å®Œæˆ: {key} ({len(data[key])} å¸§)")

            # æ¯å¤„ç†3ä¸ªè¯é¢˜è¿›è¡Œä¸€æ¬¡å†…å­˜å›æ”¶å¹¶åˆ é™¤ä¸´æ—¶å˜é‡
            if processed_topics % 3 == 0:
                # åˆ é™¤å¯èƒ½çš„ä¸´æ—¶å˜é‡
                if "msg_data" in locals():
                    del msg_data
                if "correct_timestamp" in locals():
                    del correct_timestamp
                gc.collect()
                print(f"  [å†…å­˜å›æ”¶] å·²å¤„ç† {processed_topics}/{total_topics} ä¸ªè¯é¢˜")

        # 2. ç«‹å³å…³é—­bagæ–‡ä»¶é‡Šæ”¾èµ„æº
        bag.close()
        del bag
        # åˆ é™¤æ—¶é—´ç›¸å…³å˜é‡
        gc.collect()
        print(f"âœ… bagæ–‡ä»¶å·²å…³é—­ï¼ŒåŸºç¡€æ•°æ®è¯»å–å®Œæˆ")

        # 3. æå–ç›¸æœºå¤–å‚ï¼ˆå¯èƒ½æ¶ˆè€—è¾ƒå¤šå†…å­˜ï¼‰
        print("ğŸ“ å¼€å§‹æå–ç›¸æœºå¤–å‚...")
        extrinsics = self.extract_and_format_camera_extrinsics(
            bag_file, abs_start, abs_end
        )
        data.update(extrinsics)

        # æ¸…ç†å¤–å‚å¤„ç†ä¸­çš„ä¸´æ—¶å˜é‡
        del extrinsics
        gc.collect()
        print("âœ… ç›¸æœºå¤–å‚æå–å®Œæˆ")

        # 4. æ–°å¢ï¼šæœ«ç«¯æ‰§è¡Œå™¨ä½å§¿è®¡ç®—
        print("ğŸ”§ å¼€å§‹è®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿...")
        joint_q_items = data.get("observation.sensorsData.joint_q", [])
        if joint_q_items:
            from endeffector_pose_from_bag import extract_and_format_eef_extrinsics

            joint_q_list = [item["data"] for item in joint_q_items]
            timestamps = [item["timestamp"] for item in joint_q_items]

            positions, quaternions = extract_and_format_eef_extrinsics(
                [{"joint_q": q} for q in joint_q_list],
                urdf_path="./kuavo/biped_s49.urdf",
            )

            # ç»„è£…ä¸º [{data: ..., timestamp: ...}, ...]
            data["end.position"] = [
                {"data": positions[i], "timestamp": timestamps[i]}
                for i in range(len(positions))
            ]
            data["end.orientation"] = [
                {"data": quaternions[i], "timestamp": timestamps[i]}
                for i in range(len(quaternions))
            ]

            # æ¸…ç†æœ«ç«¯æ‰§è¡Œå™¨è®¡ç®—çš„ä¸´æ—¶å˜é‡
            del joint_q_list, timestamps, positions, quaternions, joint_q_items
            gc.collect()
            print("âœ… æœ«ç«¯æ‰§è¡Œå™¨ä½å§¿è®¡ç®—å®Œæˆ")
        else:
            data["end.position"] = []
            data["end.orientation"] = []
            del joint_q_items

        # 5. è®¡ç®—æ€»ä½“å‚æ•°
        total_cut_frames = int(
            round((end_time - start_time) * bag_duration * self.TRAIN_HZ)
        )
        drop_head = (
            self.SAMPLE_DROP
            if (start_time * bag_duration * self.TRAIN_HZ) <= self.SAMPLE_DROP
            else 0
        )
        drop_tail = (
            self.SAMPLE_DROP
            if ((1 - end_time) * bag_duration * self.TRAIN_HZ) <= self.SAMPLE_DROP
            else 0
        )

        print(f"ğŸ”„ å¼€å§‹æ—¶é—´æˆ³å¯¹é½å¤„ç†...")
        print(f"  é¢„ä¼°å¸§æ•°: {total_cut_frames}")
        print(f"  å¤´éƒ¨ä¸¢å¼ƒ: {drop_head}, å°¾éƒ¨ä¸¢å¼ƒ: {drop_tail}")

        # åˆ é™¤ä¸å†éœ€è¦çš„å‚æ•°å˜é‡
        del total_cut_frames, start_time, end_time

        # 6. æ‰§è¡Œå¯¹é½ï¼ˆæœ€æ¶ˆè€—å†…å­˜çš„æ­¥éª¤ï¼‰
        aligned_data = self.align_frame_data_optimized(
            data, drop_head, drop_tail, action_config=action_config
        )

        # 7. ç«‹å³æ¸…ç†åŸå§‹æ•°æ®å’Œå¯¹é½å‚æ•°
        del data, drop_head, drop_tail, action_config
        # åˆ é™¤å¾ªç¯ä¸­å¯èƒ½æ®‹ç•™çš„å˜é‡
        if "key" in locals():
            del key
        if "topic_info" in locals():
            del topic_info
        if "topic" in locals():
            del topic
        if "msg_process_fn" in locals():
            del msg_process_fn
        if "frame_count" in locals():
            del frame_count
        if "fallback_count" in locals():
            del fallback_count
        if "processed_topics" in locals():
            del processed_topics
        if "total_topics" in locals():
            del total_topics

        gc.collect()
        print("âœ… åŸå§‹æ•°æ®å·²æ¸…ç†ï¼Œæ—¶é—´æˆ³å¯¹é½å®Œæˆ")

        return aligned_data

    def find_closest_indices_vectorized(self, timestamps, target_timestamps):
        """å‘é‡åŒ–æŸ¥æ‰¾æœ€è¿‘æ—¶é—´æˆ³ç´¢å¼•"""
        timestamps = np.array(timestamps)
        target_timestamps = np.array(target_timestamps)

        # ä½¿ç”¨ searchsorted è¿›è¡Œé«˜æ•ˆæŸ¥æ‰¾
        indices = np.searchsorted(timestamps, target_timestamps)

        # å¤„ç†è¾¹ç•Œæƒ…å†µ
        indices = np.clip(indices, 0, len(timestamps) - 1)

        # æ£€æŸ¥å·¦å³é‚»å±…ï¼Œé€‰æ‹©æ›´è¿‘çš„
        valid_left = indices > 0
        left_indices = np.where(valid_left, indices - 1, indices)

        left_diffs = np.abs(timestamps[left_indices] - target_timestamps)
        right_diffs = np.abs(timestamps[indices] - target_timestamps)

        # é€‰æ‹©è·ç¦»æ›´è¿‘çš„ç´¢å¼•
        closer_indices = np.where(left_diffs < right_diffs, left_indices, indices)

        return closer_indices

    def _preprocess_timestamps_only_deduplicate(self, data: dict) -> dict:
        """é¢„å¤„ç†æ—¶é—´æˆ³å’Œæ•°æ®ï¼šåªå»é‡å’Œæ£€æµ‹å¡é¡¿ï¼Œä¸æ’å€¼ï¼ˆæŒ‰éœ€æ’å€¼ç­–ç•¥ï¼‰"""
        preprocessed_data = {}

        for key, data_list in data.items():
            if len(data_list) == 0:
                preprocessed_data[key] = []
                continue

            print(f"é¢„å¤„ç† {key}: åŸå§‹é•¿åº¦ {len(data_list)}")

            # æ­¥éª¤1: å»é™¤é‡å¤æ—¶é—´æˆ³
            deduplicated_data = self._remove_duplicate_timestamps(data_list, key)

            # æ­¥éª¤2: æ£€æµ‹å»é‡åæ•°æ®çš„å®é™…æ—¶é—´é—´éš”å¡é¡¿ï¼ˆæ›´å‡†ç¡®ï¼‰
            self._check_actual_time_gaps(
                deduplicated_data, key, max_gap_duration=self.TIME_TOLERANCE
            )

            # æ­¥éª¤3: è·³è¿‡æ’å€¼ï¼ˆæŒ‰éœ€æ’å€¼ç­–ç•¥ï¼‰
            preprocessed_data[key] = deduplicated_data
            print(f"é¢„å¤„ç† {key}: å»é‡å {len(deduplicated_data)} å¸§ï¼ˆæœªæ’å€¼ï¼‰")

        return preprocessed_data

    def _preprocess_timestamps_and_data(self, data: dict) -> dict:
        """é¢„å¤„ç†æ—¶é—´æˆ³å’Œæ•°æ®ï¼šå»é‡ã€æ£€æµ‹å®é™…å¡é¡¿ã€æ’å€¼"""
        preprocessed_data = {}

        for key, data_list in data.items():
            if len(data_list) == 0:
                preprocessed_data[key] = []
                continue

            print(f"é¢„å¤„ç† {key}: åŸå§‹é•¿åº¦ {len(data_list)}")

            # æ­¥éª¤1: å»é™¤é‡å¤æ—¶é—´æˆ³
            deduplicated_data = self._remove_duplicate_timestamps(data_list, key)

            # æ­¥éª¤2: æ£€æµ‹å»é‡åæ•°æ®çš„å®é™…æ—¶é—´é—´éš”å¡é¡¿ï¼ˆæ›´å‡†ç¡®ï¼‰
            self._check_actual_time_gaps(
                deduplicated_data, key, max_gap_duration=self.TIME_TOLERANCE
            )

            # æ­¥éª¤3: æ—¶é—´æˆ³æ’å€¼å’Œæ•°æ®å¡«å……
            interpolated_data = self._interpolate_timestamps_and_data(
                deduplicated_data, key
            )

            preprocessed_data[key] = interpolated_data
            print(
                f"é¢„å¤„ç† {key}: å»é‡å {len(deduplicated_data)}, æ’å€¼å {len(interpolated_data)}"
            )

        return preprocessed_data

    def _check_actual_time_gaps(
        self, data_list: list, key: str, max_gap_duration: float = 2.0
    ):
        """æ£€æµ‹å»é‡åæ•°æ®çš„å®é™…æ—¶é—´é—´éš”å¡é¡¿"""
        if len(data_list) <= 1:
            return

        timestamps_seconds = np.array([item["timestamp"] for item in data_list])
        timestamps_ns = (timestamps_seconds * 1e9).astype(np.int64)

        # è®¡ç®—å®é™…æ—¶é—´é—´éš”
        time_diffs_ns = np.diff(timestamps_ns)
        time_diffs_seconds = time_diffs_ns / 1e9

        # æ‰¾å‡ºè¶…è¿‡é˜ˆå€¼çš„æ—¶é—´é—´éš”
        large_gaps = time_diffs_seconds > max_gap_duration

        if np.any(large_gaps):
            max_gap_seconds = np.max(time_diffs_seconds)
            gap_indices = np.where(large_gaps)[0]

            error_msg = (
                f"æ—¶é—´é—´éš”å¡é¡¿æ£€æµ‹ï¼š{key} è¯é¢˜å­˜åœ¨ {len(gap_indices)} ä¸ªè¶…è¿‡{max_gap_duration}sçš„æ—¶é—´é—´éš”ï¼Œ"
                f"æœ€å¤§é—´éš” {max_gap_seconds:.3f}sï¼Œæ•°æ®è´¨é‡å¼‚å¸¸ï¼Œç»ˆæ­¢å¤„ç†"
            )
            print(f"[ERROR] {error_msg}")

            # æ˜¾ç¤ºå…·ä½“çš„é—®é¢˜é—´éš”
            for i, gap_idx in enumerate(gap_indices[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                start_time = timestamps_seconds[gap_idx]
                end_time = timestamps_seconds[gap_idx + 1]
                gap_duration = time_diffs_seconds[gap_idx]
                print(
                    f"  é—´éš”{i+1}: {start_time:.6f}s -> {end_time:.6f}s, é—´éš”={gap_duration:.3f}s"
                )

            raise TimestampStuckError(
                message=error_msg,
                topic=key,
                stuck_timestamp=timestamps_seconds[gap_indices[0]],
                stuck_duration=max_gap_seconds,
                stuck_frame_count=len(gap_indices),
                threshold=max_gap_duration,
            )
        else:
            max_gap_seconds = (
                np.max(time_diffs_seconds) if len(time_diffs_seconds) > 0 else 0
            )
            print(f"  {key}: âœ“ æ—¶é—´é—´éš”æ­£å¸¸ï¼Œæœ€å¤§é—´éš” {max_gap_seconds:.3f}s")

    def _remove_duplicate_timestamps(self, data_list: list, key: str) -> list:
        """å»é™¤é‡å¤æ—¶é—´æˆ³åŠå¯¹åº”æ•°æ®ï¼ˆä½¿ç”¨çº³ç§’ç²¾åº¦ï¼‰"""
        if len(data_list) <= 1:
            return data_list

        deduplicated = []
        seen_timestamps = set()
        duplicate_count = 0

        for item in data_list:
            timestamp_seconds = item["timestamp"]
            # è½¬æ¢ä¸ºçº³ç§’ç²¾åº¦é¿å…æµ®ç‚¹ç²¾åº¦é—®é¢˜
            timestamp_ns = int(timestamp_seconds * 1e9)

            if timestamp_ns not in seen_timestamps:
                seen_timestamps.add(timestamp_ns)
                deduplicated.append(item)
            else:
                duplicate_count += 1

        if duplicate_count > 0:
            print(f"  {key}: åˆ é™¤ {duplicate_count} ä¸ªé‡å¤æ—¶é—´æˆ³")

        return deduplicated

    def _interpolate_timestamps_and_data(self, data_list: list, key: str) -> list:
        """æ—¶é—´æˆ³æ’å€¼å’Œæ•°æ®å¡«å……ï¼ˆä¿®å¤ç‰ˆæœ¬ - ä¸¥æ ¼æ§åˆ¶é—´éš”ï¼Œè¶…è¿‡2ç§’ç›´æ¥æŠ›å¼‚å¸¸ï¼‰"""
        if len(data_list) <= 1:
            return data_list

        timestamps_seconds = np.array([item["timestamp"] for item in data_list])
        timestamps_ns = (timestamps_seconds * 1e9).astype(np.int64)

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è¶…è¿‡2ç§’çš„é—´éš”ï¼Œå¦‚æœæœ‰ç›´æ¥æŠ›å‡ºå¼‚å¸¸
        time_diffs_ns = np.diff(timestamps_ns)
        time_diffs_seconds = time_diffs_ns / 1e9

        max_gap_seconds = np.max(time_diffs_seconds)
        large_gaps_2s = time_diffs_seconds > self.TIME_TOLERANCE  # 2ç§’é˜ˆå€¼

        if np.any(large_gaps_2s):
            gap_indices = np.where(large_gaps_2s)[0]
            error_msg = (
                f"æ’å€¼é˜¶æ®µå‘ç°ä¸¥é‡æ—¶é—´é—´éš”ï¼š{key} è¯é¢˜å­˜åœ¨ {len(gap_indices)} ä¸ªè¶…è¿‡{self.TIME_TOLERANCE}sçš„æ—¶é—´é—´éš”ï¼Œ"
                f"æœ€å¤§é—´éš” {max_gap_seconds:.3f}sï¼Œæ•°æ®è´¨é‡å¼‚å¸¸ï¼Œç»ˆæ­¢å¤„ç†"
            )
            print(f"[ERROR] {error_msg}")

            # æ˜¾ç¤ºå…·ä½“çš„é—®é¢˜é—´éš”
            for i, gap_idx in enumerate(gap_indices[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                start_time = timestamps_seconds[gap_idx]
                end_time = timestamps_seconds[gap_idx + 1]
                gap_duration = time_diffs_seconds[gap_idx]
                print(
                    f"  ä¸¥é‡é—´éš”{i+1}: {start_time:.6f}s -> {end_time:.6f}s, é—´éš”={gap_duration:.3f}s"
                )

            raise TimestampStuckError(
                message=error_msg,
                topic=key,
                stuck_timestamp=timestamps_seconds[gap_indices[0]],
                stuck_duration=max_gap_seconds,
                stuck_frame_count=len(gap_indices),
                threshold=2.0,
            )

        # ç¡®å®šæ’å€¼é—´éš”ï¼ˆçº³ç§’ï¼‰
        if any(cam in key for cam in ["head_cam"]) and "depth" not in key:
            # å½©è‰²è§†é¢‘ï¼š33msé—´éš” (30fps)
            target_interval_ns = int(32 * 1e6)  # çº³ç§’
            max_allowed_interval_ns = int(39.8 * 1e6)  # 37msæœ€å¤§å…è®¸é—´éš”
            data_type = "video"
        elif any(cam in key for cam in ["wrist_cam"]) and "depth" not in key:
            # å½©è‰²è§†é¢‘ï¼š33msé—´éš” (30fps)
            target_interval_ns = int(32 * 1e6)  # çº³ç§’
            max_allowed_interval_ns = int(8 * 1e6)  # 38msæœ€å¤§å…è®¸é—´éš”
            data_type = "video"
        elif "depth" in key:
            # æ·±åº¦è§†é¢‘ï¼š33msé—´éš” (30fps)
            target_interval_ns = int(32 * 1e6)  # çº³ç§’
            max_allowed_interval_ns = int(8 * 1e6)  # 38msæœ€å¤§å…è®¸é—´éš”
            data_type = "depth"

        else:
            # ä¼ æ„Ÿå™¨æ•°æ®ï¼š5msé—´éš” (100hz)
            target_interval_ns = int(10 * 1e6)  # çº³ç§’
            max_allowed_interval_ns = int(4 * 1e6)  # 5msæœ€å¤§å…è®¸é—´éš”ï¼ˆä¼ æ„Ÿå™¨æ›´ä¸¥æ ¼ï¼‰
            data_type = "sensor"

        # æ£€æµ‹éœ€è¦æ’å€¼çš„ä½ç½®ï¼ˆä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼‰
        interpolation_threshold_ns = (
            max_allowed_interval_ns  # ç›´æ¥ä½¿ç”¨æœ€å¤§å…è®¸é—´éš”ä½œä¸ºé˜ˆå€¼
        )

        large_gaps = time_diffs_ns > interpolation_threshold_ns

        if not np.any(large_gaps):
            # æ— éœ€æ’å€¼
            print(f"  {key}: æ— éœ€æ’å€¼ï¼Œæœ€å¤§é—´éš” {np.max(time_diffs_ns)/1e6:.1f}ms")
            return data_list

        print(f"  {key}: å‘ç° {np.sum(large_gaps)} ä¸ªéœ€è¦æ’å€¼çš„æ—¶é—´é—´éš”")
        print(
            f"  {key}: ç›®æ ‡é—´éš” {target_interval_ns/1e6:.1f}ms, æœ€å¤§å…è®¸é—´éš” {max_allowed_interval_ns/1e6:.1f}ms"
        )

        # æ„å»ºæ’å€¼åçš„æ•°æ®
        interpolated_data = []

        for i in range(len(data_list)):
            # æ·»åŠ å½“å‰æ•°æ®ç‚¹
            interpolated_data.append(data_list[i])

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœ¨å½“å‰ç‚¹å’Œä¸‹ä¸€ç‚¹ä¹‹é—´æ’å€¼
            if i < len(data_list) - 1 and large_gaps[i]:
                current_time_ns = timestamps_ns[i]
                next_time_ns = timestamps_ns[i + 1]
                gap_duration_ns = next_time_ns - current_time_ns
                gap_duration_seconds = gap_duration_ns / 1e9

                # åŒé‡ä¿é™©ï¼šå†æ¬¡æ£€æŸ¥é—´éš”æ˜¯å¦è¶…è¿‡self.TIME_TOLERANCE
                if gap_duration_seconds > self.TIME_TOLERANCE:
                    error_msg = f"æ’å€¼è¿‡ç¨‹ä¸­å‘ç°è¶…è¿‡{self.TIME_TOLERANCE}ç§’çš„é—´éš”ï¼š{key} åœ¨ç´¢å¼•{i}å¤„æœ‰{gap_duration_seconds:.3f}sé—´éš”"
                    print(f"[ERROR] {error_msg}")
                    raise TimestampStuckError(
                        message=error_msg,
                        topic=key,
                        stuck_timestamp=current_time_ns / 1e9,
                        stuck_duration=gap_duration_seconds,
                        stuck_frame_count=1,
                        threshold=2.0,
                    )

                # print(f"    é—´éš”{i}: {gap_duration_ns/1e6:.1f}ms éœ€è¦æ’å€¼")

                # è®¡ç®—éœ€è¦æ’å…¥å¤šå°‘ä¸ªç‚¹æ¥æ»¡è¶³æœ€å¤§é—´éš”è¦æ±‚
                num_segments_needed = int(
                    np.ceil(gap_duration_ns / max_allowed_interval_ns)
                )

                if num_segments_needed > 1:
                    # éœ€è¦æ’å€¼
                    num_interpolations = num_segments_needed - 1

                    # ç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„æ’å€¼æ—¶é—´æˆ³
                    interp_times_ns = np.linspace(
                        current_time_ns,
                        next_time_ns,
                        num_interpolations + 2,  # +2 åŒ…å«èµ·ç‚¹å’Œç»ˆç‚¹
                        dtype=np.int64,
                    )[
                        1:-1
                    ]  # å»æ‰èµ·ç‚¹å’Œç»ˆç‚¹

                    # print(f"    æ’å…¥ {len(interp_times_ns)} ä¸ªç‚¹ï¼Œå¹³å‡é—´éš” {gap_duration_ns/(num_interpolations+1)/1e6:.1f}ms")

                    # æ’å…¥æ•°æ®ç‚¹
                    for interp_time_ns in interp_times_ns:
                        interp_time_seconds = interp_time_ns / 1e9  # è½¬å›ç§’
                        interpolated_item = self._create_interpolated_data_point(
                            data_list[i], interp_time_seconds, data_type
                        )
                        interpolated_data.append(interpolated_item)

        # éªŒè¯æ’å€¼ç»“æœ
        final_timestamps = np.array([item["timestamp"] for item in interpolated_data])
        final_timestamps_ns = (final_timestamps * 1e9).astype(np.int64)
        final_intervals_ns = np.diff(final_timestamps_ns)
        final_intervals_ms = final_intervals_ns / 1e6

        max_final_interval = np.max(final_intervals_ms)
        print(f"  {key}: æ’å€¼å®Œæˆï¼Œæœ€å¤§é—´éš” {max_final_interval:.1f}ms")

        # # æœ€ç»ˆæ£€æŸ¥ï¼šå¦‚æœæ’å€¼åä»ç„¶å­˜åœ¨è¶…è¿‡é˜ˆå€¼çš„é—´éš”ï¼ŒæŠ›å‡ºå¼‚å¸¸
        # if max_final_interval > max_allowed_interval_ns / 1e6:
        #     problematic_indices = np.where(final_intervals_ms > max_allowed_interval_ns / 1e6)[0]
        #     error_msg = f"æ’å€¼åéªŒè¯å¤±è´¥ï¼š{key} ä»æœ‰ {len(problematic_indices)} ä¸ªé—´éš”è¶…è¿‡{max_allowed_interval_ns/1e6:.1f}msé˜ˆå€¼ï¼Œæœ€å¤§é—´éš”{max_final_interval:.1f}ms"
        #     print(f"[ERROR] {error_msg}")

        #     # æ˜¾ç¤ºå…·ä½“é—®é¢˜
        #     for idx in problematic_indices[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
        #         print(f"    é—®é¢˜é—´éš”{idx}: {final_intervals_ms[idx]:.1f}ms")

        #     raise TimestampStuckError(
        #         message=f"æ’å€¼åè´¨é‡éªŒè¯å¤±è´¥: {error_msg}",
        #         topic=key,
        #         stuck_timestamp=final_timestamps[problematic_indices[0]],
        #         stuck_duration=max_final_interval/1000,
        #         stuck_frame_count=len(problematic_indices),
        #         threshold=max_allowed_interval_ns/1e6/1000  # è½¬æ¢ä¸ºç§’
        #     )

        return interpolated_data

    def _interpolate_on_demand(
        self,
        aligned_data: list,
        time_errors_ms: np.ndarray,
        original_data_list: list,
        original_timestamps: np.ndarray,
        target_timestamps: np.ndarray,
        key: str,
    ) -> list:
        """
        æŒ‰éœ€æ’å€¼ï¼šåªå¯¹è¯¯å·® >10ms çš„å¸§è¿›è¡Œæ’å€¼ä¿®æ­£

        ç­–ç•¥ï¼š
        1. å¦‚æœåŸå§‹æ•°æ®ä¸­æ‰¾ä¸åˆ° <10ms çš„å¸§
        2. å°è¯•åœ¨ç›¸é‚»å¸§ä¹‹é—´æ‰¾æ›´è¿‘çš„ï¼ˆå¤åˆ¶æœ€è¿‘å¸§ï¼‰
        3. å¦‚æœä»ç„¶æ— æ³•æ»¡è¶³ <10msï¼Œä¿æŒåŸé€‰æ‹©å¹¶è®°å½•è­¦å‘Š
        """
        interpolated_count = 0
        created_count = 0

        # åˆ¤æ–­æ•°æ®ç±»å‹
        data_type = (
            "depth"
            if "depth" in key
            else (
                "video"
                if any(cam in key for cam in ["head_cam", "wrist_cam"])
                else "sensor"
            )
        )

        for i, error_ms in enumerate(time_errors_ms):
            if error_ms <= 10:  # è¯¯å·®å¯æ¥å—ï¼ˆ<10msï¼‰
                continue

            target_ts = target_timestamps[i]

            # ç­–ç•¥1: åœ¨åŸå§‹æ•°æ®ä¸­æŸ¥æ‰¾èƒ½æ»¡è¶³ <10ms çš„å¸§
            time_diffs = np.abs(original_timestamps - target_ts) * 1000
            better_candidates = np.where(time_diffs < 10)[0]

            if len(better_candidates) > 0:
                # æ‰¾åˆ°äº†æ›´å¥½çš„åŸå§‹å¸§
                best_idx = better_candidates[np.argmin(time_diffs[better_candidates])]
                aligned_data[i] = original_data_list[best_idx]
                interpolated_count += 1
            else:
                # ç­–ç•¥2: æ‰¾ä¸åˆ° <10ms çš„åŸå§‹å¸§ï¼Œåˆ›å»ºæ’å€¼å¸§
                # æ‰¾åˆ°æœ€è¿‘çš„åŸå§‹å¸§ä½œä¸ºå‚è€ƒ
                closest_idx = np.argmin(time_diffs)
                reference_frame = original_data_list[closest_idx]

                # åˆ›å»ºæ’å€¼å¸§ï¼ˆå¤åˆ¶æœ€è¿‘å¸§ï¼Œåªæ”¹æ—¶é—´æˆ³ï¼‰
                interpolated_frame = self._create_interpolated_data_point(
                    reference_frame, target_ts, data_type
                )

                aligned_data[i] = interpolated_frame
                created_count += 1

        if interpolated_count > 0:
            print(f"    [æŒ‰éœ€æ’å€¼] ä»åŸå§‹æ•°æ®é€‰æ‹©äº† {interpolated_count} å¸§")
        if created_count > 0:
            print(f"    [æŒ‰éœ€æ’å€¼] åˆ›å»ºäº† {created_count} ä¸ªæ’å€¼å¸§ï¼ˆå¤åˆ¶æœ€è¿‘å¸§ï¼‰")

        return aligned_data

    def _create_interpolated_data_point(
        self, reference_item: dict, new_timestamp: float, data_type: str
    ) -> dict:
        """åˆ›å»ºæ’å€¼æ•°æ®ç‚¹"""
        interpolated_item = reference_item.copy()
        interpolated_item["timestamp"] = new_timestamp

        # æ ¹æ®æ•°æ®ç±»å‹å¤„ç†æ•°æ®å­—æ®µ
        if data_type in ["video", "depth"]:
            # å›¾åƒæ•°æ®ï¼šå¤åˆ¶å‚è€ƒå¸§çš„æ•°æ®
            interpolated_item["interpolated"] = True
        elif data_type == "sensor":
            # ä¼ æ„Ÿå™¨æ•°æ®ï¼šä¿æŒç›¸åŒçš„æ•°å€¼ï¼ˆé›¶é˜¶ä¿æŒæ’å€¼ï¼‰
            interpolated_item["interpolated"] = True

        return interpolated_item

    def _validate_timestamp_quality(self, timestamps: np.ndarray, data_name: str):
        """éªŒè¯æ—¶é—´æˆ³è´¨é‡ï¼ˆä½¿ç”¨çº³ç§’ç²¾åº¦ï¼‰- å¢å¼ºç‰ˆæœ¬"""
        if len(timestamps) <= 1:
            return

        # è½¬æ¢ä¸ºçº³ç§’è¿›è¡Œç²¾ç¡®è®¡ç®—
        timestamps_ns = (timestamps * 1e9).astype(np.int64)
        time_diffs_ns = np.diff(timestamps_ns)
        time_diffs_ms = time_diffs_ns / 1e6  # è½¬æ¢ä¸ºæ¯«ç§’æ˜¾ç¤º

        # æ£€æŸ¥æ—¶é—´é—´éš”
        mean_interval_ms = np.mean(time_diffs_ms)
        max_interval_ms = np.max(time_diffs_ms)
        min_interval_ms = np.min(time_diffs_ms)
        std_interval_ms = np.std(time_diffs_ms)

        print(f"  {data_name} æ—¶é—´æˆ³è´¨é‡:")
        print(f"    å¹³å‡é—´éš”: {mean_interval_ms:.1f}ms")
        print(f"    æœ€å¤§é—´éš”: {max_interval_ms:.1f}ms")
        print(f"    æœ€å°é—´éš”: {min_interval_ms:.1f}ms")
        print(f"    æ ‡å‡†å·®: {std_interval_ms:.1f}ms")

        # ä¸¥æ ¼çš„è´¨é‡æ£€æŸ¥ - ä¿®æ”¹ä¸ºæ›´ä¸¥æ ¼çš„éªŒè¯
        critical_errors = []
        warnings = []

        if max_interval_ms > 40:  # 40msé˜ˆå€¼ - å…³é”®é”™è¯¯
            critical_errors.append(f"æœ€å¤§æ—¶é—´é—´éš”è¿‡å¤§: {max_interval_ms:.1f}ms")

        if min_interval_ms < 0.1:  # 0.1msé˜ˆå€¼ - å…³é”®é”™è¯¯
            critical_errors.append(f"æœ€å°æ—¶é—´é—´éš”è¿‡å°: {min_interval_ms:.1f}ms")

        if std_interval_ms > 15:  # 15msæ ‡å‡†å·®é˜ˆå€¼ - è­¦å‘Š
            warnings.append(f"æ—¶é—´é—´éš”æ³¢åŠ¨è¿‡å¤§: {std_interval_ms:.1f}ms")

        # æ£€æŸ¥é‡å¤æ—¶é—´æˆ³
        unique_timestamps = np.unique(timestamps_ns)
        if len(unique_timestamps) < len(timestamps_ns):
            duplicate_count = len(timestamps_ns) - len(unique_timestamps)
            critical_errors.append(f"ä»å­˜åœ¨ {duplicate_count} ä¸ªé‡å¤æ—¶é—´æˆ³")

        # è¾“å‡ºç»“æœ
        if critical_errors:
            print(f"    âŒ å…³é”®é”™è¯¯: {'; '.join(critical_errors)}")
            # å¯¹äºä¸»æ—¶é—´æˆ³çš„å…³é”®é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸
            if data_name == "ä¸»æ—¶é—´æˆ³":
                error_msg = (
                    f"{data_name} å­˜åœ¨å…³é”®è´¨é‡é—®é¢˜: {'; '.join(critical_errors)}"
                )
                raise TimestampStuckError(
                    message=error_msg,
                    topic=data_name,
                    stuck_timestamp=None,
                    stuck_duration=max_interval_ms / 1000,
                    stuck_frame_count=len(critical_errors),
                    threshold=0.04,
                )
        elif warnings:
            print(f"    âš ï¸  è­¦å‘Š: {'; '.join(warnings)}")
        else:
            print(f"    âœ“ æ—¶é—´æˆ³è´¨é‡è‰¯å¥½")

    def align_frame_data_optimized(
        self, data: dict, drop_head: bool, drop_tail: bool, action_config=None
    ):
        """ä¼˜åŒ–çš„æ—¶é—´æˆ³å¯¹é½å‡½æ•°ï¼ŒæŒ‰éœ€æ’å€¼ç­–ç•¥"""
        print("å¼€å§‹ä¼˜åŒ–ç‰ˆæœ¬çš„æ—¶é—´æˆ³å¯¹é½ï¼ˆæŒ‰éœ€æ’å€¼ï¼‰...")
        aligned_data = defaultdict(list)

        # 1. é¢„å¤„ç†ï¼šåªå»é‡å’Œæ£€æµ‹å¡é¡¿ï¼Œä¸æ’å€¼
        print("æ­¥éª¤1: é¢„å¤„ç†æ•°æ® - å»é‡å’Œæ£€æµ‹å¡é¡¿ï¼ˆéä¸»æ—¶é—´çº¿è·³è¿‡æ’å€¼ï¼‰")
        preprocessed_data = {}

        for key, data_list in data.items():
            if len(data_list) == 0:
                preprocessed_data[key] = []
                continue

            print(f"é¢„å¤„ç† {key}: åŸå§‹é•¿åº¦ {len(data_list)}")

            # å»é‡
            deduplicated_data = self._remove_duplicate_timestamps(data_list, key)

            # æ£€æµ‹å¡é¡¿
            self._check_actual_time_gaps(
                deduplicated_data, key, max_gap_duration=self.TIME_TOLERANCE
            )

            # ç‰¹æ®Šå¤„ç†ï¼šä¸»æ—¶é—´çº¿éœ€è¦æ’å€¼ï¼ˆå¡«è¡¥ä¸¢å¸§ï¼Œä¿è¯å®Œæ•´æ€§ï¼‰
            main_timeline = getattr(self, "MAIN_TIMESTAMP_TOPIC", "head_cam_h")
            if key == main_timeline:
                print(f"  [ä¸»æ—¶é—´çº¿] å¯¹ {key} è¿›è¡Œæ’å€¼ï¼Œå¡«è¡¥ä¸¢å¸§")
                interpolated_data = self._interpolate_timestamps_and_data(
                    deduplicated_data, key
                )
                preprocessed_data[key] = interpolated_data
                print(
                    f"é¢„å¤„ç† {key}: å»é‡å {len(deduplicated_data)}, æ’å€¼å {len(interpolated_data)} å¸§ï¼ˆä¸»æ—¶é—´çº¿ï¼‰"
                )
            else:
                # å…¶ä»–è¯é¢˜ï¼šåªå»é‡ï¼Œä¸æ’å€¼ï¼ˆæŒ‰éœ€å¯¹é½ï¼‰
                preprocessed_data[key] = deduplicated_data
                print(f"é¢„å¤„ç† {key}: å»é‡å {len(deduplicated_data)} å¸§ï¼ˆæœªæ’å€¼ï¼‰")

        print(f"[ä¼˜åŒ–] ä¸»æ—¶é—´çº¿å·²æ’å€¼ï¼Œå…¶ä»–è¯é¢˜ä¿ç•™åŸå§‹æ•°æ®")

        # 2. ç”Ÿæˆç»Ÿä¸€çš„ä¸»æ—¶é—´æˆ³åŸºå‡†
        main_timeline = getattr(self, "MAIN_TIMESTAMP_TOPIC", "head_cam_h")
        if (
            main_timeline not in preprocessed_data
            or len(preprocessed_data[main_timeline]) == 0
        ):
            main_timeline = max(
                self.DEFAULT_CAMERA_NAMES,
                key=lambda cam_k: len(preprocessed_data.get(cam_k, [])),
            )
            print(f"è­¦å‘Šï¼šä¸»æ—¶é—´æˆ³è¯é¢˜ä¸å­˜åœ¨ï¼Œä½¿ç”¨é™çº§è¯é¢˜: {main_timeline}")

        # 3. ç”Ÿæˆä¸»æ—¶é—´æˆ³åºåˆ—
        jump = self.MAIN_TIMELINE_FPS // self.TRAIN_HZ
        main_img_timestamps = [t["timestamp"] for t in preprocessed_data[main_timeline]]

        # æ ¹æ®ä¼ å…¥å‚æ•°è£å‰ªé¦–å°¾
        start_idx = self.SAMPLE_DROP if drop_head else 0
        end_idx = -self.SAMPLE_DROP if drop_tail else None
        main_img_timestamps = main_img_timestamps[start_idx:end_idx][::jump]

        # 4. æ—¶é—´æˆ³è¾¹ç•Œè¿‡æ»¤
        data_with_content = {k: v for k, v in preprocessed_data.items() if len(v) > 0}
        if not data_with_content:
            return aligned_data

        min_end = min([data[k][-1]["timestamp"] for k in data_with_content.keys()])
        main_img_timestamps = [t for t in main_img_timestamps if t < min_end]
        main_img_timestamps = np.array(main_img_timestamps)

        print(f"ä¸»æ—¶é—´çº¿: {main_timeline}, é¢„å¤„ç†åé•¿åº¦: {len(main_img_timestamps)}")

        # 5. å¤šæ¨¡æ€å¼€å¤´æ—¶é—´æˆ³ä¿®æ­£
        print("æ­¥éª¤5: å¤šæ¨¡æ€å¼€å¤´æ—¶é—´æˆ³ä¿®æ­£")
        main_img_timestamps = self._fix_multimodal_start_alignment(
            main_img_timestamps, preprocessed_data
        )

        # 6. éªŒè¯æ—¶é—´æˆ³è´¨é‡
        # self._validate_timestamp_quality(main_img_timestamps, "ä¸»æ—¶é—´æˆ³")

        # 7. å‘é‡åŒ–å¯¹é½å¤„ç†ï¼ˆå…ˆç”¨åŸå§‹æ•°æ®å¯¹é½ï¼‰
        print("æ­¥éª¤7: å¯¹é½å¤„ç†ï¼ˆå…ˆç”¨åŸå§‹æ•°æ®ï¼‰")
        for key, data_list in preprocessed_data.items():
            if len(data_list) == 0:
                aligned_data[key] = []
                continue

            timestamps = np.array([frame["timestamp"] for frame in data_list])
            closest_indices = self.find_closest_indices_vectorized(
                timestamps, main_img_timestamps
            )
            aligned_data[key] = [data_list[idx] for idx in closest_indices]

            # éªŒè¯å¯¹é½è´¨é‡
            aligned_timestamps = timestamps[closest_indices]
            time_errors_ms = np.abs(aligned_timestamps - main_img_timestamps) * 1000
            max_diff = np.max(time_errors_ms)
            mean_diff = np.mean(time_errors_ms)

            # ç»Ÿè®¡è¯¯å·®åˆ†å¸ƒ
            errors_gt_10ms = np.sum(time_errors_ms > 10)
            errors_gt_15ms = np.sum(time_errors_ms > 15)
            errors_gt_20ms = np.sum(time_errors_ms > 20)

            print(f"  {key}: å¯¹é½å®Œæˆ {len(aligned_data[key])} å¸§")
            print(f"    æ—¶é—´æˆ³è¯¯å·®: å¹³å‡ {mean_diff:.1f}ms, æœ€å¤§ {max_diff:.1f}ms")
            print(
                f"    è¯¯å·®åˆ†å¸ƒ: >10ms={errors_gt_10ms}, >15ms={errors_gt_15ms}, >20ms={errors_gt_20ms}"
            )

            # æŒ‰éœ€æ’å€¼ï¼šåªå¯¹è¯¯å·® >10ms çš„å¸§è¿›è¡Œæ’å€¼ä¿®æ­£
            if errors_gt_10ms > 0:
                print(
                    f"    [æŒ‰éœ€æ’å€¼] å‘ç° {errors_gt_10ms} å¸§è¯¯å·® >10msï¼Œè¿›è¡Œæ’å€¼ä¿®æ­£"
                )
                aligned_data[key] = self._interpolate_on_demand(
                    aligned_data[key],
                    time_errors_ms,
                    data_list,
                    timestamps,
                    main_img_timestamps,
                    key,
                )
            else:
                print(f"    âœ… æ‰€æœ‰å¸§è¯¯å·® <10msï¼Œæ— éœ€æ’å€¼")
        # === æ–°å¢æ­¥éª¤8: é™æ­¢åŒºåŸŸæ£€æµ‹å’Œè£å‰ª ===
        print("æ­¥éª¤8: é™æ­¢åŒºåŸŸæ£€æµ‹å’Œè£å‰ª")
        # aligned_data, main_img_timestamps = self._detect_and_trim_aligned_data(aligned_data, main_img_timestamps,action_config=action_config)
        # === æ–°å¢æ­¥éª¤8: å¸§ç‡è°ƒæ•´åˆ°30fps ===
        print("æ­¥éª¤8: å¸§ç‡è°ƒæ•´åˆ°30fps")
        # aligned_data, main_img_timestamps = self._adjust_frame_rate_to_30fps(aligned_data, main_img_timestamps)

        # print("æ­¥éª¤9: æœ€ç»ˆéªŒè¯å¯¹é½è´¨é‡")
        # self._final_alignment_validation(aligned_data, main_img_timestamps)

        return aligned_data

    def _detect_and_trim_aligned_data(
        self, aligned_data: dict, main_timestamps: np.ndarray, action_config=None
    ):
        """
        æ£€æµ‹å¹¶è£å‰ªå¯¹é½åæ•°æ®ä¸­çš„é™æ­¢åŒºåŸŸï¼Œå¤´å°¾è£å‰ªä¸Šé™ç”±é¦–å°¾åŠ¨ä½œæŒç»­å¸§æ•°çš„ä¸€åŠå†³å®š
        """
        from slave_utils import (
            detect_stillness_from_image_data,
            analyze_stillness_frames,
        )

        motion_threshold = 4.5
        stillness_ratio = 1
        check_duration = 10.0
        fps = self.TRAIN_HZ or 30

        camera_keys = [
            c
            for c in self.DEFAULT_CAMERA_NAMES
            if c in aligned_data and len(aligned_data[c]) > 0
        ]
        if not camera_keys:
            print("  æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç›¸æœºæ•°æ®ï¼Œè·³è¿‡é™æ­¢æ£€æµ‹")
            return aligned_data, main_timestamps

        print(f"  åŸºäº {len(camera_keys)} ä¸ªç›¸æœºæ£€æµ‹é™æ­¢åŒºåŸŸ: {camera_keys}")

        # === è®¡ç®—é¦–å°¾åŠ¨ä½œçš„æŒç»­å¸§æ•° ===
        max_head_trim_limit = None
        max_tail_trim_limit = None
        total_frames = len(main_timestamps)
        if action_config and len(action_config) > 0:
            # moments.jsonæ ¼å¼ï¼Œéœ€ä»customFieldValuesä¸­æå–start_positionå’Œend_position
            first_action = None
            last_action = None
            min_start = None
            max_end = None
            for act in action_config:
                custom_fields = act.get("customFieldValues", {})
                try:
                    sp = float(custom_fields.get("start_position", None))
                    if min_start is None or sp < min_start:
                        min_start = sp
                        first_action = act
                except Exception:
                    pass
                try:
                    ep = float(custom_fields.get("end_position", None))
                    if max_end is None or ep > max_end:
                        max_end = ep
                        last_action = act
                except Exception:
                    pass

            # è®¡ç®—å¸§åŒºé—´ï¼ˆç›´æ¥ç”¨æ¯”ä¾‹ä¹˜ä»¥æ€»å¸§æ•°ï¼‰
            if first_action is not None and last_action is not None:
                first_sp = float(first_action["customFieldValues"]["start_position"])
                first_ep = float(first_action["customFieldValues"]["end_position"])
                last_sp = float(last_action["customFieldValues"]["start_position"])
                last_ep = float(last_action["customFieldValues"]["end_position"])

                # æŒ‰æ¯”ä¾‹æ˜ å°„åˆ°å¸§ç´¢å¼•
                first_start_idx = int(
                    round(
                        (first_sp - min_start)
                        / (max_end - min_start)
                        * (total_frames - 1)
                    )
                )
                first_end_idx = int(
                    round(
                        (first_ep - min_start)
                        / (max_end - min_start)
                        * (total_frames - 1)
                    )
                )
                last_start_idx = int(
                    round(
                        (last_sp - min_start)
                        / (max_end - min_start)
                        * (total_frames - 1)
                    )
                )
                last_end_idx = int(
                    round(
                        (last_ep - min_start)
                        / (max_end - min_start)
                        * (total_frames - 1)
                    )
                )

                first_len = max(0, first_end_idx - first_start_idx)
                last_len = max(0, last_end_idx - last_start_idx)
                max_head_trim_limit = max(0, int(first_len / 2))
                max_tail_trim_limit = max(0, int(last_len / 2))
                print(
                    f"  é¦–åŠ¨ä½œå¸§åŒºé—´: {first_start_idx}-{first_end_idx}ï¼Œé•¿åº¦: {first_len}"
                )
                print(
                    f"  å°¾åŠ¨ä½œå¸§åŒºé—´: {last_start_idx}-{last_end_idx}ï¼Œé•¿åº¦: {last_len}"
                )
                print(
                    f"  é¦–åŠ¨ä½œæœ€å¤§è£å‰ªä¸Šé™: {max_head_trim_limit} å¸§ï¼Œå°¾åŠ¨ä½œæœ€å¤§è£å‰ªä¸Šé™: {max_tail_trim_limit} å¸§"
                )
            else:
                print("  æœªæ‰¾åˆ°æœ‰æ•ˆçš„åŠ¨ä½œé¦–å°¾è£å‰ªä¸Šé™")
        else:
            print("  æœªæ‰¾åˆ°æœ‰æ•ˆçš„åŠ¨ä½œé¦–å°¾è£å‰ªä¸Šé™")

        # === é™æ­¢æ£€æµ‹ ===
        all_stillness_results = {}
        for camera_key in camera_keys:
            frames_data = aligned_data[camera_key]
            print(f"  åˆ†æ {camera_key}: æ€»å¸§æ•° {len(frames_data)}")
            head_stillness, tail_stillness = detect_stillness_from_image_data(
                frames_data,
                camera_key,
                motion_threshold,
                stillness_ratio,
                check_duration,
                fps,
            )
            all_stillness_results[camera_key] = {
                "head_frames": head_stillness,
                "tail_frames": tail_stillness,
            }
            print(
                f"    {camera_key}: å¼€å¤´é™æ­¢ {head_stillness} å¸§, ç»“å°¾é™æ­¢ {tail_stillness} å¸§"
            )

        # è®¡ç®—æœ€ç»ˆè£å‰ªå¸§æ•°ï¼ˆå–æ‰€æœ‰ç›¸æœºçš„æœ€å¤§å€¼ç¡®ä¿ä¸€è‡´æ€§ï¼‰
        if all_stillness_results:
            max_head_trim = max(
                result["head_frames"] for result in all_stillness_results.values()
            )
            max_tail_trim = max(
                result["tail_frames"] for result in all_stillness_results.values()
            )
        else:
            max_head_trim = 0
            max_tail_trim = 0

        # === åº”ç”¨é¦–å°¾è£å‰ªä¸Šé™ ===
        if max_head_trim_limit is not None:
            if max_head_trim > max_head_trim_limit:
                print(
                    f"  å¼€å¤´é™æ­¢è£å‰ªå¸§æ•° {max_head_trim} è¶…è¿‡é¦–åŠ¨ä½œä¸Šé™ {max_head_trim_limit}ï¼Œå·²è¦†ç›–"
                )
                max_head_trim = max_head_trim_limit
        if max_tail_trim_limit is not None:
            if max_tail_trim > max_tail_trim_limit:
                print(
                    f"  ç»“å°¾é™æ­¢è£å‰ªå¸§æ•° {max_tail_trim} è¶…è¿‡å°¾åŠ¨ä½œä¸Šé™ {max_tail_trim_limit}ï¼Œå·²è¦†ç›–"
                )
                max_tail_trim = max_tail_trim_limit

        print(f"  æœ€ç»ˆè£å‰ªå†³å®š: å¼€å¤´ {max_head_trim} å¸§, ç»“å°¾ {max_tail_trim} å¸§")

        # åº”ç”¨è£å‰ªåˆ°æ‰€æœ‰æ•°æ®
        if max_head_trim > 0 or max_tail_trim > 0:
            trimmed_aligned_data, trimmed_main_timestamps = (
                self._trim_aligned_data_by_frames(
                    aligned_data, main_timestamps, max_head_trim, max_tail_trim
                )
            )
            print(
                f"  è£å‰ªå®Œæˆ: ä¸»æ—¶é—´æˆ³ {len(main_timestamps)} -> {len(trimmed_main_timestamps)} å¸§"
            )
            return trimmed_aligned_data, trimmed_main_timestamps
        else:
            print("  æ— éœ€è£å‰ª")
            return aligned_data, main_timestamps

    def _trim_aligned_data_by_frames(
        self,
        aligned_data: dict,
        main_timestamps: np.ndarray,
        head_trim_frames: int,
        tail_trim_frames: int,
    ):
        """æŒ‰å¸§æ•°è£å‰ªå¯¹é½åçš„æ•°æ®"""
        trimmed_data = {}

        # è£å‰ªä¸»æ—¶é—´æˆ³
        original_length = len(main_timestamps)
        start_idx = head_trim_frames
        if tail_trim_frames > 0:
            end_idx = original_length - tail_trim_frames
        else:
            end_idx = original_length

        # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
        start_idx = max(0, start_idx)
        end_idx = min(original_length, end_idx)

        if start_idx < end_idx:
            trimmed_main_timestamps = main_timestamps[start_idx:end_idx]
        else:
            trimmed_main_timestamps = np.array([])
            print("    è­¦å‘Š: ä¸»æ—¶é—´æˆ³è£å‰ªåä¸ºç©º")

        # è£å‰ªæ‰€æœ‰å¯¹é½åçš„æ•°æ®
        for key, data_list in aligned_data.items():
            if isinstance(data_list, list) and len(data_list) > 0:
                original_data_length = len(data_list)

                if start_idx < end_idx and start_idx < original_data_length:
                    actual_end_idx = min(end_idx, original_data_length)
                    trimmed_data[key] = data_list[start_idx:actual_end_idx]
                    print(
                        f"    {key}: {original_data_length} -> {len(trimmed_data[key])} (-{original_data_length - len(trimmed_data[key])})"
                    )
                else:
                    trimmed_data[key] = []
                    print(f"    è­¦å‘Š: {key} è£å‰ªåä¸ºç©º")
            else:
                # éåˆ—è¡¨æ•°æ®æˆ–ç©ºæ•°æ®ä¿æŒä¸å˜
                trimmed_data[key] = data_list

        return trimmed_data, trimmed_main_timestamps

    def _fix_multimodal_start_alignment(
        self, main_timestamps: np.ndarray, preprocessed_data: dict
    ) -> np.ndarray:
        """ä¿®æ­£å¤šæ¨¡æ€å¼€å¤´æ—¶é—´æˆ³åå·®é—®é¢˜ - ä½¿ç”¨ä¸æœ€ç»ˆéªŒè¯ä¸€è‡´çš„é€»è¾‘"""
        if len(main_timestamps) == 0:
            return main_timestamps

        # è¯†åˆ«æ‰€æœ‰æœ‰æ•ˆçš„æ•°æ®æ¨¡æ€ï¼ˆæ’é™¤å¤–å‚æ•°æ®å’Œç©ºæ•°æ®ï¼‰
        valid_keys = []
        for key in preprocessed_data.keys():
            if (
                not key.endswith("_extrinsics")
                and not key.endswith("_camera_info")
                and len(preprocessed_data[key]) > 0
            ):
                valid_keys.append(key)

        if len(valid_keys) <= 1:
            print("  âœ“ åªæœ‰ä¸€ä¸ªæˆ–é›¶ä¸ªæ•°æ®æ¨¡æ€ï¼Œæ— éœ€ä¿®æ­£")
            return main_timestamps

        print(f"  æ£€æŸ¥ {len(valid_keys)} ä¸ªæ•°æ®æ¨¡æ€çš„å¼€å¤´å¯¹é½æƒ…å†µ")

        # åˆ†ææ¯ä¸ªæ•°æ®æ¨¡æ€çš„å¼€å¤´æ—¶åˆ»æ¨¡æ€é—´æœ€å¤§æœ€å°å·®å€¼
        alignment_info = []
        max_alignment_tolerance_ms = 20  # 20mså®¹å·®
        severe_stuck_threshold_ms = 1000  # 1ç§’é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯ä¸¥é‡å¡ä½

        # å…ˆè¿›è¡Œå‘é‡åŒ–å¯¹é½ï¼Œè·å–å¯¹é½åçš„æ—¶é—´æˆ³
        aligned_timestamps_by_key = {}
        for key in valid_keys:
            timestamps = np.array(
                [item["timestamp"] for item in preprocessed_data[key]]
            )

            # æ£€æŸ¥å‰5å¸§çš„å¯¹é½æƒ…å†µ
            check_frames = min(5, len(main_timestamps), len(timestamps))
            if check_frames == 0:
                continue

            main_subset = main_timestamps[:check_frames]

            # æ‰¾åˆ°æœ€è¿‘çš„å¯¹é½ç´¢å¼•
            closest_indices = self.find_closest_indices_vectorized(
                timestamps, main_subset
            )
            aligned_timestamps = timestamps[closest_indices]

            aligned_timestamps_by_key[key] = aligned_timestamps

        # é€å¸§æ£€æŸ¥å¼€å¤´å‡ å¸§çš„æ¨¡æ€é—´æ—¶é—´æˆ³å·®å€¼
        check_frames = min(5, len(main_timestamps))
        frame_spreads = []
        severely_stuck_keys = []

        for frame_idx in range(check_frames):
            # æ”¶é›†è¯¥å¸§æ‰€æœ‰æ¨¡æ€çš„æ—¶é—´æˆ³
            frame_timestamps = []
            frame_keys = []

            for key in valid_keys:
                if key in aligned_timestamps_by_key and frame_idx < len(
                    aligned_timestamps_by_key[key]
                ):
                    frame_timestamps.append(aligned_timestamps_by_key[key][frame_idx])
                    frame_keys.append(key)

            if len(frame_timestamps) > 1:
                # è®¡ç®—è¯¥å¸§æ‰€æœ‰æ¨¡æ€æ—¶é—´æˆ³çš„æœ€å¤§æœ€å°å·®å€¼
                frame_timestamps = np.array(frame_timestamps)
                min_ts = np.min(frame_timestamps)
                max_ts = np.max(frame_timestamps)
                spread_ms = (max_ts - min_ts) * 1000

                frame_spreads.append(
                    {
                        "frame_idx": frame_idx,
                        "spread_ms": spread_ms,
                        "timestamps": frame_timestamps,
                        "keys": frame_keys,
                        "main_timestamp": main_timestamps[frame_idx],
                    }
                )

        # åˆ†æå¼€å¤´å¯¹é½è´¨é‡
        if frame_spreads:
            max_spread = max(spread["spread_ms"] for spread in frame_spreads)
            avg_spread = np.mean([spread["spread_ms"] for spread in frame_spreads])

            print(f"    å¼€å¤´{len(frame_spreads)}å¸§æ¨¡æ€é—´æ—¶é—´æˆ³å·®å€¼åˆ†æ:")
            print(f"      æœ€å¤§å·®å€¼: {max_spread:.1f}ms")
            print(f"      å¹³å‡å·®å€¼: {avg_spread:.1f}ms")

            # æ˜¾ç¤ºæ¯å¸§çš„è¯¦ç»†æƒ…å†µ
            for spread_info in frame_spreads:
                frame_idx = spread_info["frame_idx"]
                spread_ms = spread_info["spread_ms"]
                if spread_ms > max_alignment_tolerance_ms:
                    print(f"      å¸§{frame_idx}: å·®å€¼ {spread_ms:.1f}ms (è¶…è¿‡é˜ˆå€¼)")

                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡å¡ä½çš„æ¨¡æ€
                    timestamps = spread_info["timestamps"]
                    keys = spread_info["keys"]
                    main_ts = spread_info["main_timestamp"]

                    for i, (ts, key) in enumerate(zip(timestamps, keys)):
                        diff_ms = abs(ts - main_ts) * 1000
                        if diff_ms > severe_stuck_threshold_ms:
                            if key not in severely_stuck_keys:
                                severely_stuck_keys.append(key)
                                print(
                                    f"        {key}: ä¸ä¸»æ—¶é—´æˆ³åå·® {diff_ms:.1f}ms (ä¸¥é‡å¡ä½)"
                                )
                else:
                    print(f"      å¸§{frame_idx}: å·®å€¼ {spread_ms:.1f}ms (æ­£å¸¸)")

        # è¯†åˆ«ä¸åŒç±»å‹çš„é—®é¢˜æ¨¡æ€
        problematic_frames = [
            s for s in frame_spreads if s["spread_ms"] > max_alignment_tolerance_ms
        ]

        # å¤„ç†ä¸¥é‡å¡ä½çš„æ•°æ®æ¨¡æ€
        if severely_stuck_keys:
            print(
                f"  å‘ç° {len(severely_stuck_keys)} ä¸ªä¸¥é‡å¡ä½çš„æ•°æ®æ¨¡æ€ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†:"
            )

            # æŒ‰æ¨¡æ€ç±»å‹åˆ†ç±»æ˜¾ç¤º
            severely_stuck_by_type = {}
            for key in severely_stuck_keys:
                if any(cam in key for cam in ["head_cam", "wrist_cam"]):
                    modality_type = "ç›¸æœº"
                elif "action." in key:
                    modality_type = "åŠ¨ä½œ"
                elif "observation." in key:
                    modality_type = "è§‚æµ‹"
                else:
                    modality_type = "å…¶ä»–"

                if modality_type not in severely_stuck_by_type:
                    severely_stuck_by_type[modality_type] = []
                severely_stuck_by_type[modality_type].append(key)

            print("  ä¸¥é‡å¡ä½æ¨¡æ€åˆ†å¸ƒ:")
            for mod_type, keys in severely_stuck_by_type.items():
                print(f"    {mod_type}: {len(keys)} ä¸ª - {keys}")

            # å¯¹ä¸¥é‡å¡ä½çš„æ•°æ®æ¨¡æ€è¿›è¡Œæ—¶é—´æˆ³æ›¿æ¢
            for key in severely_stuck_keys:
                self._fix_severely_stuck_timestamps(
                    preprocessed_data, key, main_timestamps, max_alignment_tolerance_ms
                )

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¸¸è§„ä¿®æ­£ï¼ˆæ’é™¤å·²å¤„ç†çš„ä¸¥é‡å¡ä½æ•°æ®ï¼‰
        if not problematic_frames or len(severely_stuck_keys) == len(valid_keys):
            if severely_stuck_keys:
                print("  âœ“ ä¸¥é‡å¡ä½æ•°æ®å·²å¤„ç†ï¼Œå…¶ä»–æ¨¡æ€å¼€å¤´å¯¹é½è‰¯å¥½")
            else:
                print("  âœ“ æ‰€æœ‰æ¨¡æ€å¼€å¤´å¯¹é½è‰¯å¥½ï¼Œæ— éœ€ä¿®æ­£")
            return main_timestamps

        print(
            f"  å‘ç°å¼€å¤´ {len(problematic_frames)} å¸§å­˜åœ¨æ¨¡æ€é—´å¯¹é½åå·®è¿‡å¤§ï¼Œå¼€å§‹ä¿®æ­£..."
        )

        # ç»Ÿè®¡é—®é¢˜æ¨¡æ€ï¼ˆæ’é™¤ä¸¥é‡å¡ä½çš„ï¼‰
        normal_problematic_keys = set()
        for spread_info in problematic_frames:
            for key in spread_info["keys"]:
                if key not in severely_stuck_keys:
                    normal_problematic_keys.add(key)

        if normal_problematic_keys:
            # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º
            problematic_by_type = {}
            for key in normal_problematic_keys:
                if any(cam in key for cam in ["head_cam", "wrist_cam"]):
                    modality_type = "ç›¸æœº"
                elif "action." in key:
                    modality_type = "åŠ¨ä½œ"
                elif "observation." in key:
                    modality_type = "è§‚æµ‹"
                else:
                    modality_type = "å…¶ä»–"

                if modality_type not in problematic_by_type:
                    problematic_by_type[modality_type] = []
                problematic_by_type[modality_type].append(key)

            print("  é—®é¢˜æ¨¡æ€åˆ†å¸ƒ:")
            for mod_type, keys in problematic_by_type.items():
                print(f"    {mod_type}: {len(keys)} ä¸ª - {keys}")

        # ç­–ç•¥ï¼šæ‰¾åˆ°æ‰€æœ‰æ­£å¸¸æ•°æ®æ¨¡æ€éƒ½èƒ½è‰¯å¥½å¯¹é½çš„æ—¶é—´èŒƒå›´
        best_start_idx = 0
        min_max_spread = float("inf")

        # åœ¨å‰20å¸§ä¸­å¯»æ‰¾æœ€ä½³èµ·å§‹ç‚¹
        search_range = min(20, len(main_timestamps))

        # åªè€ƒè™‘éä¸¥é‡å¡ä½çš„æ•°æ®æ¨¡æ€
        normal_valid_keys = [
            key for key in valid_keys if key not in severely_stuck_keys
        ]

        for start_candidate in range(search_range):
            if start_candidate >= len(main_timestamps):
                break

            candidate_timestamps = main_timestamps[start_candidate:]
            if len(candidate_timestamps) < 10:  # è‡³å°‘ä¿ç•™10å¸§
                break

            # æ£€æŸ¥ä»è¿™ä¸ªèµ·å§‹ç‚¹å¼€å§‹çš„å¯¹é½æƒ…å†µ
            check_frames_candidate = min(5, len(candidate_timestamps))
            candidate_subset = candidate_timestamps[:check_frames_candidate]

            # è®¡ç®—è¿™ä¸ªèµ·å§‹ç‚¹çš„æ¨¡æ€é—´æœ€å¤§å·®å€¼
            max_spread_at_this_start = 0
            valid_alignment = True

            for frame_idx in range(check_frames_candidate):
                # æ”¶é›†è¯¥å¸§æ‰€æœ‰æ­£å¸¸æ¨¡æ€çš„æ—¶é—´æˆ³
                frame_timestamps = []

                for key in normal_valid_keys:
                    timestamps = np.array(
                        [item["timestamp"] for item in preprocessed_data[key]]
                    )

                    # æ‰¾åˆ°èƒ½è¦†ç›–å€™é€‰æ—¶é—´æˆ³çš„æ•°æ®
                    valid_indices = np.where(timestamps >= candidate_subset[0])[0]
                    if len(valid_indices) < len(candidate_subset):
                        valid_alignment = False
                        break

                    closest_indices = self.find_closest_indices_vectorized(
                        timestamps, candidate_subset
                    )
                    if frame_idx < len(closest_indices) and closest_indices[
                        frame_idx
                    ] < len(timestamps):
                        frame_timestamps.append(timestamps[closest_indices[frame_idx]])

                if not valid_alignment:
                    break

                if len(frame_timestamps) > 1:
                    frame_timestamps = np.array(frame_timestamps)
                    spread_ms = (
                        np.max(frame_timestamps) - np.min(frame_timestamps)
                    ) * 1000
                    max_spread_at_this_start = max(max_spread_at_this_start, spread_ms)

            if valid_alignment and max_spread_at_this_start < min_max_spread:
                min_max_spread = max_spread_at_this_start
                best_start_idx = start_candidate

            # å¦‚æœæ‰¾åˆ°äº†å¾ˆå¥½çš„å¯¹é½ç‚¹ï¼Œæå‰é€€å‡º
            if min_max_spread <= max_alignment_tolerance_ms:
                break

        # åº”ç”¨ä¿®æ­£
        if best_start_idx > 0:
            original_length = len(main_timestamps)
            main_timestamps = main_timestamps[best_start_idx:]
            removed_frames = original_length - len(main_timestamps)

            worst_before = max(spread["spread_ms"] for spread in problematic_frames)

            print(f"  âœ“ ä¿®æ­£å®Œæˆï¼šç§»é™¤å¼€å¤´ {removed_frames} å¸§")
            print(f"    æœ€å¤§æ¨¡æ€é—´å·®å€¼: {worst_before:.1f}ms -> {min_max_spread:.1f}ms")
            print(f"    ä¿®æ­£åä¸»æ—¶é—´æˆ³é•¿åº¦: {len(main_timestamps)}")

            # é‡æ–°éªŒè¯ä¿®æ­£æ•ˆæœ
            print("  éªŒè¯ä¿®æ­£æ•ˆæœ:")
            check_frames_verify = min(3, len(main_timestamps))

            for frame_idx in range(check_frames_verify):
                frame_timestamps = []
                frame_keys = []

                for key in normal_valid_keys[:5]:  # åªéªŒè¯å‰5ä¸ªæ¨¡æ€
                    timestamps = np.array(
                        [item["timestamp"] for item in preprocessed_data[key]]
                    )
                    closest_indices = self.find_closest_indices_vectorized(
                        timestamps, main_timestamps[:check_frames_verify]
                    )
                    if frame_idx < len(closest_indices) and closest_indices[
                        frame_idx
                    ] < len(timestamps):
                        frame_timestamps.append(timestamps[closest_indices[frame_idx]])
                        frame_keys.append(key)

                if len(frame_timestamps) > 1:
                    frame_timestamps = np.array(frame_timestamps)
                    spread_ms = (
                        np.max(frame_timestamps) - np.min(frame_timestamps)
                    ) * 1000
                    print(f"    å¸§{frame_idx}: ä¿®æ­£åæ¨¡æ€é—´å·®å€¼ {spread_ms:.1f}ms")

            if len(normal_valid_keys) > 5:
                print(f"    ... å…¶ä½™ {len(normal_valid_keys) - 5} ä¸ªæ­£å¸¸æ¨¡æ€ä¹Ÿå·²ä¿®æ­£")

        else:
            print(f"  âš ï¸ æ— æ³•æ‰¾åˆ°æ»¡æ„çš„ä¿®æ­£æ–¹æ¡ˆï¼Œä¿æŒåŸå§‹æ—¶é—´æˆ³")
            print(f"  å»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡ï¼Œå½“å‰æœ€å°æ¨¡æ€é—´æœ€å¤§å·®å€¼: {min_max_spread:.1f}ms")

        return main_timestamps

    def _fix_severely_stuck_timestamps(
        self,
        preprocessed_data: dict,
        key: str,
        main_timestamps: np.ndarray,
        tolerance_ms: float = 20,
    ):
        """ä¿®å¤ä¸¥é‡å¡ä½çš„æ•°æ®æ¨¡æ€çš„æ—¶é—´æˆ³"""
        print(f"  å¼€å§‹ä¿®å¤ä¸¥é‡å¡ä½çš„æ•°æ®æ¨¡æ€: {key}")

        data_list = preprocessed_data[key]
        if len(data_list) == 0:
            return

        # è·å–åŸå§‹æ—¶é—´æˆ³
        original_timestamps = np.array([item["timestamp"] for item in data_list])

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ­£å¸¸æ—¶é—´æˆ³çš„ä½ç½®
        normal_start_index = None
        for i in range(len(original_timestamps)):
            if i < len(main_timestamps):
                main_ts = main_timestamps[i]
                data_ts = original_timestamps[i]
                diff_ms = abs(data_ts - main_ts) * 1000

                if diff_ms <= tolerance_ms:
                    normal_start_index = i
                    print(f"    åœ¨ç´¢å¼• {i} å¤„æ‰¾åˆ°æ­£å¸¸æ—¶é—´æˆ³ï¼Œåå·® {diff_ms:.1f}ms")
                    break

        if normal_start_index is None:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ­£å¸¸çš„æ—¶é—´æˆ³ï¼Œå¯»æ‰¾æ•°æ®å¼€å§‹å˜åŒ–çš„ä½ç½®
            print(f"    æœªæ‰¾åˆ°æ­£å¸¸æ—¶é—´æˆ³ï¼Œå¯»æ‰¾æ•°æ®å¼€å§‹å˜åŒ–çš„ä½ç½®...")

            # å¯»æ‰¾æ—¶é—´æˆ³å¼€å§‹æ˜æ˜¾å˜åŒ–çš„ä½ç½®
            for i in range(1, min(len(original_timestamps), len(main_timestamps))):
                time_change = abs(original_timestamps[i] - original_timestamps[0])
                if time_change > 1.0:  # æ—¶é—´æˆ³å˜åŒ–è¶…è¿‡1ç§’
                    # æ£€æŸ¥è¿™ä¸ªä½ç½®æ˜¯å¦èƒ½ä¸ä¸»æ—¶é—´æˆ³å¯¹é½
                    expected_main_ts = (
                        main_timestamps[i]
                        if i < len(main_timestamps)
                        else main_timestamps[-1]
                        + (i - len(main_timestamps) + 1) * 0.033
                    )
                    diff_ms = abs(original_timestamps[i] - expected_main_ts) * 1000

                    if diff_ms <= tolerance_ms * 5:  # æ”¾å®½5å€å®¹å·®
                        normal_start_index = i
                        print(
                            f"    åœ¨ç´¢å¼• {i} å¤„æ‰¾åˆ°æ•°æ®å˜åŒ–ç‚¹ï¼Œå¼€å§‹æ­£å¸¸åŒæ­¥ï¼Œåå·® {diff_ms:.1f}ms"
                        )
                        break

        # æ‰§è¡Œæ—¶é—´æˆ³æ›¿æ¢
        replaced_count = 0
        if normal_start_index is not None and normal_start_index > 0:
            # ä»å¼€å¤´åˆ°normal_start_indexï¼Œä½¿ç”¨ä¸»æ—¶é—´æˆ³æ›¿æ¢
            for i in range(min(normal_start_index, len(main_timestamps))):
                if i < len(data_list):
                    old_timestamp = data_list[i]["timestamp"]
                    new_timestamp = main_timestamps[i]
                    data_list[i]["timestamp"] = new_timestamp
                    data_list[i]["timestamp_replaced"] = True
                    data_list[i]["original_timestamp"] = old_timestamp
                    replaced_count += 1

            print(f"    âœ“ æ›¿æ¢äº†å‰ {replaced_count} ä¸ªæ—¶é—´æˆ³")
            print(f"    ä»ç´¢å¼• {normal_start_index} å¼€å§‹ä½¿ç”¨åŸå§‹æ—¶é—´æˆ³")

        else:
            # å¦‚æœå§‹ç»ˆæ— æ³•åŒæ­¥ï¼Œæ›¿æ¢æ›´å¤šçš„å¼€å¤´æ—¶é—´æˆ³
            max_replace_count = min(
                50, len(data_list), len(main_timestamps)
            )  # æœ€å¤šæ›¿æ¢50å¸§

            print(f"    æ— æ³•æ‰¾åˆ°åŒæ­¥ç‚¹ï¼Œå¼ºåˆ¶æ›¿æ¢å‰ {max_replace_count} ä¸ªæ—¶é—´æˆ³")

            for i in range(max_replace_count):
                if i < len(data_list):
                    old_timestamp = data_list[i]["timestamp"]
                    new_timestamp = main_timestamps[i]
                    data_list[i]["timestamp"] = new_timestamp
                    data_list[i]["timestamp_replaced"] = True
                    data_list[i]["original_timestamp"] = old_timestamp
                    replaced_count += 1

            print(f"    âš ï¸ å¼ºåˆ¶æ›¿æ¢äº†å‰ {replaced_count} ä¸ªæ—¶é—´æˆ³")

        # éªŒè¯ä¿®å¤æ•ˆæœ
        print(f"  éªŒè¯ {key} ä¿®å¤æ•ˆæœ:")
        new_timestamps = np.array([item["timestamp"] for item in data_list])
        check_frames = min(5, len(main_timestamps), len(new_timestamps))

        if check_frames > 0:
            main_subset = main_timestamps[:check_frames]
            data_subset = new_timestamps[:check_frames]
            time_diffs_ms = np.abs(data_subset - main_subset) * 1000
            max_diff = np.max(time_diffs_ms)
            avg_diff = np.mean(time_diffs_ms)

            print(
                f"    ä¿®å¤åå¼€å¤´{check_frames}å¸§: æœ€å¤§åå·® {max_diff:.1f}ms, å¹³å‡åå·® {avg_diff:.1f}ms"
            )

            if max_diff <= tolerance_ms:
                print(f"    âœ“ {key} ä¿®å¤æˆåŠŸï¼Œå¼€å¤´åå·®å·²æ§åˆ¶åœ¨ {tolerance_ms}ms å†…")
            else:
                print(f"    âš ï¸ {key} ä¿®å¤åä»æœ‰åå·®ï¼Œä½†å·²æ˜¾è‘—æ”¹å–„")

        # æ›´æ–°é¢„å¤„ç†æ•°æ®
        preprocessed_data[key] = data_list

    def _adjust_frame_rate_to_30fps(
        self, aligned_data: dict, main_timestamps: np.ndarray
    ):
        """
        è°ƒæ•´å¸§ç‡åˆ°30fpsèŒƒå›´å†…ï¼ˆ29.95-30.05Hzï¼‰
        é€šè¿‡æ’å¸§æˆ–æŠ½å¸§æ¥è¾¾åˆ°ç›®æ ‡å¸§ç‡
        """
        print("=" * 60)
        print("å¼€å§‹å¸§ç‡è°ƒæ•´åˆ°30fps...")

        if len(main_timestamps) < 2:
            print("  âš ï¸ æ—¶é—´æˆ³æ•°é‡ä¸è¶³ï¼Œè·³è¿‡å¸§ç‡è°ƒæ•´")
            return aligned_data, main_timestamps

        # è®¡ç®—å½“å‰å¸§ç‡
        time_span = main_timestamps[-1] - main_timestamps[0]
        current_fps = len(main_timestamps) / time_span
        target_fps_min = 29.905
        target_fps_max = 30.095

        print(f"  å½“å‰å¸§ç‡: {current_fps:.2f}Hz")
        print(f"  ç›®æ ‡èŒƒå›´: {target_fps_min:.2f}-{target_fps_max:.2f}Hz")
        print(f"  å½“å‰å¸§æ•°: {len(main_timestamps)}")
        print(f"  æ—¶é—´è·¨åº¦: {time_span:.3f}s")

        # æ£€æŸ¥æ˜¯å¦åœ¨ç›®æ ‡èŒƒå›´å†…
        if target_fps_min <= current_fps <= target_fps_max:
            print(f"  âœ“ å¸§ç‡å·²åœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œæ— éœ€è°ƒæ•´")
            return aligned_data, main_timestamps

        # è½¬æ¢ä¸ºnumpyæ•°ç»„ä¾¿äºæ“ä½œ
        main_timestamps = np.array(main_timestamps)

        # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„æ•°æ®æ¨¡æ€
        valid_modalities = {}
        for key, data_list in aligned_data.items():
            if len(data_list) > 0:
                valid_modalities[key] = list(data_list)  # è½¬æ¢ä¸ºåˆ—è¡¨ä¾¿äºæ’å…¥/åˆ é™¤

        if current_fps < target_fps_min:
            # å¸§ç‡å¤ªä½ï¼Œéœ€è¦æ’å¸§
            print(f"  å¸§ç‡è¿‡ä½ï¼Œå¼€å§‹æ’å¸§...")
            main_timestamps, valid_modalities = self._insert_frames_to_increase_fps(
                main_timestamps, valid_modalities, target_fps_min, time_span
            )
        elif current_fps > target_fps_max:
            # å¸§ç‡å¤ªé«˜ï¼Œéœ€è¦æŠ½å¸§
            print(f"  å¸§ç‡è¿‡é«˜ï¼Œå¼€å§‹æŠ½å¸§...")
            main_timestamps, valid_modalities = self._remove_frames_to_decrease_fps(
                main_timestamps, valid_modalities, target_fps_max, time_span
            )

        # éªŒè¯è°ƒæ•´ç»“æœ
        final_time_span = main_timestamps[-1] - main_timestamps[0]
        final_fps = len(main_timestamps) / final_time_span

        print(f"  è°ƒæ•´åå¸§ç‡: {final_fps:.2f}Hz")
        print(f"  è°ƒæ•´åå¸§æ•°: {len(main_timestamps)}")
        print(f"  è°ƒæ•´åæ—¶é—´è·¨åº¦: {final_time_span:.3f}s")

        if target_fps_min <= final_fps <= target_fps_max:
            print(f"  âœ“ å¸§ç‡è°ƒæ•´æˆåŠŸï¼")
        else:
            print(f"  âš ï¸ å¸§ç‡è°ƒæ•´åä»ä¸åœ¨ç›®æ ‡èŒƒå›´å†…")

        # è½¬æ¢å›åŸæ ¼å¼
        aligned_data_adjusted = {}
        for key, data_list in valid_modalities.items():
            aligned_data_adjusted[key] = data_list

        # æ·»åŠ ç©ºçš„æ¨¡æ€æ•°æ®
        for key, data_list in aligned_data.items():
            if key not in aligned_data_adjusted:
                aligned_data_adjusted[key] = []

        print("=" * 60)
        return aligned_data_adjusted, main_timestamps

    def _insert_frames_to_increase_fps(
        self,
        main_timestamps: np.ndarray,
        valid_modalities: dict,
        target_fps: float,
        time_span: float,
    ):
        """
        é€šè¿‡æ’å¸§æ¥æé«˜å¸§ç‡åˆ°ç›®æ ‡å€¼
        """
        target_frame_count = int(time_span * target_fps)
        current_frame_count = len(main_timestamps)
        frames_to_insert = target_frame_count - current_frame_count

        print(f"    éœ€è¦æ’å…¥ {frames_to_insert} å¸§")

        if frames_to_insert <= 0:
            return main_timestamps, valid_modalities

        # è®¡ç®—æ—¶é—´é—´éš”
        time_intervals = np.diff(main_timestamps) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        insertion_threshold_ms = 33.0  # 33msé˜ˆå€¼

        inserted_count = 0
        max_iterations = frames_to_insert * 2  # é˜²æ­¢æ— é™å¾ªç¯
        iteration = 0

        while inserted_count < frames_to_insert and iteration < max_iterations:
            iteration += 1

            # é‡æ–°è®¡ç®—é—´éš”ï¼ˆå› ä¸ºæ’å…¥ä¼šæ”¹å˜ï¼‰
            time_intervals = np.diff(main_timestamps) * 1000

            # æ‰¾åˆ°æœ€å¤§çš„é—´éš”
            max_interval_idx = np.argmax(time_intervals)
            max_interval_ms = time_intervals[max_interval_idx]

            if max_interval_ms <= insertion_threshold_ms:
                print(f"    æ— æ³•æ‰¾åˆ°è¶…è¿‡{insertion_threshold_ms}msçš„é—´éš”è¿›è¡Œæ’å¸§")
                break

            # åœ¨æœ€å¤§é—´éš”å¤„æ’å…¥ä¸€å¸§
            insert_pos = max_interval_idx + 1

            # è®¡ç®—æ’å…¥æ—¶é—´æˆ³ï¼ˆä¸¤å¸§ä¸­é—´ï¼‰
            prev_timestamp = main_timestamps[max_interval_idx]
            next_timestamp = main_timestamps[max_interval_idx + 1]
            new_timestamp = (prev_timestamp + next_timestamp) / 2

            # æ’å…¥ä¸»æ—¶é—´æˆ³
            main_timestamps = np.insert(main_timestamps, insert_pos, new_timestamp)

            # ä¸ºæ‰€æœ‰æ¨¡æ€æ’å…¥æ•°æ®ï¼ˆå¤åˆ¶å‰ä¸€å¸§ï¼‰
            for key, data_list in valid_modalities.items():
                if insert_pos <= len(data_list):
                    # å¤åˆ¶å‰ä¸€å¸§æ•°æ®
                    reference_frame = data_list[max_interval_idx].copy()
                    reference_frame["timestamp"] = new_timestamp
                    reference_frame["frame_inserted"] = True  # æ ‡è®°ä¸ºæ’å…¥å¸§
                    data_list.insert(insert_pos, reference_frame)

            inserted_count += 1

            if inserted_count % 10 == 0:  # æ¯æ’å…¥10å¸§è¾“å‡ºä¸€æ¬¡è¿›åº¦
                current_fps = len(main_timestamps) / (
                    main_timestamps[-1] - main_timestamps[0]
                )
                print(f"    å·²æ’å…¥ {inserted_count} å¸§ï¼Œå½“å‰å¸§ç‡: {current_fps:.2f}Hz")

        print(f"    å®é™…æ’å…¥äº† {inserted_count} å¸§")

        return main_timestamps, valid_modalities

    # def _remove_frames_to_decrease_fps(self, main_timestamps: np.ndarray, valid_modalities: dict,
    #                                 target_fps: float, time_span: float):
    #     """
    #     é€šè¿‡æŠ½å¸§æ¥é™ä½å¸§ç‡åˆ°ç›®æ ‡å€¼
    #     """
    #     target_frame_count = int(time_span * target_fps)
    #     current_frame_count = len(main_timestamps)
    #     frames_to_remove = current_frame_count - target_frame_count

    #     print(f"    éœ€è¦åˆ é™¤ {frames_to_remove} å¸§")

    #     if frames_to_remove <= 0:
    #         return main_timestamps, valid_modalities

    #     removal_threshold_ms = 40.0  # 40msé˜ˆå€¼
    #     removed_count = 0
    #     max_iterations = frames_to_remove * 2  # é˜²æ­¢æ— é™å¾ªç¯
    #     iteration = 0

    #     # åˆ›å»ºå¯åˆ é™¤å¸§çš„å€™é€‰åˆ—è¡¨ï¼ˆæ’é™¤é¦–å°¾å¸§ï¼‰
    #     removable_indices = list(range(1, len(main_timestamps) - 1))

    #     while removed_count < frames_to_remove and iteration < max_iterations and removable_indices:
    #         iteration += 1

    #         # å¯»æ‰¾å¯ä»¥å®‰å…¨åˆ é™¤çš„å¸§
    #         best_remove_idx = None
    #         min_max_interval = float('inf')

    #         for candidate_idx in removable_indices[:]:  # ä½¿ç”¨åˆ‡ç‰‡åˆ›å»ºå‰¯æœ¬
    #             if candidate_idx <= 0 or candidate_idx >= len(main_timestamps) - 1:
    #                 removable_indices.remove(candidate_idx)
    #                 continue

    #             # è®¡ç®—åˆ é™¤è¯¥å¸§åå‰åå¸§çš„é—´éš”
    #             prev_timestamp = main_timestamps[candidate_idx - 1]
    #             next_timestamp = main_timestamps[candidate_idx + 1]
    #             resulting_interval_ms = (next_timestamp - prev_timestamp) * 1000

    #             # æ£€æŸ¥æ˜¯å¦æ»¡è¶³åˆ é™¤æ¡ä»¶
    #             if resulting_interval_ms < removal_threshold_ms:
    #                 # é€‰æ‹©åˆ é™¤åé—´éš”æœ€å°çš„å¸§ï¼ˆæ›´å®‰å…¨ï¼‰
    #                 if resulting_interval_ms < min_max_interval:
    #                     min_max_interval = resulting_interval_ms
    #                     best_remove_idx = candidate_idx

    #         if best_remove_idx is None:
    #             print(f"    æ— æ³•æ‰¾åˆ°å¯ä»¥å®‰å…¨åˆ é™¤çš„å¸§ï¼ˆåˆ é™¤åé—´éš”éœ€å°äº{removal_threshold_ms}msï¼‰")
    #             break

    #         # åˆ é™¤é€‰ä¸­çš„å¸§
    #         # åˆ é™¤ä¸»æ—¶é—´æˆ³
    #         main_timestamps = np.delete(main_timestamps, best_remove_idx)

    #         # åˆ é™¤æ‰€æœ‰æ¨¡æ€çš„å¯¹åº”æ•°æ®
    #         for key, data_list in valid_modalities.items():
    #             if best_remove_idx < len(data_list):
    #                 del data_list[best_remove_idx]

    #         # æ›´æ–°å¯åˆ é™¤ç´¢å¼•åˆ—è¡¨ï¼ˆè°ƒæ•´ç´¢å¼•å€¼ï¼‰
    #         removable_indices = [idx - 1 if idx > best_remove_idx else idx for idx in removable_indices]
    #         removable_indices = [idx for idx in removable_indices if 0 < idx < len(main_timestamps) - 1]

    #         removed_count += 1

    #         if removed_count % 10 == 0:  # æ¯åˆ é™¤10å¸§è¾“å‡ºä¸€æ¬¡è¿›åº¦
    #             current_fps = len(main_timestamps) / (main_timestamps[-1] - main_timestamps[0])
    #             print(f"    å·²åˆ é™¤ {removed_count} å¸§ï¼Œå½“å‰å¸§ç‡: {current_fps:.2f}Hz")

    #     # æ£€æŸ¥æ˜¯å¦æˆåŠŸè¾¾åˆ°ç›®æ ‡
    #     if removed_count < frames_to_remove:
    #         final_fps = len(main_timestamps) / (main_timestamps[-1] - main_timestamps[0])
    #         error_msg = (
    #             f"æ— æ³•é€šè¿‡æŠ½å¸§è¾¾åˆ°ç›®æ ‡å¸§ç‡ã€‚éœ€è¦åˆ é™¤ {frames_to_remove} å¸§ï¼Œ"
    #             f"ä½†åªèƒ½å®‰å…¨åˆ é™¤ {removed_count} å¸§ã€‚å½“å‰å¸§ç‡: {final_fps:.2f}Hz"
    #         )
    #         print(f"    âŒ {error_msg}")

    #         raise TimestampStuckError(
    #             message=f"å¸§ç‡è°ƒæ•´å¤±è´¥: {error_msg}",
    #             topic="frame_rate_adjustment",
    #             stuck_timestamp=None,
    #             stuck_duration=None,
    #             stuck_frame_count=frames_to_remove - removed_count,
    #             threshold=target_fps
    #         )

    #     print(f"    å®é™…åˆ é™¤äº† {removed_count} å¸§")

    #     return main_timestamps, valid_modalities
    def _remove_frames_to_decrease_fps(
        self,
        main_timestamps: np.ndarray,
        valid_modalities: dict,
        target_fps: float,
        time_span: float,
    ):
        """
        é€šè¿‡æ»‘åŠ¨çª—å£åˆ é™¤+å±€éƒ¨æ—¶é—´æˆ³é‡æ–°å¹³å‡æ¥é™ä½å¸§ç‡åˆ°ç›®æ ‡å€¼
        åˆ é™¤åå¯¹çª—å£å†…æ—¶é—´æˆ³é‡æ–°å¹³å‡åˆ†å¸ƒï¼Œå¹¶åŒæ­¥è°ƒæ•´æ‰€æœ‰æ¨¡æ€
        """
        target_frame_count = int(time_span * target_fps)
        current_frame_count = len(main_timestamps)
        frames_to_remove = current_frame_count - target_frame_count

        print(
            f"    éœ€è¦åˆ é™¤ {frames_to_remove} å¸§ (ä» {current_frame_count} å¸§é™åˆ° {target_frame_count} å¸§)"
        )

        if frames_to_remove <= 0:
            return main_timestamps, valid_modalities

        removed_count = 0
        max_iterations = frames_to_remove * 3  # é˜²æ­¢æ— é™å¾ªç¯
        iteration = 0

        # æ»‘åŠ¨çª—å£å‚æ•°
        window_size = 5  # åˆå§‹çª—å£å¤§å°ï¼ˆå¿…é¡»ä¸ºå¥‡æ•°ï¼‰
        max_window_size = 15  # æœ€å¤§çª—å£å¤§å°
        max_interval_threshold_ms = 40.0  # æœ€å¤§é—´éš”é˜ˆå€¼

        print(f"    ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ é™¤+é‡æ–°å¹³å‡ç®—æ³•ï¼Œåˆå§‹çª—å£å¤§å°: {window_size}")

        while removed_count < frames_to_remove and iteration < max_iterations:
            iteration += 1

            if len(main_timestamps) <= window_size + 2:  # ä¿è¯è‡³å°‘æœ‰è¶³å¤Ÿçš„å¸§æ•°
                print(f"    å‰©ä½™å¸§æ•°è¿‡å°‘({len(main_timestamps)})ï¼Œæ— æ³•ç»§ç»­åˆ é™¤")
                break

            # å¯»æ‰¾æœ€ä½³åˆ é™¤å€™é€‰
            best_candidate = None
            best_score = float("inf")
            candidates_found = 0

            # æ»‘åŠ¨çª—å£éå†æ‰€æœ‰å¯èƒ½çš„åˆ é™¤ä½ç½®
            for start_idx in range(len(main_timestamps) - window_size + 1):
                end_idx = start_idx + window_size
                center_idx = start_idx + window_size // 2  # çª—å£ä¸­å¿ƒç´¢å¼•

                # è·³è¿‡é¦–å°¾å¸§é™„è¿‘çš„çª—å£
                if center_idx <= 1 or center_idx >= len(main_timestamps) - 2:
                    continue

                # æå–çª—å£æ—¶é—´æˆ³
                window_timestamps = main_timestamps[start_idx:end_idx]

                # æ¨¡æ‹Ÿåˆ é™¤çª—å£ä¸­å¿ƒå¸§
                timestamps_after_removal = np.concatenate(
                    [
                        window_timestamps[: window_size // 2],  # ä¸­å¿ƒå¸§ä¹‹å‰
                        window_timestamps[window_size // 2 + 1 :],  # ä¸­å¿ƒå¸§ä¹‹å
                    ]
                )

                # å¯¹åˆ é™¤åçš„æ—¶é—´æˆ³è¿›è¡Œé‡æ–°å¹³å‡åˆ†å¸ƒ
                reaveraged_timestamps = self._reaverage_timestamps_in_window(
                    timestamps_after_removal,
                    window_timestamps[0],
                    window_timestamps[-1],
                )

                # æ£€æŸ¥é‡æ–°å¹³å‡åçš„æœ€å¤§æ—¶é—´é—´éš”
                if len(reaveraged_timestamps) > 1:
                    reaveraged_intervals_ms = np.diff(reaveraged_timestamps) * 1000
                    max_reaveraged_interval = np.max(reaveraged_intervals_ms)

                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³40msé™åˆ¶
                    if max_reaveraged_interval <= max_interval_threshold_ms:
                        candidates_found += 1

                        # è®¡ç®—è¯„åˆ†ï¼ˆä¼˜å…ˆé€‰æ‹©é‡æ–°å¹³å‡åé—´éš”æœ€å°ä¸”æœ€å‡åŒ€çš„ï¼‰
                        interval_score = max_reaveraged_interval
                        uniformity_score = np.std(reaveraged_intervals_ms) * 2
                        density_score = -np.mean(
                            np.diff(window_timestamps) * 1000
                        )  # ä¼˜å…ˆåˆ é™¤å¯†é›†åŒºåŸŸ

                        total_score = interval_score + uniformity_score + density_score

                        if total_score < best_score:
                            best_score = total_score
                            best_candidate = {
                                "start_idx": start_idx,
                                "end_idx": end_idx,
                                "remove_idx": center_idx,
                                "window_size": window_size,
                                "original_timestamps": window_timestamps,
                                "reaveraged_timestamps": reaveraged_timestamps,
                                "max_interval_after": max_reaveraged_interval,
                                "score": total_score,
                            }

            # å¦‚æœæ‰¾åˆ°äº†åˆé€‚çš„åˆ é™¤å€™é€‰
            if best_candidate is not None:
                # æ‰§è¡Œåˆ é™¤å’Œé‡æ–°å¹³å‡
                # ä¿®æ”¹åçš„ä»£ç 
                new_timestamps, success = self._execute_window_removal_and_reaverage(
                    main_timestamps, valid_modalities, best_candidate
                )

                if success:
                    main_timestamps = new_timestamps  # æ›´æ–°æ—¶é—´æˆ³æ•°ç»„
                    removed_count += 1

                    if removed_count % 10 == 0:
                        current_fps = len(main_timestamps) / (
                            main_timestamps[-1] - main_timestamps[0]
                        )
                        print(
                            f"      å·²åˆ é™¤ {removed_count} å¸§ï¼Œå½“å‰å¸§ç‡: {current_fps:.2f}Hz"
                        )
                        print(
                            f"      æœ€æ–°åˆ é™¤: çª—å£{best_candidate['start_idx']}-{best_candidate['end_idx']}, "
                            f"åˆ é™¤åæœ€å¤§é—´éš”: {best_candidate['max_interval_after']:.1f}ms"
                        )
                else:
                    print(f"    æ‰§è¡Œåˆ é™¤å¤±è´¥ï¼Œè·³è¿‡æ­¤å€™é€‰")

            else:
                # æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„åˆ é™¤å€™é€‰
                print(
                    f"    ç¬¬{iteration}è½®: çª—å£å¤§å°{window_size}ä¸‹æ‰¾åˆ° {candidates_found} ä¸ªå€™é€‰"
                )

                # æ‰©å¤§çª—å£å¤§å°å†å°è¯•
                if window_size < max_window_size:
                    window_size += 2  # ä¿æŒå¥‡æ•°
                    print(f"    æ‰©å¤§çª—å£å¤§å°åˆ°: {window_size}ï¼Œç»§ç»­å°è¯•")
                    continue
                else:
                    # çª—å£å·²ç»æœ€å¤§ï¼Œæ— æ³•ç»§ç»­
                    print(
                        f"    çª—å£å¤§å°å·²è¾¾åˆ°æœ€å¤§({window_size})ï¼Œæ— æ³•æ‰¾åˆ°æ›´å¤šå¯åˆ é™¤ä½ç½®"
                    )
                    break

        # æœ€ç»ˆéªŒè¯å’Œç»Ÿè®¡
        final_fps = len(main_timestamps) / (main_timestamps[-1] - main_timestamps[0])

        print(f"    åˆ é™¤å®Œæˆç»Ÿè®¡:")
        print(f"      ç›®æ ‡åˆ é™¤: {frames_to_remove} å¸§")
        print(f"      å®é™…åˆ é™¤: {removed_count} å¸§")
        print(f"      æœ€ç»ˆå¸§ç‡: {final_fps:.3f}Hz")

        # éªŒè¯æœ€ç»ˆæ—¶é—´æˆ³è´¨é‡
        if len(main_timestamps) > 1:
            final_intervals_ms = np.diff(main_timestamps) * 1000
            max_final_interval = np.max(final_intervals_ms)
            avg_final_interval = np.mean(final_intervals_ms)
            std_final_interval = np.std(final_intervals_ms)

            print(f"      æœ€ç»ˆæ—¶é—´æˆ³è´¨é‡:")
            print(f"        æœ€å¤§é—´éš”: {max_final_interval:.1f}ms")
            print(f"        å¹³å‡é—´éš”: {avg_final_interval:.1f}ms")
            print(f"        é—´éš”æ ‡å‡†å·®: {std_final_interval:.1f}ms")

            # ä¸¥æ ¼éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦ä»æœ‰è¶…è¿‡40msçš„é—´éš”
            large_intervals = final_intervals_ms > max_interval_threshold_ms
            if np.any(large_intervals):
                large_count = np.sum(large_intervals)
                worst_interval = np.max(final_intervals_ms)

                error_msg = (
                    f"åˆ é™¤åéªŒè¯å¤±è´¥ï¼šä»æœ‰ {large_count} ä¸ªé—´éš”è¶…è¿‡{max_interval_threshold_ms}msï¼Œ"
                    f"æœ€å¤§é—´éš”{worst_interval:.1f}ms"
                )
                print(f"        âŒ {error_msg}")

                # æ˜¾ç¤ºå…·ä½“é—®é¢˜é—´éš”
                problem_indices = np.where(large_intervals)[0]
                for i, idx in enumerate(problem_indices[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    interval_value = final_intervals_ms[idx]
                    start_time = main_timestamps[idx]
                    end_time = main_timestamps[idx + 1]
                    print(
                        f"          é—®é¢˜é—´éš”{i+1}: {start_time:.6f}s -> {end_time:.6f}s, é—´éš”={interval_value:.1f}ms"
                    )

                raise TimestampStuckError(
                    message=f"ä¸¥æ ¼é—´éš”éªŒè¯å¤±è´¥: {error_msg}",
                    topic="strict_interval_validation",
                    stuck_timestamp=main_timestamps[problem_indices[0]],
                    stuck_duration=worst_interval / 1000,
                    stuck_frame_count=large_count,
                    threshold=max_interval_threshold_ms / 1000,
                )
            else:
                print(f"        âœ“ æ‰€æœ‰é—´éš”éƒ½åœ¨{max_interval_threshold_ms}msä»¥å†…")

        # ä¸¥æ ¼æ£€æŸ¥ï¼šæ˜¯å¦è¾¾åˆ°ç›®æ ‡å¸§ç‡
        if removed_count < frames_to_remove:
            shortfall = frames_to_remove - removed_count
            error_msg = (
                f"åˆ é™¤æœªå®Œæˆï¼šéœ€è¦åˆ é™¤ {frames_to_remove} å¸§ï¼Œå®é™…åˆ é™¤ {removed_count} å¸§ï¼Œ"
                f"è¿˜å·® {shortfall} å¸§ã€‚å½“å‰å¸§ç‡: {final_fps:.3f}Hzï¼Œç›®æ ‡: â‰¤{target_fps:.3f}Hz"
            )

            print(f"    âŒ {error_msg}")

            raise TimestampStuckError(
                message=f"ä¸¥æ ¼å¸§ç‡è°ƒæ•´å¤±è´¥: {error_msg}",
                topic="strict_frame_rate_adjustment",
                stuck_timestamp=None,
                stuck_duration=None,
                stuck_frame_count=shortfall,
                threshold=target_fps,
            )

        # æœ€ç»ˆæ£€æŸ¥ï¼šéªŒè¯å¸§ç‡æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if final_fps > target_fps:
            fps_excess = final_fps - target_fps
            error_msg = (
                f"æœ€ç»ˆå¸§ç‡éªŒè¯å¤±è´¥ï¼šå½“å‰å¸§ç‡ {final_fps:.3f}Hz ä»ç„¶è¶…è¿‡ç›®æ ‡ {target_fps:.3f}Hzï¼Œ"
                f"è¶…å‡º {fps_excess:.3f}Hz"
            )

            print(f"    âŒ {error_msg}")

            raise TimestampStuckError(
                message=f"ä¸¥æ ¼å¸§ç‡ç›®æ ‡æœªè¾¾æˆ: {error_msg}",
                topic="strict_fps_target",
                stuck_timestamp=None,
                stuck_duration=None,
                stuck_frame_count=frames_to_remove - removed_count,
                threshold=target_fps,
            )

        print(f"    âœ“ æ»‘åŠ¨çª—å£åˆ é™¤+é‡æ–°å¹³å‡æˆåŠŸå®Œæˆ")
        print(f"      æœ€ç»ˆå¸§ç‡: {final_fps:.3f}Hz â‰¤ {target_fps:.3f}Hz")
        print(
            f"      æœ€å¤§æ—¶é—´é—´éš”: {max_final_interval:.1f}ms â‰¤ {max_interval_threshold_ms}ms"
        )

        return main_timestamps, valid_modalities

    def _reaverage_timestamps_in_window(
        self,
        timestamps_after_removal: np.ndarray,
        window_start_time: float,
        window_end_time: float,
    ) -> np.ndarray:
        """
        å¯¹åˆ é™¤å¸§åçš„çª—å£å†…æ—¶é—´æˆ³è¿›è¡Œé‡æ–°å¹³å‡åˆ†å¸ƒ
        ä¿®æ­£ç‰ˆï¼šåªå¯¹å†…éƒ¨æ—¶é—´æˆ³é‡æ–°å¹³å‡ï¼Œä¿æŒä¸¤ç«¯ä¸å˜

        Args:
            timestamps_after_removal: åˆ é™¤ä¸­å¿ƒå¸§åçš„æ—¶é—´æˆ³æ•°ç»„
            window_start_time: çª—å£å¼€å§‹æ—¶é—´ï¼ˆä¿æŒä¸å˜ï¼‰
            window_end_time: çª—å£ç»“æŸæ—¶é—´ï¼ˆä¿æŒä¸å˜ï¼‰

        Returns:
            é‡æ–°å¹³å‡åˆ†å¸ƒåçš„æ—¶é—´æˆ³æ•°ç»„
        """
        if len(timestamps_after_removal) <= 2:
            # å¦‚æœåªæœ‰2ä¸ªæˆ–æ›´å°‘çš„ç‚¹ï¼Œæ— æ³•è¿›è¡Œå†…éƒ¨é‡æ–°å¹³å‡
            return timestamps_after_removal

        # ä½¿ç”¨ä¼ å…¥çš„çª—å£èµ·å§‹å’Œç»“æŸæ—¶é—´ï¼Œè€Œä¸æ˜¯æ•°ç»„çš„é¦–å°¾æ—¶é—´
        start_time = window_start_time
        end_time = window_end_time

        # å†…éƒ¨ç‚¹æ•°é‡
        num_internal_points = len(timestamps_after_removal) - 2

        if num_internal_points <= 0:
            # æ²¡æœ‰å†…éƒ¨ç‚¹ï¼Œåªè¿”å›é¦–å°¾æ—¶é—´æˆ³
            return np.array([start_time, end_time])

        # é‡æ–°å¹³å‡ï¼šåœ¨èµ·å§‹å’Œç»“æŸæ—¶é—´ä¹‹é—´å‡åŒ€åˆ†å¸ƒå†…éƒ¨ç‚¹
        internal_timestamps = np.linspace(
            start_time, end_time, num_internal_points + 2
        )[1:-1]

        # æ„å»ºå®Œæ•´çš„é‡æ–°å¹³å‡æ—¶é—´æˆ³æ•°ç»„
        reaveraged_timestamps = np.concatenate(
            [
                [start_time],  # èµ·å§‹ç‚¹ä¿æŒä¸å˜
                internal_timestamps,  # å†…éƒ¨ç‚¹é‡æ–°å¹³å‡
                [end_time],  # ç»“æŸç‚¹ä¿æŒä¸å˜
            ]
        )

        return reaveraged_timestamps

    def _execute_window_removal_and_reaverage(
        self, main_timestamps: np.ndarray, valid_modalities: dict, candidate: dict
    ) -> bool:
        """
        æ‰§è¡Œçª—å£åˆ é™¤å’Œé‡æ–°å¹³å‡æ“ä½œï¼ŒåŒæ­¥æ›´æ–°æ‰€æœ‰æ¨¡æ€
        ä¿®æ­£ç‰ˆï¼šåªå¯¹çª—å£å†…éƒ¨æ—¶é—´æˆ³é‡æ–°å¹³å‡ï¼Œä¸¤ç«¯ä¿æŒä¸å˜ï¼›å­æ—¶é—´æˆ³ä½¿ç”¨å˜åŒ–é‡åŒæ­¥
        """
        try:
            start_idx = candidate["start_idx"]
            end_idx = candidate["end_idx"]
            remove_idx = candidate["remove_idx"]
            window_size = candidate["window_size"]

            # ç¡®ä¿çª—å£å¤§å°è‡³å°‘ä¸º5ï¼ˆåˆ é™¤åè‡³å°‘4ä¸ªç‚¹ï¼Œå†…éƒ¨è‡³å°‘2ä¸ªç‚¹å¯ä»¥å¹³å‡ï¼‰
            if window_size < 5:
                print(f"    çª—å£å¤§å° {window_size} < 5ï¼Œæ— æ³•å®‰å…¨è¿›è¡Œå†…éƒ¨é‡æ–°å¹³å‡")
                return main_timestamps, False

            # 1. åˆ é™¤ä¸»æ—¶é—´æˆ³çš„ä¸­å¿ƒå¸§
            main_timestamps_list = main_timestamps.tolist()
            del main_timestamps_list[remove_idx]

            # 2. åŒæ­¥åˆ é™¤æ‰€æœ‰æ¨¡æ€çš„å¯¹åº”å¸§
            for key, data_list in valid_modalities.items():
                if remove_idx < len(data_list):
                    del data_list[remove_idx]

            # 3. æ›´æ–°ä¸»æ—¶é—´æˆ³æ•°ç»„
            new_main_timestamps = np.array(main_timestamps_list)

            # 4. é‡æ–°è®¡ç®—çª—å£èŒƒå›´ï¼ˆåˆ é™¤åç´¢å¼•ä¼šå˜åŒ–ï¼‰
            window_start_idx = start_idx
            window_end_idx = end_idx - 1  # åˆ é™¤äº†ä¸€å¸§ï¼Œæ‰€ä»¥end_idxè¦å‡1

            # ç¡®ä¿ç´¢å¼•èŒƒå›´æœ‰æ•ˆ
            if window_start_idx >= len(new_main_timestamps) or window_end_idx > len(
                new_main_timestamps
            ):
                print(f"    åˆ é™¤åçª—å£ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡æ­¤æ¬¡æ“ä½œ")
                return main_timestamps, False

            # 5. æå–çª—å£å†…çš„æ—¶é—´æˆ³è¿›è¡Œé‡æ–°å¹³å‡ï¼ˆåªå¹³å‡å†…éƒ¨ç‚¹ï¼Œä¿æŒä¸¤ç«¯ä¸å˜ï¼‰
            window_timestamps = new_main_timestamps[window_start_idx:window_end_idx]

            if len(window_timestamps) < 3:
                print(
                    f"    åˆ é™¤åçª—å£å†…æ—¶é—´æˆ³è¿‡å°‘({len(window_timestamps)})ï¼Œæ— æ³•é‡æ–°å¹³å‡"
                )
                return main_timestamps, False

            # 6. é‡æ–°å¹³å‡ï¼šåªå¹³å‡å†…éƒ¨ç‚¹ï¼Œä¿æŒé¦–å°¾ä¸å˜
            start_time = window_timestamps[0]  # çª—å£èµ·å§‹æ—¶é—´ï¼ˆä¿æŒä¸å˜ï¼‰
            end_time = window_timestamps[-1]  # çª—å£ç»“æŸæ—¶é—´ï¼ˆä¿æŒä¸å˜ï¼‰

            # è®¡ç®—å†…éƒ¨ç‚¹çš„æ–°æ—¶é—´æˆ³ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
            num_internal_points = len(window_timestamps) - 2  # å†…éƒ¨ç‚¹æ•°é‡

            if num_internal_points > 0:
                # åœ¨èµ·å§‹å’Œç»“æŸæ—¶é—´ä¹‹é—´å‡åŒ€åˆ†å¸ƒå†…éƒ¨ç‚¹
                internal_new_timestamps = np.linspace(
                    start_time, end_time, num_internal_points + 2
                )[1:-1]

                # æ„å»ºå®Œæ•´çš„é‡æ–°å¹³å‡æ—¶é—´æˆ³æ•°ç»„
                reaveraged_timestamps = np.concatenate(
                    [
                        [start_time],  # èµ·å§‹ç‚¹ä¿æŒä¸å˜
                        internal_new_timestamps,  # å†…éƒ¨ç‚¹é‡æ–°å¹³å‡
                        [end_time],  # ç»“æŸç‚¹ä¿æŒä¸å˜
                    ]
                )
            else:
                # å¦‚æœæ²¡æœ‰å†…éƒ¨ç‚¹ï¼Œç›´æ¥ä½¿ç”¨åŸæ—¶é—´æˆ³
                reaveraged_timestamps = window_timestamps

            # éªŒè¯é‡æ–°å¹³å‡åçš„é—´éš”
            if len(reaveraged_timestamps) > 1:
                reaveraged_intervals_ms = np.diff(reaveraged_timestamps) * 1000
                max_reaveraged_interval = np.max(reaveraged_intervals_ms)

                if max_reaveraged_interval > 40:
                    print(
                        f"    é‡æ–°å¹³å‡åæœ€å¤§é—´éš” {max_reaveraged_interval:.1f}ms ä»è¶…è¿‡40ms"
                    )
                    return main_timestamps, False

            # 7. æ›´æ–°çª—å£å†…çš„ä¸»æ—¶é—´æˆ³
            for i, new_timestamp in enumerate(reaveraged_timestamps):
                global_idx = window_start_idx + i
                if global_idx < len(new_main_timestamps):
                    old_timestamp = new_main_timestamps[global_idx]
                    timestamp_delta = new_timestamp - old_timestamp

                    # æ›´æ–°ä¸»æ—¶é—´æˆ³
                    new_main_timestamps[global_idx] = new_timestamp

                    # 8. åŒæ­¥æ›´æ–°æ‰€æœ‰æ¨¡æ€å¯¹åº”å¸§çš„æ—¶é—´æˆ³ï¼ˆä½¿ç”¨å˜åŒ–é‡ï¼‰
                    for key, data_list in valid_modalities.items():
                        if global_idx < len(data_list):
                            if "timestamp" in data_list[global_idx]:
                                # ä¿å­˜åŸå§‹æ—¶é—´æˆ³
                                original_modality_timestamp = data_list[global_idx][
                                    "timestamp"
                                ]

                                # åº”ç”¨ç›¸åŒçš„æ—¶é—´æˆ³å˜åŒ–é‡ï¼ˆä¿æŒå„æ¨¡æ€é—´çš„ç›¸å¯¹å…³ç³»ï¼‰
                                new_modality_timestamp = (
                                    original_modality_timestamp + timestamp_delta
                                )

                                # æ›´æ–°æ—¶é—´æˆ³
                                data_list[global_idx][
                                    "timestamp"
                                ] = new_modality_timestamp

                                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                                data_list[global_idx]["timestamp_reaveraged"] = True
                                data_list[global_idx][
                                    "original_timestamp"
                                ] = original_modality_timestamp
                                data_list[global_idx][
                                    "timestamp_delta"
                                ] = timestamp_delta
                                data_list[global_idx][
                                    "main_timestamp_new"
                                ] = new_timestamp

            # 9. å°†æ›´æ–°åçš„ä¸»æ—¶é—´æˆ³æ•°ç»„å¤åˆ¶å›åŸæ•°ç»„
            # main_timestamps[:] = new_main_timestamps[:]

            # 10. æœ€ç»ˆéªŒè¯æ“ä½œç»“æœ
            if len(new_main_timestamps) > 1:
                # éªŒè¯æ•´ä¸ªçª—å£çš„é—´éš”
                window_intervals_ms = (
                    np.diff(new_main_timestamps[window_start_idx:window_end_idx]) * 1000
                )
                max_interval = (
                    np.max(window_intervals_ms) if len(window_intervals_ms) > 0 else 0
                )

                if max_interval > 40:
                    print(f"    âš ï¸ çª—å£é‡æ–°å¹³å‡åé—´éš”ä»ç„¶è¿‡å¤§: {max_interval:.1f}ms")
                    return main_timestamps, False

                # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
                avg_interval = (
                    np.mean(window_intervals_ms) if len(window_intervals_ms) > 0 else 0
                )
                print(
                    f"    âœ“ çª—å£é‡æ–°å¹³å‡æˆåŠŸ: å¹³å‡é—´éš” {avg_interval:.1f}ms, æœ€å¤§é—´éš” {max_interval:.1f}ms"
                )

            return new_main_timestamps, True

        except Exception as e:
            print(f"    æ‰§è¡Œçª—å£åˆ é™¤å’Œé‡æ–°å¹³å‡æ—¶å‡ºé”™: {e}")
            return main_timestamps, False

    def _final_alignment_validation(
        self, aligned_data: dict, main_timestamps: np.ndarray
    ):
        """æœ€ç»ˆéªŒè¯å¯¹é½åçš„æ•°æ®è´¨é‡ï¼Œä¸æ»¡è¶³è¦æ±‚åˆ™æŠ›å‡ºå¼‚å¸¸"""

        # === æ–°å¢éªŒè¯ï¼šä¸»æ—¶é—´æˆ³é•¿åº¦å’Œæ—¶é—´è·¨åº¦æ£€æŸ¥ ===
        print("  éªŒè¯0: ä¸»æ—¶é—´æˆ³åŸºæœ¬è¦æ±‚æ£€æŸ¥")

        # éªŒè¯é•¿åº¦è¦æ±‚ï¼ˆè‡³å°‘300å¸§ï¼‰
        min_required_frames = 300
        if len(main_timestamps) < min_required_frames:
            error_msg = f"ä¸»æ—¶é—´æˆ³é•¿åº¦ {len(main_timestamps)} å°äºæœ€ä½è¦æ±‚ {min_required_frames} å¸§"
            print(f"    âŒ {error_msg}")

            raise TimestampStuckError(
                message=f"æ•°æ®é•¿åº¦ä¸è¶³: {error_msg}",
                topic="main_timeline_length",
                stuck_timestamp=(
                    main_timestamps[0] if len(main_timestamps) > 0 else None
                ),
                stuck_duration=None,
                stuck_frame_count=len(main_timestamps),
                threshold=min_required_frames,
            )
        else:
            print(
                f"    âœ“ ä¸»æ—¶é—´æˆ³é•¿åº¦éªŒè¯é€šè¿‡: {len(main_timestamps)} å¸§ (>= {min_required_frames})"
            )

        # éªŒè¯æ—¶é—´è·¨åº¦è¦æ±‚ï¼ˆè‡³å°‘10ç§’ï¼‰
        min_required_duration = 10.0  # ç§’
        if len(main_timestamps) > 1:
            time_span = main_timestamps[-1] - main_timestamps[0]

            if time_span < min_required_duration:
                error_msg = f"ä¸»æ—¶é—´æˆ³æ—¶é—´è·¨åº¦ {time_span:.3f}s å°äºæœ€ä½è¦æ±‚ {min_required_duration}s"
                print(f"    âŒ {error_msg}")

                raise TimestampStuckError(
                    message=f"æ•°æ®æ—¶é—´è·¨åº¦ä¸è¶³: {error_msg}",
                    topic="main_timeline_duration",
                    stuck_timestamp=main_timestamps[0],
                    stuck_duration=time_span,
                    stuck_frame_count=len(main_timestamps),
                    threshold=min_required_duration,
                )
            else:
                print(
                    f"    âœ“ ä¸»æ—¶é—´æˆ³æ—¶é—´è·¨åº¦éªŒè¯é€šè¿‡: {time_span:.3f}s (>= {min_required_duration}s)"
                )

            # === æ–°å¢éªŒè¯ï¼šä¸»æ—¶é—´æˆ³é¢‘ç‡æ£€æŸ¥ ===
            # è®¡ç®—å®é™…é¢‘ç‡ï¼šå¸§æ•° / æ—¶é—´è·¨åº¦
            actual_fps = len(main_timestamps) / time_span
            max_required_fps = 30.095  # Hz
            min_required_fps = 29.905

            if actual_fps > max_required_fps:
                error_msg = (
                    f"ä¸»æ—¶é—´æˆ³é¢‘ç‡ {actual_fps:.2f}Hz å¤§äºæœ€å¤§è¦æ±‚ {max_required_fps}Hz"
                )
                print(f"    âŒ {error_msg}")

                raise TimestampStuckError(
                    message=f"æ•°æ®é¢‘ç‡è¿‡å¤§: {error_msg}",
                    topic="main_timeline_fps",
                    stuck_timestamp=main_timestamps[0],
                    stuck_duration=time_span,
                    stuck_frame_count=len(main_timestamps),
                    threshold=max_required_fps,
                )
            elif actual_fps < min_required_fps:
                error_msg = (
                    f"ä¸»æ—¶é—´æˆ³é¢‘ç‡ {actual_fps:.2f}Hz å°äºæœ€ä½è¦æ±‚ {min_required_fps}Hz"
                )
                print(f"    âŒ {error_msg}")

                raise TimestampStuckError(
                    message=f"æ•°æ®é¢‘ç‡ä¸è¶³: {error_msg}",
                    topic="main_timeline_fps",
                    stuck_timestamp=main_timestamps[0],
                    stuck_duration=time_span,
                    stuck_frame_count=len(main_timestamps),
                    threshold=min_required_fps,
                )
            else:
                print(
                    f"    âœ“ ä¸»æ—¶é—´æˆ³é¢‘ç‡éªŒè¯é€šè¿‡: {min_required_fps}Hz <={actual_fps:.2f}Hz (<= {max_required_fps}Hz)"
                )
        else:
            # åªæœ‰ä¸€ä¸ªæ—¶é—´æˆ³çš„æƒ…å†µ
            error_msg = (
                f"ä¸»æ—¶é—´æˆ³åªæœ‰ {len(main_timestamps)} ä¸ªï¼Œæ— æ³•è®¡ç®—æ—¶é—´è·¨åº¦å’Œé¢‘ç‡"
            )
            print(f"    âŒ {error_msg}")

            raise TimestampStuckError(
                message=f"æ•°æ®ä¸è¶³ä»¥è®¡ç®—æ—¶é—´è·¨åº¦å’Œé¢‘ç‡: {error_msg}",
                topic="main_timeline_duration",
                stuck_timestamp=(
                    main_timestamps[0] if len(main_timestamps) > 0 else None
                ),
                stuck_duration=0,
                stuck_frame_count=len(main_timestamps),
                threshold=min_required_duration,
            )

        # éªŒè¯1: æ£€æŸ¥ä¸»æ—¶é—´æˆ³é—´éš”æ˜¯å¦è¶…è¿‡40ms
        print("  éªŒè¯1: ä¸»æ—¶é—´æˆ³é—´éš”æ£€æŸ¥")
        if len(main_timestamps) > 1:
            main_timestamps_ns = (main_timestamps * 1e9).astype(np.int64)
            main_intervals_ns = np.diff(main_timestamps_ns)
            main_intervals_ms = main_intervals_ns / 1e6

            max_main_interval = np.max(main_intervals_ms)
            if max_main_interval > 40:
                error_msg = f"ä¸»æ—¶é—´æˆ³æœ€å¤§é—´éš” {max_main_interval:.1f}ms è¶…è¿‡40msé˜ˆå€¼"
                print(f"    âŒ {error_msg}")

                # æ˜¾ç¤ºå…·ä½“çš„å¤§é—´éš”
                large_interval_indices = np.where(main_intervals_ms > 40)[0]
                print(f"    ä¸»æ—¶é—´æˆ³å¤§é—´éš”è¯¦æƒ…:")
                for idx in large_interval_indices[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(
                        f"      é—´éš”{idx}: {main_intervals_ms[idx]:.1f}ms "
                        f"({main_timestamps[idx]:.6f}s -> {main_timestamps[idx+1]:.6f}s)"
                    )

                raise TimestampStuckError(
                    message=f"æ—¶é—´æˆ³é—´éš”éªŒè¯å¤±è´¥: {error_msg}",
                    topic="main_timeline",
                    stuck_timestamp=None,
                    stuck_duration=max_main_interval / 1000,
                    stuck_frame_count=None,
                    threshold=0.04,
                )
            else:
                print(f"    âœ“ ä¸»æ—¶é—´æˆ³é—´éš”éªŒè¯é€šè¿‡: æœ€å¤§é—´éš” {max_main_interval:.1f}ms")

        # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆæ¨¡æ€çš„æ—¶é—´æˆ³
        valid_modalities = {}
        for key, data_list in aligned_data.items():
            # è·³è¿‡å¤–å‚æ•°æ®å’Œcamera_infoæ•°æ®çš„éªŒè¯
            if (
                len(data_list) == 0
                or key.endswith("_extrinsics")
                or key.endswith("_camera_info")
                or key.startswith("end_")
            ):
                continue

            aligned_timestamps = np.array([item["timestamp"] for item in data_list])

            # æ£€æŸ¥é•¿åº¦ä¸€è‡´æ€§
            if len(aligned_timestamps) != len(main_timestamps):
                print(
                    f"  âŒ {key}: é•¿åº¦ä¸åŒ¹é… ({len(aligned_timestamps)} vs {len(main_timestamps)})"
                )
                continue

            valid_modalities[key] = aligned_timestamps

        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ¨¡æ€ï¼Œè·³è¿‡éªŒè¯
        if not valid_modalities:
            print("  âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®æ¨¡æ€è¿›è¡ŒéªŒè¯")
            return

        print(f"  å¼€å§‹éªŒè¯ {len(valid_modalities)} ä¸ªæ•°æ®æ¨¡æ€çš„æ—¶é—´æˆ³åŒæ­¥...")

        alignment_errors = []

        # éªŒè¯2: æ¯ä¸ªæ—¶åˆ»æ‰€æœ‰æ¨¡æ€æ—¶é—´æˆ³çš„æœ€å¤§æœ€å°å·®å€¼ä¸è¶…è¿‡20ms
        print("  éªŒè¯2: æ¯ä¸ªæ—¶åˆ»å¤šæ¨¡æ€é—´çš„æ—¶é—´æˆ³å·®å€¼")
        frame_sync_errors = []

        # å°†æ‰€æœ‰æ¨¡æ€çš„æ—¶é—´æˆ³è½¬æ¢ä¸ºçº³ç§’ç²¾åº¦
        all_timestamps_ns = {}
        for key, timestamps in valid_modalities.items():
            all_timestamps_ns[key] = (timestamps * 1e9).astype(np.int64)

        # é€å¸§æ£€æŸ¥
        max_frame_spread = 0
        worst_frame_idx = -1

        for frame_idx in range(len(main_timestamps)):
            # æ”¶é›†è¯¥å¸§æ‰€æœ‰æ¨¡æ€çš„æ—¶é—´æˆ³
            frame_timestamps_ns = []
            frame_keys = []

            for key, timestamps_ns in all_timestamps_ns.items():
                if frame_idx < len(timestamps_ns):
                    frame_timestamps_ns.append(timestamps_ns[frame_idx])
                    frame_keys.append(key)

            if len(frame_timestamps_ns) > 1:
                # è®¡ç®—è¯¥å¸§æ‰€æœ‰æ¨¡æ€æ—¶é—´æˆ³çš„èŒƒå›´
                min_ts_ns = np.min(frame_timestamps_ns)
                max_ts_ns = np.max(frame_timestamps_ns)
                spread_ns = max_ts_ns - min_ts_ns
                spread_ms = spread_ns / 1e6

                if spread_ms > max_frame_spread:
                    max_frame_spread = spread_ms
                    worst_frame_idx = frame_idx

                if spread_ms > 20:
                    frame_sync_errors.append(
                        {
                            "frame_idx": frame_idx,
                            "spread_ms": spread_ms,
                            "timestamps": frame_timestamps_ns,
                            "keys": frame_keys,
                        }
                    )

        if frame_sync_errors:
            error_msg = (
                f"å‘ç° {len(frame_sync_errors)} ä¸ªæ—¶åˆ»çš„å¤šæ¨¡æ€æ—¶é—´æˆ³å·®å€¼è¶…è¿‡20msé˜ˆå€¼"
            )
            print(f"    âŒ {error_msg}")

            # æ˜¾ç¤ºæœ€ä¸¥é‡çš„å‡ ä¸ªæ—¶åˆ»
            sorted_errors = sorted(
                frame_sync_errors, key=lambda x: x["spread_ms"], reverse=True
            )
            for i, error in enumerate(sorted_errors[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªæœ€ä¸¥é‡çš„
                frame_idx = error["frame_idx"]
                spread_ms = error["spread_ms"]
                timestamps_s = [ts_ns / 1e9 for ts_ns in error["timestamps"]]
                keys = error["keys"]

                print(f"      æ—¶åˆ»{frame_idx}: æ—¶é—´æˆ³å·®å€¼ {spread_ms:.1f}ms")
                for key, ts_s in zip(keys, timestamps_s):
                    print(f"        {key}: {ts_s:.6f}s")

            alignment_errors.append(
                f"å¤šæ¨¡æ€åŒæ­¥: {error_msg}ï¼Œæœ€å¤§å·®å€¼ {max_frame_spread:.1f}ms"
            )
        else:
            print(f"    âœ“ å¤šæ¨¡æ€æ—¶é—´æˆ³åŒæ­¥éªŒè¯é€šè¿‡ï¼Œæœ€å¤§å·®å€¼ {max_frame_spread:.1f}ms")

        # å¦‚æœæœ‰éªŒè¯é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸
        if alignment_errors:
            error_summary = "; ".join(alignment_errors)
            detailed_msg = (
                f"ä¸¥æ ¼å¯¹é½éªŒè¯å¤±è´¥:\n"
                f"- æœ€å¤§å¸§å†…æ—¶é—´æˆ³å·®å€¼: {max_frame_spread:.1f}ms\n"
                f"- éªŒè¯é”™è¯¯: {error_summary}\n"
                f"- å‚ä¸éªŒè¯çš„æ•°æ®æ¨¡æ€æ•°: {len(valid_modalities)}\n"
                f"- ä¸»æ—¶é—´æˆ³é•¿åº¦: {len(main_timestamps)}"
            )

            print(f"[ERROR] {detailed_msg}")
            raise TimestampStuckError(
                message=f"ä¸¥æ ¼å¯¹é½éªŒè¯å¤±è´¥: {error_summary}",
                topic="strict_alignment_validation",
                stuck_timestamp=(
                    main_timestamps[worst_frame_idx] if worst_frame_idx >= 0 else None
                ),
                stuck_duration=max_frame_spread / 1000,
                stuck_frame_count=len(alignment_errors),
                threshold=0.02,
            )

        # éªŒè¯é€šè¿‡ï¼Œè¾“å‡ºæ€»ç»“
        print("=" * 60)
        print("âœ“ ä¸¥æ ¼å¯¹é½éªŒè¯é€šè¿‡!")

        # === æ›´æ–°ï¼šè¾“å‡ºåŸºæœ¬è¦æ±‚éªŒè¯ç»“æœï¼ˆåŒ…å«é¢‘ç‡ï¼‰ ===
        time_span = (
            main_timestamps[-1] - main_timestamps[0] if len(main_timestamps) > 1 else 0
        )
        actual_fps = len(main_timestamps) / time_span if time_span > 0 else 0

        print(
            f"  - ä¸»æ—¶é—´æˆ³é•¿åº¦: {len(main_timestamps)} å¸§ (è¦æ±‚ >= {min_required_frames} )"
        )
        print(
            f"  - ä¸»æ—¶é—´æˆ³æ—¶é—´è·¨åº¦: {time_span:.3f}s (è¦æ±‚ >= {min_required_duration}s)"
        )
        print(f"  - ä¸»æ—¶é—´æˆ³é¢‘ç‡: {actual_fps:.2f}Hz (è¦æ±‚ 29.95~30.05Hz)")

        # ç»Ÿè®¡ä¸åŒç±»å‹çš„æ¨¡æ€ï¼ˆæ’é™¤å¤–å‚å’Œcamera_infoï¼‰
        regular_modalities = [
            k
            for k in aligned_data.keys()
            if (
                not k.endswith("_extrinsics")
                and not k.endswith("_camera_info")
                and len(aligned_data[k]) > 0
            )
        ]
        extrinsics_modalities = [
            k
            for k in aligned_data.keys()
            if k.endswith("_extrinsics") and len(aligned_data[k]) > 0
        ]
        camera_info_modalities = [
            k
            for k in aligned_data.keys()
            if k.endswith("_camera_info") and len(aligned_data[k]) > 0
        ]

        print(f"  - å‚ä¸éªŒè¯çš„æ•°æ®æ¨¡æ€æ•°: {len(valid_modalities)}")
        print(f"  - è·³è¿‡éªŒè¯çš„å¤–å‚æ¨¡æ€æ•°: {len(extrinsics_modalities)}")
        print(f"  - è·³è¿‡éªŒè¯çš„ç›¸æœºä¿¡æ¯æ¨¡æ€æ•°: {len(camera_info_modalities)}")
        print(f"  - æœ€å¤§å¸§å†…æ—¶é—´æˆ³å·®å€¼: {max_frame_spread:.1f}ms")

        if len(main_timestamps) > 1:
            print(f"  - ä¸»æ—¶é—´æˆ³å¹³å‡é—´éš”: {np.mean(main_intervals_ms):.1f}ms")
            print(f"  - ä¸»æ—¶é—´æˆ³æœ€å¤§é—´éš”: {max_main_interval:.1f}ms")

        if extrinsics_modalities:
            print(f"  - å¤–å‚æ¨¡æ€: {extrinsics_modalities}")
        if camera_info_modalities:
            print(f"  - ç›¸æœºä¿¡æ¯æ¨¡æ€: {camera_info_modalities}")

        print("=" * 60)

    def align_frame_data(self, data: dict):
        aligned_data = defaultdict(list)
        main_timeline = max(
            self.DEFAULT_CAMERA_NAMES, key=lambda cam_k: len(data.get(cam_k, []))
        )
        jump = self.MAIN_TIMELINE_FPS // self.TRAIN_HZ
        main_img_timestamps = [t["timestamp"] for t in data[main_timeline]][
            self.SAMPLE_DROP : -self.SAMPLE_DROP
        ][::jump]
        min_end = min(
            [data[k][-1]["timestamp"] for k in data.keys() if len(data[k]) > 0]
        )
        main_img_timestamps = [t for t in main_img_timestamps if t < min_end]
        for stamp in main_img_timestamps:
            stamp_sec = stamp
            for key, v in data.items():
                if len(v) > 0:
                    this_obs_time_seq = [this_frame["timestamp"] for this_frame in v]
                    time_array = np.array([t for t in this_obs_time_seq])
                    idx = np.argmin(np.abs(time_array - stamp_sec))
                    aligned_data[key].append(v[idx])
                else:
                    aligned_data[key] = []
        print(
            f"Aligned {key}: {len((data[main_timeline]))} -> {len(next(iter(aligned_data.values())))}"
        )
        for k, v in aligned_data.items():
            if len(v) > 0:
                print(v[0]["timestamp"], v[1]["timestamp"], k)
        return aligned_data

    def list_bag_files(self, bag_dir: str):
        return sorted(glob.glob(os.path.join(bag_dir, "*.bag")))

    def process_rosbag_dir(self, bag_dir: str):
        all_data = []
        # æŒ‰ç…§æ–‡ä»¶åæ’åºï¼Œè·å– bag æ–‡ä»¶åˆ—è¡¨
        bag_files = self.list_bag_files(bag_dir)
        episode_id = 0
        for bf in bag_files:
            print(f"Processing bag file: {bf}")
            episode_data = self.process_rosbag(bf)
            all_data.append(episode_data)

        return all_data


class PostProcessorUtils:

    @staticmethod
    def torque_to_current_batch(
        torque_data: np.ndarray,
        MOTOR_C2T=[
            2,
            1.05,
            1.05,
            2,
            2.1,
            2.1,
            2,
            1.05,
            1.05,
            2,
            2.1,
            2.1,
            1.05,
            5,
            2.3,
            5,
            4.7,
            4.7,
            4.7,
            1.05,
            5,
            2.3,
            5,
            4.7,
            4.7,
            4.7,
            0.21,
            4.7,
        ],
    ):
        """
        å°†æ‰­çŸ©æ•°æ®æ‰¹é‡è½¬æ¢ä¸ºç”µæµæ•°æ®

        Args:
            torque_data: æ‰­çŸ©æ•°æ®æ•°ç»„(N, M)
            MOTOR_C2T: ç”µæµè½¬æ‰­çŸ©ç³»æ•°æ•°ç»„ï¼Œé»˜è®¤å€¼ä¸º kuavo-ros-control ä¸­å®šä¹‰çš„ç³»æ•°
        Returns:
            ç”µæµæ•°æ®æ•°ç»„
        """
        if torque_data.shape[1] != len(MOTOR_C2T):
            print(
                f"è­¦å‘Š: æ‰­çŸ©æ•°æ®é•¿åº¦({torque_data.shape[1]})ä¸C2Tç³»æ•°æ•°é‡({len(MOTOR_C2T)})ä¸åŒ¹é…"
            )
            return None

        # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        current_data = torque_data.copy()

        from itertools import chain

        # 13~18 ä¸ºå·¦è‡‚ruiwoç”µæœºæ•°æ®, 20~27 ä¸ºå³è‡‚ruiwoç”µæœºæ•°æ®
        # å¯¹äºè¿™äº›ç”µæœºéœ€è¦å…ˆé™¤ä»¥MOTOR_C2Tç³»æ•°å†ä¹˜ä»¥2.1
        for i in chain(range(13, 19), range(20, 28)):  # ä¿®æ­£ä¸º27+1=28
            current_data[:, i] = (torque_data[:, i] / MOTOR_C2T[i]) * 2.1

        # 1, 2, 7, 8, 12, 19 å·ç”µæœºéœ€è¦ç‰¹æ®Šå¤„ç†
        for i in [1, 2, 7, 8, 12, 19]:
            current_data[:, i] = (torque_data[:, i] / MOTOR_C2T[i]) * 1.2

        # å…¶ä»–ç”µæœºï¼šECç”µæœºï¼Œç›´æ¥é™¤ä»¥MOTOR_C2Tç³»æ•°
        other_indices = [
            i
            for i in range(len(MOTOR_C2T))
            if i not in chain(range(13, 19), range(20, 28), [1, 2, 7, 8, 12, 19])
        ]
        for i in other_indices:
            current_data[:, i] = torque_data[:, i] / MOTOR_C2T[i]

        return current_data

    @staticmethod
    def current_to_torque(
        current_data: np.ndarray,
        MOTOR_C2T=[
            2,
            1.05,
            1.05,
            2,
            2.1,
            2.1,
            2,
            1.05,
            1.05,
            2,
            2.1,
            2.1,
            1.05,
            5,
            2.3,
            5,
            4.7,
            4.7,
            4.7,
            1.05,
            5,
            2.3,
            5,
            4.7,
            4.7,
            4.7,
            0.21,
            4.7,
        ],
    ):
        """
        å°† sensors_data_raw ä¸­çš„ joint_torque ç”µæµæ•°æ®è½¬æ¢ä¸ºæ‰­çŸ©æ•°æ®

        Args:
            current_data: ç”µæµæ•°æ®æ•°ç»„(N, 28)
            MOTOR_C2T: ç”µæµè½¬æ‰­çŸ©ç³»æ•°æ•°ç»„ï¼Œé»˜è®¤å€¼ä¸º kuavo-ros-control ä¸­å®šä¹‰çš„ç³»æ•°
        Returns:
            æ‰­çŸ©æ•°æ®æ•°ç»„
        """
        if len(current_data) != len(MOTOR_C2T):
            print(
                f"è­¦å‘Š: ç”µæµæ•°æ®é•¿åº¦({len(current_data)})ä¸C2Tç³»æ•°æ•°é‡({len(MOTOR_C2T)})ä¸åŒ¹é…"
            )
            # æ‰©å±•æˆ–æˆªæ–­ç³»æ•°æ•°ç»„
            return None

        torque_data = []
        # "MOTORS_TYPE":[
        # "PA100_18", "PA100", "PA100", "PA100_18", "CK", "CK",
        # "PA100_18", "PA100", "PA100", "PA100_18", "CK", "CK",
        # "PA100", "ruiwo", "ruiwo", "ruiwo", "ruiwo", "ruiwo", "ruiwo",
        # "PA100", "ruiwo", "ruiwo", "ruiwo", "ruiwo", "ruiwo", "ruiwo", "ruiwo", "ruiwo"],

        for i, current in enumerate(current_data):
            # kuavo-ros-control/src/kuavo_common/include/kuavo_common/common/kuavo_settings.h
            # ä¸­å®šä¹‰äº† ruiwo ç”µæœºç”µæµè½¬æ‰­çŸ©ç³»æ•° CK_C2T = 2.1ï¼Œæ‰€ä»¥è¿™é‡Œé™¤ä»¥ 2.1 è½¬åŒ–å›åŸå§‹ç”µæµ

            # 13~18 ä¸ºå·¦è‡‚ruiwoç”µæœºæ•°æ®, 20~25 ä¸ºå³è‡‚ruiwoç”µæœºæ•°æ®
            # å¯¹äºè¿™äº›ç”µæœºéœ€è¦å…ˆé™¤ä»¥2.1è½¬æ¢å›åŸå§‹ç”µæµ
            if 13 <= i <= 18 or 20 <= i <= 27:
                torque = (current / 2.1) * MOTOR_C2T[i]
            elif i == 1 or i == 2 or i == 7 or i == 8 or i == 12 or i == 19:
                torque = (current / 1.2) * MOTOR_C2T[i]
            else:

                # EC ç”µæœº sensors_data_raw ä¸­å·²ç»æ˜¯æ‰­çŸ©å€¼
                torque = current
            torque_data.append(torque)

        return np.array(torque_data)

    @staticmethod
    def current_to_torque_batch(
        current_data: np.ndarray,
        MOTOR_C2T=[
            2,
            1.05,
            1.05,
            2,
            2.1,
            2.1,
            2,
            1.05,
            1.05,
            2,
            2.1,
            2.1,
            1.05,
            5,
            2.3,
            5,
            4.7,
            4.7,
            4.7,
            1.05,
            5,
            2.3,
            5,
            4.7,
            4.7,
            4.7,
            0.21,
            4.7,
        ],
    ):
        """
        å°† sensors_data_raw ä¸­çš„ joint_torque ç”µæµæ•°æ®è½¬æ¢ä¸ºæ‰­çŸ©æ•°æ®

        Args:
            current_data: ç”µæµæ•°æ®æ•°ç»„(N, M)
            MOTOR_C2T: ç”µæµè½¬æ‰­çŸ©ç³»æ•°æ•°ç»„ï¼Œé»˜è®¤å€¼ä¸º kuavo-ros-control ä¸­å®šä¹‰çš„ç³»æ•°
        Returns:
            æ‰­çŸ©æ•°æ®æ•°ç»„
        """
        if current_data.shape[1] != len(MOTOR_C2T):
            print(
                f"è­¦å‘Š: ç”µæµæ•°æ®é•¿åº¦({current_data.shape[1]})ä¸C2Tç³»æ•°æ•°é‡({len(MOTOR_C2T)})ä¸åŒ¹é…"
            )
            # æ‰©å±•æˆ–æˆªæ–­ç³»æ•°æ•°ç»„
            return None

        from itertools import chain

        for i in chain(range(13, 19), range(20, 28)):
            current_data[:, i] = current_data[:, i] / 2.1 * MOTOR_C2T[i]
        for i in [1, 2, 7, 8, 12, 19]:
            current_data[:, i] = current_data[:, i] / 1.2 * MOTOR_C2T[i]
        # å¯¹äºå…¶ä»–ç”µæœºç›´æ¥ä½¿ç”¨åŸå§‹ç”µæµ
        # EC ç”µæœº sensors_data_raw ä¸­å·²ç»æ˜¯æ‰­çŸ©å€¼
        return current_data

    @staticmethod
    def save_to_hdf5(low_dim_data, file_path):
        """å°†æ•°æ®ä¿å­˜ä¸ºç¬¦åˆåº“å¸•æ€é€šç”¨ç‰ˆæ•°æ®æ ¼å¼çš„HDF5æ–‡ä»¶"""
        import h5py

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        def create_datasets_recursively(group, data_dict, current_path=""):
            """é€’å½’åˆ›å»ºæ•°æ®é›†å’Œç»„"""
            for key, value in data_dict.items():
                full_path = f"{current_path}/{key}" if current_path else key

                if isinstance(value, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œåˆ›å»ºå­ç»„å¹¶é€’å½’å¤„ç†
                    subgroup = group.create_group(key)
                    create_datasets_recursively(subgroup, value, full_path)
                else:
                    # å¦‚æœæ˜¯æ•°æ®ï¼Œåˆ›å»ºæ•°æ®é›†
                    try:
                        # å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®
                        if isinstance(value, (list, tuple)):
                            value = np.array(value)

                        # æ ¹æ®æ•°æ®ç±»å‹å’Œè·¯å¾„è¿›è¡Œç‰¹æ®Šå¤„ç†
                        processed_value = process_data_by_path(value, full_path)

                        # åˆ›å»ºæ•°æ®é›†
                        group.create_dataset(key, data=processed_value)
                        print(
                            f"åˆ›å»ºæ•°æ®é›†: {full_path}, å½¢çŠ¶: {processed_value.shape}, ç±»å‹: {processed_value.dtype}"
                        )

                    except Exception as e:
                        print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºæ•°æ®é›† {full_path}: {e}")
                        # åˆ›å»ºç©ºæ•°æ®é›†ä½œä¸ºå ä½ç¬¦
                        try:
                            empty_data = np.array([])
                            group.create_dataset(key, data=empty_data)
                        except:
                            pass

        def process_data_by_path(value, path):
            """æ ¹æ®æ•°æ®è·¯å¾„å¯¹æ•°æ®è¿›è¡Œç‰¹æ®Šå¤„ç†"""
            # æ—¶é—´æˆ³å¤„ç† - æ‰©å±•è¯†åˆ«æ–°çš„æ—¶é—´æˆ³å­—æ®µ
            timestamp_fields = [
                "timestamps",
                "head_color_mp4_camera_timestamps",
                "hand_left_color_mp4_timestamps",
                "hand_right_color_mp4_timestamps",
                "head_depth_mkv_camera_timestamps",
                "hand_left_depth_mkv_timestamps",
                "hand_right_depth_mkv_timestamps",
                "camera_extrinsics_timestamps",
                "head_timestamps" "joint_timestamps",
                "effector_dexhand_timestamps",
                "effector_lejuclaw_timestamps",
            ]

            if any(ts_field in path for ts_field in timestamp_fields):
                if value.dtype != np.int64:
                    # è½¬æ¢æ—¶é—´æˆ³ä¸ºçº³ç§’çº§æ•´æ•°
                    if np.issubdtype(value.dtype, np.floating):
                        return (value * 1e9).astype(np.int64)
                    else:
                        return value.astype(np.int64)
                return value

            # ç´¢å¼•æ•°æ®å¤„ç†
            elif "index" in path:
                return value.astype(np.int64)

            # å…¶ä»–æ•°å€¼æ•°æ®å¤„ç†
            elif np.issubdtype(value.dtype, np.number):
                # æ ¹æ®æ•°æ®ç±»å‹å†³å®šç²¾åº¦
                if np.issubdtype(value.dtype, np.integer):
                    return value.astype(np.int32)
                else:
                    return value.astype(np.float32)

            # ä¿æŒåŸå§‹æ•°æ®ç±»å‹
            return value

        def add_missing_required_fields(f, low_dim_data):
            """æ·»åŠ åº“å¸•æ€æ ¼å¼ä¸­å¿…éœ€ä½†ç¼ºå¤±çš„å­—æ®µï¼Œä½¿ç”¨nullæœºåˆ¶"""

            # è·å–æ—¶é—´æˆ³é•¿åº¦ä½œä¸ºå‚è€ƒ
            if "timestamps" in low_dim_data:
                N = len(low_dim_data["timestamps"])
            else:
                N = 1000  # é»˜è®¤å€¼
                for key, value in low_dim_data.items():
                    if hasattr(value, "__len__") and not isinstance(value, str):
                        N = len(value)
                        break

            # åˆ›å»ºæ§åˆ¶ç´¢å¼•
            control_indices = np.arange(N, dtype=np.int64)

            def create_null_dataset(group, name, shape, dtype):
                """åˆ›å»ºä¸€ä¸ªè¡¨ç¤ºç¼ºå¤±æ•°æ®çš„æ•°æ®é›†"""
                # æ–¹æ³•1: ä½¿ç”¨NaNè¡¨ç¤ºç¼ºå¤±æ•°æ®ï¼ˆä»…é€‚ç”¨äºæµ®ç‚¹æ•°ï¼‰
                if dtype == np.float32 or dtype == np.float64:
                    data = np.full(shape, np.nan, dtype=dtype)
                    dataset = group.create_dataset(name, data=data)
                    # æ·»åŠ å±æ€§æ ‡è®°è¿™æ˜¯ç¼ºå¤±æ•°æ®
                    dataset.attrs["missing_data"] = True
                    dataset.attrs["description"] = f"Missing data filled with NaN"
                    return dataset

                # æ–¹æ³•2: åˆ›å»ºç©ºæ•°æ®é›†ï¼ˆå¯¹äºæ•´æ•°ç±»å‹ï¼‰
                elif np.issubdtype(dtype, np.integer):
                    # å¯¹äºæ•´æ•°ï¼Œä½¿ç”¨æœ€å°å€¼è¡¨ç¤ºç¼ºå¤±
                    if dtype == np.int32:
                        fill_value = np.iinfo(np.int32).min
                    elif dtype == np.int64:
                        fill_value = np.iinfo(np.int64).min
                    else:
                        fill_value = -999999  # é»˜è®¤ç¼ºå¤±å€¼

                    data = np.full(shape, fill_value, dtype=dtype)
                    dataset = group.create_dataset(name, data=data)
                    dataset.attrs["missing_data"] = True
                    dataset.attrs["fill_value"] = fill_value
                    dataset.attrs["description"] = (
                        f"Missing data filled with {fill_value}"
                    )
                    return dataset

                # æ–¹æ³•3: ä¸åˆ›å»ºæ•°æ®é›†ï¼Œä»…æ·»åŠ å ä½ç¬¦å±æ€§
                else:
                    # åˆ›å»ºä¸€ä¸ªåªæœ‰å±æ€§çš„ç»„æ¥è¡¨ç¤ºç¼ºå¤±
                    missing_group = group.create_group(name + "_missing")
                    missing_group.attrs["missing_data"] = True
                    missing_group.attrs["expected_shape"] = shape
                    missing_group.attrs["expected_dtype"] = str(dtype)
                    missing_group.attrs["description"] = (
                        "Data not available - missing field"
                    )
                    return missing_group

            def create_optional_dataset(
                group, name, shape, dtype, description="Optional field not available"
            ):
                """åˆ›å»ºå¯é€‰çš„æ•°æ®é›†ï¼Œæ˜ç¡®æ ‡è®°ä¸ºä¸å¯ç”¨"""
                # æ–¹æ³•4: åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†ï¼Œé•¿åº¦ä¸º0
                empty_data = np.array([], dtype=dtype)
                dataset = group.create_dataset(name, data=empty_data, maxshape=shape)
                dataset.attrs["data_available"] = False
                dataset.attrs["expected_shape"] = shape
                dataset.attrs["description"] = description
                return dataset

            # æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„ action ç»„å­—æ®µ
            if "action" in f:
                action_group = f["action"]

                # # æ·»åŠ ç¼ºå¤±çš„ robot ç»„
                # if "robot" not in action_group:
                #     robot_group = action_group.create_group("robot")
                #     create_null_dataset(robot_group, "velocity", (N, 2), np.float32)
                #     create_null_dataset(robot_group, "index", (N,), np.float32)
                #     print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: action/robot (ä½¿ç”¨NaNè¡¨ç¤ºç¼ºå¤±)")

                # # æ·»åŠ ç¼ºå¤±çš„ waist ç»„
                # if "waist" not in action_group:
                #     waist_group = action_group.create_group("waist")
                #     create_null_dataset(waist_group, "position", (N, 2), np.float32)
                #     create_null_dataset(waist_group, "index", (N,), np.float32)
                #     print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: action/waist (ä½¿ç”¨NaNè¡¨ç¤ºç¼ºå¤±)")

                # # æ·»åŠ ç¼ºå¤±çš„ end ç»„
                # if "end" not in action_group:
                #     end_group = action_group.create_group("end")
                #     create_null_dataset(end_group, "orientation", (N, 2, 4), np.float32)
                #     create_null_dataset(end_group, "position", (N, 2, 3), np.float32)
                #     create_null_dataset(end_group, "index", (N,), np.float32)
                #     print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: action/end (ä½¿ç”¨NaNè¡¨ç¤ºç¼ºå¤±)")

            # æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„ state ç»„å­—æ®µ
            if "state" in f:
                state_group = f["state"]

                # # æ·»åŠ ç¼ºå¤±çš„ end ç»„
                # if "end" not in state_group:
                #     end_group = state_group.create_group("end")
                #     create_null_dataset(end_group, "angular", (N, 2, 3), np.float32)
                #     create_null_dataset(end_group, "orientation", (N, 2, 4), np.float32)
                #     create_null_dataset(end_group, "position", (N, 2, 3), np.float32)
                #     create_null_dataset(end_group, "velocity", (N, 2, 3), np.float32)
                #     create_null_dataset(end_group, "wrench", (N, 2, 6), np.float32)
                #     print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: state/end (ä½¿ç”¨NaNè¡¨ç¤ºç¼ºå¤±)")

                # æ·»åŠ ç¼ºå¤±çš„ robot ç»„
                if "robot" not in state_group:
                    robot_group = state_group.create_group("robot")

                    # å¯¹äºæœºå™¨äººå§¿æ€ï¼Œå¦‚æœæ²¡æœ‰IMUæ•°æ®ï¼Œæ˜ç¡®æ ‡è®°ä¸ºç¼ºå¤±
                    if "imu" in low_dim_data and "quat_xyzw" in low_dim_data["imu"]:
                        imu_data_quat_xyzw = low_dim_data["imu"]["quat_xyzw"]
                        if (
                            hasattr(imu_data_quat_xyzw, "shape")
                            and len(imu_data_quat_xyzw.shape) > 1
                            and imu_data_quat_xyzw.shape[1] >= 4
                        ):
                            # æœ‰IMUæ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
                            orientation = np.zeros((N, 4), dtype=np.float32)
                            orientation[:, :] = imu_data_quat_xyzw
                            dataset = robot_group.create_dataset(
                                "orientation", data=orientation
                            )
                            dataset.attrs["data_source"] = "IMU sensor"
                            dataset.attrs["missing_data"] = False
                            print(f"ä»IMUæ•°æ®æå–æœºå™¨äººå§¿æ€")
                        else:
                            # IMUæ•°æ®æ ¼å¼ä¸å¯¹ï¼Œæ ‡è®°ä¸ºç¼ºå¤±
                            create_null_dataset(
                                robot_group, "orientation", (N, 4), np.float32
                            )
                            print(f"IMUæ•°æ®æ ¼å¼å¼‚å¸¸ï¼Œå§¿æ€æ•°æ®æ ‡è®°ä¸ºç¼ºå¤±")
                    else:
                        # æ²¡æœ‰IMUæ•°æ®ï¼Œæ ‡è®°ä¸ºç¼ºå¤±
                        create_null_dataset(
                            robot_group, "orientation", (N, 4), np.float32
                        )
                        print(f"æ— IMUæ•°æ®ï¼Œå§¿æ€æ•°æ®æ ‡è®°ä¸ºç¼ºå¤±")

                    # å…¶ä»–æœºå™¨äººçŠ¶æ€æ ‡è®°ä¸ºç¼ºå¤±
                    # create_null_dataset(robot_group, "orientation_drift", (N, 4), np.float32)
                    # create_null_dataset(robot_group, "position", (N, 3), np.float32)
                    # create_null_dataset(robot_group, "position_drift", (N, 3), np.float32)
                    print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: state/robot (ä½¿ç”¨NaN/ç¼ºå¤±å€¼è¡¨ç¤º)")

                # # æ·»åŠ ç¼ºå¤±çš„ waist ç»„
                # if "waist" not in state_group:
                #     waist_group = state_group.create_group("waist")
                #     create_null_dataset(waist_group, "effort", (N, 2), np.float32)
                #     create_null_dataset(waist_group, "position", (N, 2), np.float32)
                #     create_null_dataset(waist_group, "velocity", (N, 2), np.float32)
                #     print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: state/waist (ä½¿ç”¨NaNè¡¨ç¤ºç¼ºå¤±)")

                # ä¸ºç°æœ‰ç»„æ·»åŠ ç¼ºå¤±çš„æ•°æ®é›†
                # if "effector" in state_group:
                #     effector_group = state_group["effector"]
                #     if "force" not in effector_group:
                #         create_null_dataset(effector_group, "force", (N, 2), np.float32)
                #         print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: state/effector/force (ä½¿ç”¨NaNè¡¨ç¤ºç¼ºå¤±)")

                if "head" in state_group:
                    head_group = state_group["head"]
                    if "effort" not in head_group:
                        create_null_dataset(head_group, "effort", (N, 2), np.float32)
                        print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: state/head/effort (ä½¿ç”¨NaNè¡¨ç¤ºç¼ºå¤±)")

                if "joint" in state_group:
                    joint_group = state_group["joint"]
                    # è·å–å…³èŠ‚æ•°é‡
                    joint_count = 14  # é»˜è®¤å€¼
                    if "position" in joint_group:
                        joint_count = joint_group["position"].shape[1]
                    elif "velocity" in joint_group:
                        joint_count = joint_group["velocity"].shape[1]

                    if "current_value" not in joint_group:
                        create_null_dataset(
                            joint_group, "current_value", (N, joint_count), np.float32
                        )
                        print(
                            f"æ·»åŠ ç¼ºå¤±å­—æ®µ: state/joint/current_value (ä½¿ç”¨NaNè¡¨ç¤ºç¼ºå¤±)"
                        )

                    if "effort" not in joint_group:
                        create_null_dataset(
                            joint_group, "effort", (N, joint_count), np.float32
                        )
                        print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: state/joint/effort (ä½¿ç”¨NaNè¡¨ç¤ºç¼ºå¤±)")

            # æ·»åŠ  other_sensors ç»„ï¼ˆæ ‡è®°ä¸ºå¯é€‰ï¼‰
            # if "other_sensors" not in f:
            #     other_group = f.create_group("other_sensors")
            #     other_group.attrs['description'] = 'Optional sensor data - currently empty'
            #     other_group.attrs['data_available'] = False
            #     print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: other_sensors (æ ‡è®°ä¸ºå¯é€‰æ•°æ®)")
            # æ–°å¢ï¼šåœ¨æ ¹çº§åˆ«æ·»åŠ æ—¶é—´æˆ³å­—æ®µçš„å­˜åœ¨æ€§ä¿¡æ¯

        # åˆ›å»º HDF5 æ–‡ä»¶
        with h5py.File(file_path, "w") as f:
            print(f"å¼€å§‹åˆ›å»ºHDF5æ–‡ä»¶: {file_path}")

            # é€’å½’åˆ›å»ºæ‰€æœ‰æ•°æ®é›†å’Œç»„
            create_datasets_recursively(f, low_dim_data)

            # æ·»åŠ åº“å¸•æ€æ ¼å¼è¦æ±‚çš„ç¼ºå¤±å­—æ®µå¡«å……ä¸ºNaNæˆ–ç¼ºå¤±å€¼
            add_missing_required_fields(f, low_dim_data)

        print(f"æ•°æ®å·²æˆåŠŸä¿å­˜ä¸ºHDF5æ ¼å¼: {file_path}")
        return file_path


if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    print("åˆ›å»ºæµ‹è¯•å®ä¾‹...")

    # æ¨¡æ‹Ÿé…ç½®
    class TestConfig:
        def __init__(self):
            self.default_camera_names = ["head_cam_h"]
            self.train_hz = 30
            self.main_timeline_fps = 30
            self.sample_drop = 0
            self.resize = type("obj", (object,), {"width": 640, "height": 480})()
            self.topics = []
            self.eef_type = "dexhand"

    config = TestConfig()
    reader = KuavoRosbagReader(config)

    # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼šæ¨¡æ‹Ÿå‰ç½®æ­¥éª¤å¤„ç†åçš„æ•°æ®ç‰¹å¾
    # - æ‰€æœ‰é—´éš”éƒ½å°äº40ms
    # - æ€»å¸§ç‡çº¦32Hzï¼ˆéœ€è¦åˆ é™¤å¸§é™åˆ°30Hzï¼‰
    print("åˆ›å»ºæµ‹è¯•æ•°æ®...")

    # ç”Ÿæˆ32Hzçš„åŸºæœ¬æ—¶é—´æˆ³åºåˆ—
    base_interval = 1.0 / 32.0  # 32Hz = 31.25msé—´éš”
    total_frames = 800  # è¶³å¤Ÿé•¿çš„æ•°æ®
    total_duration = total_frames * base_interval  # æ€»æ—¶é•¿

    # åˆ›å»ºå‡åŒ€çš„32Hzæ—¶é—´æˆ³ä½œä¸ºåŸºç¡€
    uniform_timestamps = np.linspace(1.0, 1.0 + total_duration, total_frames)

    # æ·»åŠ ä¸€äº›éšæœºæ€§ï¼Œä½†ç¡®ä¿é—´éš”å§‹ç»ˆ<40ms
    timestamps = []
    for i in range(total_frames):
        base_ts = uniform_timestamps[i]

        if i == 0:
            # ç¬¬ä¸€å¸§ä¿æŒä¸å˜
            timestamps.append(base_ts)
        else:
            # æ·»åŠ éšæœºåç§»ï¼Œä½†ç¡®ä¿ä¸å‰ä¸€å¸§çš„é—´éš”åœ¨15-38msä¹‹é—´
            prev_ts = timestamps[-1]
            min_interval = 0.015  # 15ms
            max_interval = 0.038  # 38ms

            # è®¡ç®—ç†æƒ³çš„ä¸‹ä¸€ä¸ªæ—¶é—´æˆ³
            ideal_next = prev_ts + base_interval

            # æ·»åŠ éšæœºåç§»ï¼Œä½†é™åˆ¶åœ¨å®‰å…¨èŒƒå›´å†…
            random_offset = np.random.uniform(-0.008, 0.008)  # Â±8mséšæœºåç§»
            candidate_ts = ideal_next + random_offset

            # ç¡®ä¿é—´éš”åœ¨å®‰å…¨èŒƒå›´å†…
            actual_interval = candidate_ts - prev_ts
            if actual_interval < min_interval:
                candidate_ts = prev_ts + min_interval
            elif actual_interval > max_interval:
                candidate_ts = prev_ts + max_interval

            timestamps.append(candidate_ts)

    main_timestamps = np.array(timestamps)

    # éªŒè¯ç”Ÿæˆçš„æ—¶é—´æˆ³è´¨é‡
    intervals_ms = np.diff(main_timestamps) * 1000
    max_interval_ms = np.max(intervals_ms)
    min_interval_ms = np.min(intervals_ms)
    avg_interval_ms = np.mean(intervals_ms)

    # ç¡®ä¿æ‰€æœ‰é—´éš”éƒ½å°äº40ms
    assert (
        max_interval_ms < 40.0
    ), f"ç”Ÿæˆçš„æœ€å¤§é—´éš” {max_interval_ms:.1f}ms è¶…è¿‡40msé™åˆ¶"

    # å­æ—¶é—´æˆ³ï¼šæ¯”ä¸»æ—¶é—´æˆ³æ™š2ms
    child_timestamps = main_timestamps + 0.002

    # åˆ›å»ºå¯¹åº”çš„æ•°æ®
    valid_modalities = {
        "head_cam_h": [
            {"timestamp": ts, "data": f"main_frame_{i}", "frame_id": i}
            for i, ts in enumerate(main_timestamps)
        ],
        "child_sensor": [
            {"timestamp": ts, "data": f"child_data_{i}", "sensor_value": i * 10}
            for i, ts in enumerate(child_timestamps)
        ],
    }

    # è®¡ç®—åˆå§‹å¸§ç‡
    time_span = main_timestamps[-1] - main_timestamps[0]
    initial_fps = len(main_timestamps) / time_span
    target_fps = 30.095

    print(f"æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ:")
    print(f"  ä¸»æ—¶é—´æˆ³é•¿åº¦: {len(main_timestamps)}")
    print(f"  æ—¶é—´è·¨åº¦: {time_span:.3f}s")
    print(f"  åˆå§‹å¸§ç‡: {initial_fps:.2f}Hz")
    print(f"  ç›®æ ‡å¸§ç‡: {target_fps:.2f}Hz")
    print(f"  éœ€è¦åˆ é™¤çº¦ {len(main_timestamps) - int(time_span * target_fps)} å¸§")

    print(f"  æ—¶é—´é—´éš”ç»Ÿè®¡:")
    print(f"    å¹³å‡é—´éš”: {avg_interval_ms:.1f}ms")
    print(f"    æœ€å¤§é—´éš”: {max_interval_ms:.1f}ms")
    print(f"    æœ€å°é—´éš”: {min_interval_ms:.1f}ms")
    print(f"    âœ“ æ‰€æœ‰é—´éš”éƒ½åœ¨40msä»¥å†…ï¼ˆæ¨¡æ‹Ÿå‰ç½®å¤„ç†å®Œæˆï¼‰")

    # éªŒè¯å¸§ç‡åˆç†æ€§
    if 31.5 <= initial_fps <= 33.0:
        print(f"    âœ“ åˆå§‹å¸§ç‡ {initial_fps:.2f}Hz åœ¨æœŸæœ›èŒƒå›´å†…ï¼ˆ31.5-33Hzï¼‰")
    else:
        print(f"    âš ï¸ åˆå§‹å¸§ç‡ {initial_fps:.2f}Hz ä¸åœ¨æœŸæœ›èŒƒå›´å†…")

    print("\nåˆå§‹æ•°æ®æ ·æœ¬ï¼ˆå‰10å¸§ï¼‰:")
    print("ä¸»æ—¶é—´æˆ³:")
    for i in range(min(10, len(main_timestamps))):
        interval_ms = 0
        if i > 0:
            interval_ms = (main_timestamps[i] - main_timestamps[i - 1]) * 1000
        print(f"  å¸§{i}: {main_timestamps[i]:.6f}s (é—´éš”: {interval_ms:.1f}ms)")

    print("\nå­æ—¶é—´æˆ³æ ·æœ¬ï¼ˆå‰5å¸§ï¼‰:")
    for i in range(min(5, len(valid_modalities["child_sensor"]))):
        item = valid_modalities["child_sensor"][i]
        main_ts = main_timestamps[i]
        diff_ms = (item["timestamp"] - main_ts) * 1000
        print(
            f"  å¸§{i}: {item['timestamp']:.6f}s (ä¸ä¸»æ—¶é—´æˆ³å·®: {diff_ms:.1f}ms, æ•°æ®: {item['data']})"
        )

    print("\nå¼€å§‹æµ‹è¯• _remove_frames_to_decrease_fps...")
    print("=" * 60)

    try:
        # è°ƒç”¨å‡½æ•°è¿›è¡Œæµ‹è¯•
        result_timestamps, result_modalities = reader._remove_frames_to_decrease_fps(
            main_timestamps.copy(),  # ä½¿ç”¨å‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
            {k: list(v) for k, v in valid_modalities.items()},  # æ·±æ‹·è´
            target_fps,
            time_span,
        )

        print("=" * 60)
        print("æµ‹è¯•å®Œæˆ!")

        # éªŒè¯ç»“æœ
        final_time_span = result_timestamps[-1] - result_timestamps[0]
        final_fps = len(result_timestamps) / final_time_span

        print(f"\nç»“æœç»Ÿè®¡:")
        print(f"  æœ€ç»ˆæ—¶é—´æˆ³é•¿åº¦: {len(result_timestamps)}")
        print(f"  æœ€ç»ˆæ—¶é—´è·¨åº¦: {final_time_span:.3f}s")
        print(f"  æœ€ç»ˆå¸§ç‡: {final_fps:.3f}Hz")
        print(f"  åˆ é™¤å¸§æ•°: {len(main_timestamps) - len(result_timestamps)}")

        # éªŒè¯æœ€ç»ˆæ—¶é—´æˆ³è´¨é‡
        if len(result_timestamps) > 1:
            final_intervals_ms = np.diff(result_timestamps) * 1000
            max_final_interval = np.max(final_intervals_ms)
            avg_final_interval = np.mean(final_intervals_ms)
            std_final_interval = np.std(final_intervals_ms)

            print(f"\næœ€ç»ˆæ—¶é—´æˆ³è´¨é‡:")
            print(f"  æœ€å¤§é—´éš”: {max_final_interval:.1f}ms")
            print(f"  å¹³å‡é—´éš”: {avg_final_interval:.1f}ms")
            print(f"  é—´éš”æ ‡å‡†å·®: {std_final_interval:.1f}ms")

            if max_final_interval <= 40:
                print(f"  âœ“ æ‰€æœ‰é—´éš”éƒ½åœ¨40msä»¥å†…")
            else:
                large_final_intervals = np.sum(final_intervals_ms > 40)
                print(f"  âŒ ä»æœ‰ {large_final_intervals} ä¸ªé—´éš”è¶…è¿‡40ms")

        # éªŒè¯å­æ—¶é—´æˆ³åŒæ­¥æ€§ï¼ˆæŠ½æ ·æ£€æŸ¥ï¼‰
        print(f"\nå­æ—¶é—´æˆ³åŒæ­¥æ€§éªŒè¯ï¼ˆæŠ½æ ·æ£€æŸ¥å‰20å¸§ï¼‰:")
        sync_errors = 0
        check_frames = min(
            20, len(result_timestamps), len(result_modalities["child_sensor"])
        )

        for i in range(check_frames):
            main_ts = result_timestamps[i]
            child_item = result_modalities["child_sensor"][i]
            child_ts = child_item["timestamp"]
            expected_diff = 0.002  # åŸå§‹2mså·®å€¼
            actual_diff = child_ts - main_ts
            diff_error = abs(actual_diff - expected_diff) * 1000

            if diff_error > 0.1:  # 0.1mså®¹å·®
                sync_errors += 1
                print(f"  å¸§{i}: åŒæ­¥åå·® {diff_error:.3f}ms")

                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡æ–°å¹³å‡è¿‡çš„å¸§
                reaveraged = child_item.get("timestamp_reaveraged", False)
                if reaveraged:
                    delta = child_item.get("timestamp_delta", 0)
                    print(f"        (è¯¥å¸§å·²é‡æ–°å¹³å‡, delta: {delta:.6f}s)")

        if sync_errors == 0:
            print(f"  âœ“ æŠ½æ ·æ£€æŸ¥çš„ {check_frames} å¸§éƒ½ä¿æŒäº†2msçš„ç›¸å¯¹å…³ç³»")
        else:
            print(f"  âŒ åœ¨ {check_frames} å¸§ä¸­å‘ç° {sync_errors} ä¸ªåŒæ­¥åå·®")

        # æ˜¾ç¤ºå¤„ç†å‰åçš„å¯¹æ¯”
        print(f"\nå¤„ç†å‰åå¯¹æ¯”:")
        print(
            f"  é•¿åº¦: {len(main_timestamps)} -> {len(result_timestamps)} (-{len(main_timestamps) - len(result_timestamps)})"
        )
        print(f"  å¸§ç‡: {initial_fps:.2f}Hz -> {final_fps:.2f}Hz")
        print(f"  æœ€å¤§é—´éš”: {max_interval_ms:.1f}ms -> {max_final_interval:.1f}ms")

        # æµ‹è¯•ç»“è®º
        length_ok = len(result_timestamps) >= 300  # æœ€ç»ˆé•¿åº¦è¶³å¤Ÿ
        fps_ok = final_fps <= target_fps  # å¸§ç‡è¾¾æ ‡
        interval_ok = max_final_interval <= 40  # é—´éš”è¾¾æ ‡
        sync_ok = sync_errors == 0  # åŒæ­¥è¾¾æ ‡

        print(f"\næµ‹è¯•ç»“è®º:")
        print(
            f"  é•¿åº¦æ£€æŸ¥: {'âœ…' if length_ok else 'âŒ'} ({len(result_timestamps)} >= 300)"
        )
        print(
            f"  å¸§ç‡æ£€æŸ¥: {'âœ…' if fps_ok else 'âŒ'} ({final_fps:.2f} <= {target_fps})"
        )
        print(
            f"  é—´éš”æ£€æŸ¥: {'âœ…' if interval_ok else 'âŒ'} ({max_final_interval:.1f} <= 40ms)"
        )
        print(f"  åŒæ­¥æ£€æŸ¥: {'âœ…' if sync_ok else 'âŒ'}")

        if length_ok and fps_ok and interval_ok and sync_ok:
            print(f"  ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ»‘åŠ¨çª—å£åˆ é™¤+é‡æ–°å¹³å‡ç®—æ³•å·¥ä½œæ­£å¸¸")
        else:
            print(f"  âš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

        # é¢å¤–æ£€æŸ¥ï¼šéªŒè¯åˆ é™¤+é‡æ–°å¹³å‡çš„æ•ˆæœ
        print(f"\né‡æ–°å¹³å‡æ•ˆæœéªŒè¯:")
        reaveraged_count = 0
        for item in result_modalities["child_sensor"]:
            if item.get("timestamp_reaveraged", False):
                reaveraged_count += 1

        if reaveraged_count > 0:
            print(f"  âœ“ å…±æœ‰ {reaveraged_count} å¸§ç»è¿‡é‡æ–°å¹³å‡å¤„ç†")
            print(
                f"  âœ“ é‡æ–°å¹³å‡æ¯”ä¾‹: {reaveraged_count/len(result_modalities['child_sensor'])*100:.1f}%"
            )
        else:
            print(f"  âš ï¸ æ²¡æœ‰å¸§ç»è¿‡é‡æ–°å¹³å‡å¤„ç†")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥ï¼Œå‡ºç°å¼‚å¸¸:")
        print(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"  é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback

        traceback.print_exc()
