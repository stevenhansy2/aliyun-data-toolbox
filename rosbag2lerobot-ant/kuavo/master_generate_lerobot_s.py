"""
Script to convert Kuavo rosbag data to the LeRobot dataset v2.0 format.

Example usage: uv run examples/aloha_real/convert_aloha_data_to_lerobot.py --raw-dir /path/to/raw/data --repo-id <org>/<dataset-name>
ç°åœ¨idæ˜¯å”¯ä¸€æŒ‡ç¤ºç‰ˆæœ¬å˜é‡ï¼Œä¿®æ”¹äº†å…¥å‚çš„ç»“æ„ï¼Œæ·»åŠ äº†æè¿°ä¿¡æ¯è‡³æ¯ä¸ªbagçš„æ¯ä¸ªstepä¸­ï¼Œæ·»åŠ äº†ä½¿ç”¨ks_standardä¸‹è½½bagï¼Œé€šè¿‡é™åˆ¶çº¿ç¨‹ä¸ªæ•°å‡å°‘å†…å­˜å ç”¨ï¼Œä¸ºæœ€æ–°ç‰ˆæœ¬ã€‚å¯¹åº”jsonå…¥å‚ä¸º request_new2.json
"""

from merge_batches import (
    merge_parquet_files,
    merge_meta_files,
    get_batch_dirs,
    merge_metadata,
)
from collections import OrderedDict
import custom_patches
import uuid
import psutil
import gc
import dataclasses
from pathlib import Path
import shutil
from typing import Literal
import sys
from lerobot.datasets.lerobot_dataset import HF_LEROBOT_HOME
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.datasets.compute_stats import get_feature_stats
import numpy as np
import torch
import tqdm
import json
from config_dataset_slave import Config, load_config_from_json
import argparse
import requests
import time
import uuid
import joblib
import gc
from copy import deepcopy
from slave_utils import (
    move_and_rename_depth_videos,
    save_camera_extrinsic_params,
    save_camera_info_to_json_new,
    save_depth_videos_16U_parallel,
    save_depth_videos_enhanced_parallel,
    swap_left_right_data_if_needed,
    flip_camera_arrays_if_needed,
)
from kuavo_dataset_slave_s import (
    KuavoRosbagReader,
    DEFAULT_JOINT_NAMES_LIST,
    DEFAULT_LEG_JOINT_NAMES,
    DEFAULT_ARM_JOINT_NAMES,
    DEFAULT_HEAD_JOINT_NAMES,
    # DEFAULT_CAMERA_NAMES,
    DEFAULT_JOINT_NAMES,
    DEFAULT_LEJUCLAW_JOINT_NAMES,
    DEFAULT_DEXHAND_JOINT_NAMES,
    PostProcessorUtils,
)
import zipfile
import datetime
import einops
from math import ceil
from copy import deepcopy
import rosbag
import cv2
import os
import shutil
import concurrent.futures
import tempfile
from pathlib import Path

import requests
import time
import uuid
from copy import deepcopy
import logging

import os
import time
import subprocess
import multiprocessing
import queue
import threading

LEROBOT_HOME = HF_LEROBOT_HOME


def save_image_bytes_to_temp(
    imgs_per_cam: dict, imgs_per_cam_depth: dict, temp_base_dir: str, batch_id: int
):
    """
    ç›´æ¥ä¿å­˜å›¾åƒå­—èŠ‚æµåˆ°ä¸´æ—¶ç›®å½•ï¼ˆä¸è§£ç ã€ä¸ç¼©æ”¾ï¼Œä¼ å…¥ä»€ä¹ˆå°ºå¯¸å°±ä¿å­˜ä»€ä¹ˆå°ºå¯¸ï¼‰

    Args:
        imgs_per_cam: å½©è‰²å›¾åƒå­—èŠ‚æµ {camera: [bytes, ...]}
        imgs_per_cam_depth: æ·±åº¦å›¾åƒå­—èŠ‚æµ {camera: [bytes, ...]}
        temp_base_dir: ä¸´æ—¶ç›®å½•åŸºè·¯å¾„
        batch_id: æ‰¹æ¬¡ID
    """
    import os

    cam_stats = {}
    # ä¿å­˜å½©è‰²å›¾åƒ
    for camera, jpeg_list in imgs_per_cam.items():
        camera_dir = os.path.join(temp_base_dir, "color", camera)
        os.makedirs(camera_dir, exist_ok=True)

        for i, jpeg_bytes in enumerate(jpeg_list):
            frame_path = os.path.join(
                camera_dir, f"batch_{batch_id:04d}_frame_{i:06d}.jpg"
            )
            with open(frame_path, "wb") as f:
                f.write(jpeg_bytes)
            if i == 0:
                img_np_bgr = cv2.imdecode(
                    np.frombuffer(jpeg_bytes, np.uint8), cv2.IMREAD_COLOR
                )
                if img_np_bgr is None:
                    raise ValueError(
                        f"Failed to decode color image for camera {camera} at frame {i}"
                    )
                h0, w0 = img_np_bgr.shape[:2]
                img_np = cv2.cvtColor(img_np_bgr, cv2.COLOR_BGR2RGB)
                img_np = einops.rearrange(img_np, "h w c -> c h w")
                key = f"observation.images.{camera}"
                cam_stats[key] = get_feature_stats(
                    [img_np], axis=(0, 2, 3), keepdims=True
                )
                cam_stats[key] = {
                    k: v if k == "count" else np.squeeze(v / 255.0, axis=0)
                    for k, v in cam_stats[key].items()
                }
                # é¢å¤–è®°å½•å®é™…å›¾åƒé«˜å®½ï¼Œä¾› info.json ä½¿ç”¨
                cam_stats[key]["height"] = int(h0)
                cam_stats[key]["width"] = int(w0)

    # ä¿å­˜æ·±åº¦å›¾åƒï¼ˆPNGå­—èŠ‚æµï¼‰
    if imgs_per_cam_depth is not None:
        for camera, png_list in imgs_per_cam_depth.items():
            camera_dir = os.path.join(temp_base_dir, "depth", camera)
            os.makedirs(camera_dir, exist_ok=True)

            for i, png_bytes in enumerate(png_list):
                frame_path = os.path.join(
                    camera_dir, f"batch_{batch_id:04d}_frame_{i:06d}.png"
                )
                png_magic = bytes([137, 80, 78, 71, 13, 10, 26, 10])
                if isinstance(png_bytes, bytes):
                    idx_png = png_bytes.find(png_magic)
                    if idx_png != -1:
                        png_data = png_bytes[idx_png:]
                        with open(frame_path, "wb") as f:
                            f.write(png_data)

    print(f"[TEMP] æ‰¹æ¬¡{batch_id} å›¾åƒå­—èŠ‚æµå·²ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•")
    return cam_stats


def _encode_color_camera_worker(
    camera_dir: str, camera: str, out_path: str, train_hz: int, stats_output_dir: str
):
    """
    å­è¿›ç¨‹ï¼šç¼–ç å½©è‰²è§†é¢‘ï¼ˆPyAVï¼‰
    """
    import av
    from PIL import Image
    import glob
    import shutil
    import gc

    try:
        frame_files = sorted(glob.glob(os.path.join(camera_dir, "*.jpg")))
        print(f"[VIDEO][COLOR] {camera}: å¸§æ•° {len(frame_files)}")
        if len(frame_files) == 0:
            shutil.rmtree(camera_dir, ignore_errors=True)
            gc.collect()
            print(f"[VIDEO][COLOR] {camera}: æ— å¸§ï¼Œå·²æ¸…ç†")
            return

        video_options = {
            "g": "2",
            "crf": "30",
            "svtav1-params": "threads=6:lp=4",
        }
        first_img = Image.open(frame_files[0])
        width, height = first_img.size

        with av.open(str(out_path), "w") as output:
            stream = output.add_stream("libx264", train_hz, options=video_options)
            stream.pix_fmt = "yuv420p"
            stream.width = width
            stream.height = height

            for frame_file in frame_files:
                img = Image.open(frame_file).convert("RGB")
                frame = av.VideoFrame.from_image(img)
                packet = stream.encode(frame)
                if packet:
                    output.mux(packet)
            packet = stream.encode()
            if packet:
                output.mux(packet)
        print(f"[VIDEO][COLOR] âœ… {camera} å®Œæˆ: {out_path}")
    except Exception as e:
        print(f"[VIDEO][COLOR] âŒ {camera} å¤±è´¥: {e}")
    finally:
        shutil.rmtree(camera_dir, ignore_errors=True)
        gc.collect()
        print(f"[VIDEO][COLOR] ğŸ—‘ï¸  {camera} ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")


def _encode_depth_camera_worker(
    camera_dir: str, camera: str, out_path: str, train_hz: int, apply_denoise: bool
):
    """
    å­è¿›ç¨‹ï¼šç¼–ç æ·±åº¦è§†é¢‘ï¼ˆffmpeg + å¯é€‰å»å™ªï¼‰
    """
    import glob
    import shutil
    import tempfile
    import gc
    import numpy as np
    import cv2
    import subprocess

    try:
        frame_files = sorted(glob.glob(os.path.join(camera_dir, "*.png")))
        print(f"[VIDEO][DEPTH] {camera}: å¸§æ•° {len(frame_files)}")
        if len(frame_files) == 0:
            shutil.rmtree(camera_dir, ignore_errors=True)
            gc.collect()
            print(f"[VIDEO][DEPTH] {camera}: æ— å¸§ï¼Œå·²æ¸…ç†")
            return

        is_hand_camera = "wrist_cam" in camera
        if is_hand_camera and apply_denoise:
            print(f"[VIDEO][DEPTH] {camera}: åº”ç”¨å»å™ª")

        with tempfile.TemporaryDirectory() as processed_dir:
            for idx, frame_file in enumerate(frame_files):
                img = cv2.imread(frame_file, cv2.IMREAD_UNCHANGED)
                if img is None:
                    continue
                if img.ndim > 2:
                    img = img[:, :, 0]
                if img.dtype != np.uint16:
                    img = img.astype(np.uint16)

                if is_hand_camera and apply_denoise:
                    try:
                        from video_denoising import repair_depth_noise_focused

                        img = repair_depth_noise_focused(
                            img,
                            max_valid_depth=10000,
                            median_kernel=5,
                            detect_white_spots=True,
                            spot_size_range=(10, 1000),
                        )
                    except Exception:
                        pass

                processed_path = os.path.join(processed_dir, f"frame_{idx:06d}.png")
                cv2.imwrite(processed_path, img)
                if idx % 50 == 0:
                    gc.collect()

            cmd = [
                "ffmpeg",
                "-y",
                "-framerate",
                str(train_hz),
                "-i",
                os.path.join(processed_dir, "frame_%06d.png"),
                "-c:v",
                "ffv1",
                "-pix_fmt",
                "gray16le",
                out_path,
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            print(f"[VIDEO][DEPTH] âœ… {camera} å®Œæˆ: {out_path}")
    except Exception as e:
        print(f"[VIDEO][DEPTH] âŒ {camera} å¤±è´¥: {e}")
    finally:
        shutil.rmtree(camera_dir, ignore_errors=True)
        gc.collect()
        print(f"[VIDEO][DEPTH] ğŸ—‘ï¸  {camera} ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")


# ==================== æµå¼è§†é¢‘ç¼–ç å™¨ ====================


class StreamingColorVideoEncoder:
    """
    å•ç›¸æœºæµå¼è§†é¢‘ç¼–ç å™¨ï¼šé€šè¿‡æœ‰ç•Œé˜Ÿåˆ—æ¥æ”¶å¸§ï¼ŒWorker çº¿ç¨‹å®æ—¶è§£ç å¹¶ç¼–ç åˆ° PyAV å®¹å™¨ã€‚

    ç‰¹ç‚¹ï¼š
    - PyAV å®¹å™¨ä»åˆå§‹åŒ–æ—¶å°±ä¿æŒæ‰“å¼€
    - æœ‰ç•Œé˜Ÿåˆ—å®ç°èƒŒå‹ï¼ˆé˜Ÿåˆ—æ»¡æ—¶é˜»å¡å…¥é˜Ÿï¼‰
    - æ— éœ€ä¸´æ—¶æ–‡ä»¶ï¼Œç›´æ¥å†…å­˜ç¼–ç 

    ç”¨æ³•:
        encoder = StreamingColorVideoEncoder("camera_top", output_path, train_hz=30)
        for batch_frames in batches:
            for frame_bytes in batch_frames:
                encoder.put(frame_bytes)
        stats = encoder.finish()
    """

    SENTINEL = object()  # ç»“æŸä¿¡å·

    def __init__(
        self, camera: str, output_path: str, train_hz: int = 30, queue_limit: int = 100
    ):
        """
        Args:
            camera: ç›¸æœºåç§°
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            train_hz: å¸§ç‡
            queue_limit: é˜Ÿåˆ—ä¸Šé™ï¼ˆèƒŒå‹æ§åˆ¶ï¼‰
        """
        self.camera = camera
        self.output_path = output_path
        self.train_hz = train_hz
        self.queue_limit = queue_limit

        # ç»Ÿè®¡
        self._frame_count = 0  # å…¥é˜Ÿå¸§æ•°
        self._encoded_count = 0  # å·²ç¼–ç å¸§æ•°
        self._block_count = 0  # å…¥é˜Ÿé˜»å¡æ¬¡æ•°
        self._start_time = time.time()

        # çŠ¶æ€
        self._finished = False
        self._error = None
        self._width = None
        self._height = None

        # æœ‰ç•Œé˜Ÿåˆ—
        self._queue = queue.Queue(maxsize=queue_limit)

        # PyAV å®¹å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œåœ¨ç¬¬ä¸€å¸§æ—¶ç¡®å®šå°ºå¯¸ï¼‰
        self._container = None
        self._stream = None
        self._container_lock = threading.Lock()

        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        self._worker = threading.Thread(
            target=self._encode_worker, name=f"StreamEnc-{camera}", daemon=True
        )
        self._worker.start()

        print(f"[STREAMING][{camera}] ç¼–ç å™¨å·²å¯åŠ¨ (é˜Ÿåˆ—ä¸Šé™={queue_limit})")

    def put(self, frame_bytes: bytes):
        """
        å°†å¸§æ”¾å…¥é˜Ÿåˆ—ï¼ˆé˜»å¡å¼ï¼Œå®ç°èƒŒå‹ï¼‰

        Args:
            frame_bytes: JPEG å›¾åƒå­—èŠ‚æµ
        """
        if self._finished:
            raise RuntimeError(
                f"StreamingColorVideoEncoder({self.camera}) already finished"
            )

        if self._error:
            raise self._error

        # è®°å½•é˜»å¡
        if self._queue.full():
            self._block_count += 1

        self._queue.put(frame_bytes)  # é˜»å¡ç­‰å¾…ç©ºä½
        self._frame_count += 1

    def finish(self) -> dict:
        """
        å‘é€ç»“æŸä¿¡å·ï¼Œç­‰å¾…ç¼–ç å®Œæˆï¼Œå…³é—­å®¹å™¨ã€‚

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if self._finished:
            return self._get_stats()

        # å‘é€ç»“æŸä¿¡å·
        self._queue.put(self.SENTINEL)
        self._worker.join()
        self._finished = True

        # å…³é—­å®¹å™¨
        with self._container_lock:
            if self._container is not None:
                try:
                    # åˆ·æ–°ç¼–ç å™¨
                    if self._stream is not None:
                        packet = self._stream.encode(None)
                        if packet:
                            self._container.mux(packet)
                    self._container.close()
                except Exception as e:
                    print(f"[STREAMING][{self.camera}] å…³é—­å®¹å™¨æ—¶å‡ºé”™: {e}")

        if self._error:
            raise self._error

        elapsed = time.time() - self._start_time
        print(
            f"[STREAMING][{self.camera}] å®Œæˆ: {self._encoded_count} å¸§, é˜»å¡ {self._block_count} æ¬¡, è€—æ—¶ {elapsed:.1f}s"
        )

        return self._get_stats()

    def _get_stats(self) -> dict:
        return {
            "camera": self.camera,
            "frame_count": self._frame_count,
            "encoded_count": self._encoded_count,
            "block_count": self._block_count,
            "elapsed": time.time() - self._start_time,
        }

    def _encode_worker(self):
        """å·¥ä½œçº¿ç¨‹ï¼šä»é˜Ÿåˆ—å–å¸§ï¼Œè§£ç å¹¶ç¼–ç """
        import av
        from PIL import Image
        import io

        try:
            while True:
                item = self._queue.get()
                if item is self.SENTINEL:
                    break

                frame_bytes = item

                # è§£ç  JPEG
                try:
                    img = Image.open(io.BytesIO(frame_bytes)).convert("RGB")
                except Exception as e:
                    print(
                        f"[STREAMING][{self.camera}] å¸§ {self._encoded_count} è§£ç å¤±è´¥: {e}"
                    )
                    continue

                # å»¶è¿Ÿåˆå§‹åŒ–å®¹å™¨ï¼ˆç¬¬ä¸€å¸§æ—¶ç¡®å®šå°ºå¯¸ï¼‰
                if self._container is None:
                    self._init_container(img.width, img.height)

                # ç¼–ç 
                with self._container_lock:
                    if self._container is not None and self._stream is not None:
                        frame = av.VideoFrame.from_image(img)
                        frame.pts = self._encoded_count
                        packet = self._stream.encode(frame)
                        if packet:
                            self._container.mux(packet)
                        self._encoded_count += 1

        except Exception as e:
            self._error = e
            print(f"[STREAMING][{self.camera}] ç¼–ç é”™è¯¯: {e}")

    def _init_container(self, width: int, height: int):
        """åˆå§‹åŒ– PyAV å®¹å™¨"""
        import av

        with self._container_lock:
            if self._container is not None:
                return

            self._width = width
            self._height = height

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.output_path), exist_ok=True)

            # åˆ›å»ºå®¹å™¨
            self._container = av.open(self.output_path, mode="w")

            # åˆ›å»ºè§†é¢‘æµ
            video_options = {
                "g": "2",
                "crf": "30",
            }
            self._stream = self._container.add_stream(
                "libx264", self.train_hz, options=video_options
            )
            self._stream.pix_fmt = "yuv420p"
            self._stream.width = width
            self._stream.height = height

            print(
                f"[STREAMING][{self.camera}] å®¹å™¨å·²åˆå§‹åŒ–: {width}x{height} @ {self.train_hz}fps"
            )


class StreamingVideoEncoderManager:
    """
    å¤šç›¸æœºæµå¼ç¼–ç ç®¡ç†å™¨ï¼šç®¡ç†å¤šä¸ª StreamingColorVideoEncoder å®ä¾‹ã€‚

    èŒè´£ï¼š
    - ä¸ºæ¯ä¸ªç›¸æœºåˆ›å»ºç¼–ç å™¨
    - æä¾› feed_batch() æ–¹æ³•æ‰¹é‡å–‚å…¥å¸§
    - æä¾› finalize() æ–¹æ³•ç­‰å¾…æ‰€æœ‰ç¼–ç å®Œæˆ
    - é”™è¯¯ä¼ æ’­ï¼šä»»ä¸€ç¼–ç å™¨å¤±è´¥åˆ™ç»ˆæ­¢

    ç”¨æ³•:
        manager = StreamingVideoEncoderManager(cameras, output_dir, uuid, train_hz)
        for batch_id, imgs_per_cam in batches:
            manager.feed_batch(imgs_per_cam, batch_id)
        manager.finalize()
    """

    def __init__(
        self,
        cameras: list,
        video_output_dir: str,
        uuid_str: str,
        train_hz: int = 30,
        queue_limit: int = 100,
    ):
        """
        Args:
            cameras: ç›¸æœºåç§°åˆ—è¡¨
            video_output_dir: è§†é¢‘è¾“å‡ºç›®å½•
            uuid_str: æ•°æ®é›† UUID
            train_hz: å¸§ç‡
            queue_limit: æ¯ä¸ªç›¸æœºçš„é˜Ÿåˆ—ä¸Šé™
        """
        self.cameras = cameras
        self.video_output_dir = video_output_dir
        self.uuid_str = uuid_str
        self.train_hz = train_hz
        self.queue_limit = queue_limit

        self._encoders = {}
        self._start_time = time.time()
        self._total_frames = 0
        self._batches_fed = 0
        self._cam_stats = {}  # å­˜å‚¨ç¬¬ä¸€æ‰¹æ¬¡çš„ cam_statsï¼ˆç”¨äº meta æ–‡ä»¶ï¼‰

        # ä¸ºæ¯ä¸ªç›¸æœºåˆ›å»ºç¼–ç å™¨
        for camera in cameras:
            output_path = os.path.join(
                video_output_dir,
                "videos",
                "chunk-000",
                f"observation.images.{camera}",
                "episode_000000.mp4",
            )
            self._encoders[camera] = StreamingColorVideoEncoder(
                camera=camera,
                output_path=output_path,
                train_hz=train_hz,
                queue_limit=queue_limit,
            )

        print(
            f"[STREAMING] åˆå§‹åŒ–æµå¼ç¼–ç ç®¡ç†å™¨: {len(cameras)} ç›¸æœº, é˜Ÿåˆ—ä¸Šé™={queue_limit}"
        )

    def feed_batch(self, imgs_per_cam: dict, batch_id: int) -> dict:
        """
        å°†ä¸€ä¸ªæ‰¹æ¬¡çš„å¸§å–‚å…¥æ‰€æœ‰ç›¸æœºç¼–ç å™¨ã€‚

        Args:
            imgs_per_cam: æ¯ä¸ªç›¸æœºçš„å¸§åˆ—è¡¨ {camera: [frame_bytes, ...]}
            batch_id: æ‰¹æ¬¡ID

        Returns:
            cam_stats: å›¾åƒç»Ÿè®¡ä¿¡æ¯ï¼Œæ ¼å¼ä¸ save_image_bytes_to_temp() å…¼å®¹
        """
        import cv2
        import numpy as np
        import einops

        cam_stats = {}
        batch_total = 0
        batch_blocks = 0
        block_details = {}

        for camera, frame_list in imgs_per_cam.items():
            if camera not in self._encoders:
                continue

            encoder = self._encoders[camera]
            before_blocks = encoder._block_count

            # è®¡ç®—ç¬¬ä¸€å¸§çš„å›¾åƒç»Ÿè®¡ï¼ˆç”¨äº meta æ–‡ä»¶ï¼‰ï¼Œå¹¶è®°å½•å®é™…é«˜å®½
            if len(frame_list) > 0 and batch_id == 1:
                first_frame_bytes = frame_list[0]
                img_np_bgr = cv2.imdecode(
                    np.frombuffer(first_frame_bytes, np.uint8), cv2.IMREAD_COLOR
                )
                if img_np_bgr is not None:
                    h0, w0 = img_np_bgr.shape[:2]
                    img_np = cv2.cvtColor(img_np_bgr, cv2.COLOR_BGR2RGB)
                    img_np = einops.rearrange(img_np, "h w c -> c h w")
                    key = f"observation.images.{camera}"
                    cam_stats[key] = get_feature_stats(
                        [img_np], axis=(0, 2, 3), keepdims=True
                    )
                    cam_stats[key] = {
                        k: v if k == "count" else np.squeeze(v / 255.0, axis=0)
                        for k, v in cam_stats[key].items()
                    }
                    cam_stats[key]["height"] = int(h0)
                    cam_stats[key]["width"] = int(w0)

            # å–‚å…¥æ‰€æœ‰å¸§åˆ°ç¼–ç å™¨
            for frame_bytes in frame_list:
                encoder.put(frame_bytes)

            after_blocks = encoder._block_count
            blocks_this_batch = after_blocks - before_blocks
            batch_blocks += blocks_this_batch
            batch_total += len(frame_list)
            block_details[camera] = blocks_this_batch

        self._total_frames += batch_total
        self._batches_fed += 1

        # ä¿å­˜ç¬¬ä¸€æ‰¹æ¬¡çš„ cam_statsï¼ˆç”¨äº meta æ–‡ä»¶ç”Ÿæˆï¼‰
        if batch_id == 1 and cam_stats:
            self._cam_stats = cam_stats

        # æ—¥å¿—
        if batch_blocks > 0:
            block_info = ", ".join(
                [f"{c}: é˜»å¡{b}æ¬¡" for c, b in block_details.items() if b > 0]
            )
            print(
                f"[STREAMING] Batch {batch_id}: å·²å–‚å…¥ {batch_total // len(imgs_per_cam)} å¸§/ç›¸æœº ({block_info})"
            )
        else:
            print(
                f"[STREAMING] Batch {batch_id}: å·²å–‚å…¥ {batch_total // len(imgs_per_cam)} å¸§/ç›¸æœº"
            )

        return self._cam_stats

    def finalize(self) -> dict:
        """
        ç­‰å¾…æ‰€æœ‰ç¼–ç å™¨å®Œæˆï¼Œæ”¶é›†ç»Ÿè®¡ä¿¡æ¯ã€‚

        Returns:
            æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"[STREAMING] ç­‰å¾…ç¼–ç å®Œæˆ...")

        all_stats = {}
        total_blocks = 0
        total_encoded = 0

        for camera, encoder in self._encoders.items():
            try:
                stats = encoder.finish()
                all_stats[camera] = stats
                total_blocks += stats["block_count"]
                total_encoded += stats["encoded_count"]
            except Exception as e:
                print(f"[STREAMING][{camera}] å®Œæˆæ—¶å‡ºé”™: {e}")
                raise

        elapsed = time.time() - self._start_time
        print(
            f"[STREAMING] å…¨éƒ¨å®Œæˆ: {total_encoded} å¸§, æ€»é˜»å¡ {total_blocks} æ¬¡, æ€»è€—æ—¶ {elapsed:.1f}s"
        )

        return {
            "cameras": all_stats,
            "total_frames": self._total_frames,
            "total_encoded": total_encoded,
            "total_blocks": total_blocks,
            "batches_fed": self._batches_fed,
            "elapsed": elapsed,
        }


def _encode_batch_segment_color(
    batch_id: int,
    camera: str,
    temp_dir: str,
    segment_dir: str,
    train_hz: int,
    chunk_size: int = 800,
) -> str:
    """
    ç¼–ç å•ä¸ª batch çš„å½©è‰²è§†é¢‘ç‰‡æ®µ

    Args:
        batch_id: æ‰¹æ¬¡ID
        camera: ç›¸æœºåç§°
        temp_dir: ä¸´æ—¶å¸§ç›®å½• (åŒ…å« batch_XXXX_frame_XXXXXX.jpg)
        segment_dir: ç‰‡æ®µè¾“å‡ºç›®å½•
        train_hz: è§†é¢‘å¸§ç‡
        chunk_size: æ¯æ‰¹æ¬¡å¸§æ•°ï¼Œç”¨äºè®¡ç®—å…¨å±€å¸§èµ·å§‹ä½ç½®

    Returns:
        ç‰‡æ®µæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
    """
    import av
    from PIL import Image
    import glob
    import time as time_module

    start_time = time_module.time()

    # æŸ¥æ‰¾è¯¥ batch çš„å¸§
    pattern = os.path.join(temp_dir, f"batch_{batch_id:04d}_frame_*.jpg")
    frame_files = sorted(glob.glob(pattern))

    if len(frame_files) == 0:
        print(f"[PIPELINE][{camera}] Batch {batch_id}: æ— å¸§ï¼Œè·³è¿‡")
        return None

    # è¾“å‡ºç‰‡æ®µè·¯å¾„
    os.makedirs(segment_dir, exist_ok=True)
    segment_path = os.path.join(segment_dir, f"segment_{batch_id:04d}.mp4")

    # è®¡ç®—è¯¥ batch çš„å…¨å±€å¸§èµ·å§‹ä½ç½® (batch_id ä» 1 å¼€å§‹)
    global_frame_start = (batch_id - 1) * chunk_size

    try:
        video_options = {
            "g": "2",
            "crf": "30",
        }
        first_img = Image.open(frame_files[0])
        width, height = first_img.size

        with av.open(str(segment_path), "w") as output:
            stream = output.add_stream("libx264", train_hz, options=video_options)
            stream.pix_fmt = "yuv420p"
            stream.width = width
            stream.height = height

            for local_idx, frame_file in enumerate(frame_files):
                img = Image.open(frame_file).convert("RGB")
                frame = av.VideoFrame.from_image(img)
                # è®¾ç½®å…¨å±€ PTSï¼Œç¡®ä¿æ‹¼æ¥åæ—¶é—´æˆ³è¿ç»­
                frame.pts = global_frame_start + local_idx
                packet = stream.encode(frame)
                if packet:
                    output.mux(packet)
            packet = stream.encode()
            if packet:
                output.mux(packet)

        elapsed_ms = (time_module.time() - start_time) * 1000
        print(
            f"[PIPELINE][{camera}] Batch {batch_id}: {len(frame_files)} å¸§ (èµ·å§‹å¸§={global_frame_start}) â†’ {segment_path} ({elapsed_ms:.0f}ms)"
        )
        return segment_path

    except Exception as e:
        print(f"[PIPELINE][{camera}] Batch {batch_id} ç¼–ç å¤±è´¥: {e}")
        return None


def _concat_segments_ffmpeg(
    segment_paths: list, output_path: str, train_hz: int = 30
) -> bool:
    """
    ä½¿ç”¨ ffmpeg æ‹¼æ¥è§†é¢‘ç‰‡æ®µï¼ˆé‡æ–°ç¼–ç ä»¥ç¡®ä¿æ—¶é—´æˆ³æ­£ç¡®ï¼‰

    Args:
        segment_paths: ç‰‡æ®µæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
        output_path: æœ€ç»ˆè¾“å‡ºè·¯å¾„
        train_hz: è§†é¢‘å¸§ç‡

    Returns:
        æˆåŠŸè¿”å› True
    """
    import tempfile
    import time as time_module

    if not segment_paths:
        print(f"[PIPELINE] æ‹¼æ¥å¤±è´¥: æ— ç‰‡æ®µ")
        return False

    start_time = time_module.time()

    # åˆ›å»º filelist.txt
    with tempfile.NamedTemporaryFile(mode="w", suffix=".txt", delete=False) as f:
        for seg in segment_paths:
            f.write(f"file '{seg}'\n")
        filelist_path = f.name

    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # ä½¿ç”¨é‡æ–°ç¼–ç ç¡®ä¿æ—¶é—´æˆ³ä¸¥æ ¼è¿ç»­
        cmd = [
            "ffmpeg",
            "-y",
            "-f",
            "concat",
            "-safe",
            "0",
            "-i",
            filelist_path,
            "-c:v",
            "libx264",
            "-preset",
            "fast",
            "-crf",
            "30",
            "-g",
            "2",
            "-pix_fmt",
            "yuv420p",
            "-r",
            str(train_hz),
            "-vsync",
            "cfr",  # å¼ºåˆ¶æ’å®šå¸§ç‡ï¼Œç¡®ä¿æ—¶é—´æˆ³ç²¾ç¡®
            output_path,
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            print(f"[PIPELINE] ffmpeg æ‹¼æ¥å¤±è´¥: {result.stderr}")
            return False

        elapsed_ms = (time_module.time() - start_time) * 1000
        print(
            f"[PIPELINE] æ‹¼æ¥å®Œæˆ: {len(segment_paths)} ç‰‡æ®µ â†’ {output_path} ({elapsed_ms:.0f}ms)"
        )
        return True

    finally:
        os.unlink(filelist_path)


class BatchSegmentEncoder:
    """
    æ‰¹æ¬¡åˆ†æ®µè§†é¢‘ç¼–ç å™¨ - å®ç°æ‰¹å¤„ç†ä¸è§†é¢‘ç¼–ç çš„æµæ°´çº¿å¹¶è¡Œ

    å·¥ä½œæµç¨‹:
    1. ä¸»çº¿ç¨‹è°ƒç”¨ submit_batch(batch_id) æäº¤ç¼–ç ä»»åŠ¡
    2. å·¥ä½œçº¿ç¨‹æ± å¼‚æ­¥ç¼–ç å„æ‰¹æ¬¡çš„è§†é¢‘ç‰‡æ®µ
    3. ä¸»çº¿ç¨‹è°ƒç”¨ finalize() ç­‰å¾…ç¼–ç å®Œæˆå¹¶æ‹¼æ¥æœ€ç»ˆè§†é¢‘
    """

    def __init__(
        self,
        temp_base_dir: str,
        segment_base_dir: str,
        video_output_dir: str,
        cameras: list,
        train_hz: int,
        uuid_str: str,
        chunk_size: int = 800,
        max_workers: int = 3,
    ):
        """
        Args:
            temp_base_dir: ä¸´æ—¶å¸§ç›®å½• (åŒ…å« color/{camera}/)
            segment_base_dir: ç‰‡æ®µä¸´æ—¶ç›®å½•
            video_output_dir: æœ€ç»ˆè§†é¢‘è¾“å‡ºç›®å½•
            cameras: ç›¸æœºåˆ—è¡¨
            train_hz: è§†é¢‘å¸§ç‡
            uuid_str: æ•°æ®é›† UUID
            chunk_size: æ¯æ‰¹æ¬¡å¸§æ•°ï¼Œç”¨äºè®¡ç®—å…¨å±€å¸§ PTS
            max_workers: æœ€å¤§å¹¶è¡Œç¼–ç æ•°
        """
        import queue
        import threading

        self.temp_base_dir = temp_base_dir
        self.segment_base_dir = segment_base_dir
        self.video_output_dir = video_output_dir
        self.cameras = cameras
        self.train_hz = train_hz
        self.uuid_str = uuid_str
        self.chunk_size = chunk_size
        self.max_workers = max_workers

        # ä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœå­˜å‚¨
        self.task_queue = queue.Queue()
        self.segments = {cam: [] for cam in cameras}  # {camera: [segment_path, ...]}
        self.lock = threading.Lock()
        self.stop_flag = threading.Event()

        # ç»Ÿè®¡
        self.batches_submitted = 0
        self.batches_encoded = 0
        self.start_time = None

        # é”™è¯¯çŠ¶æ€
        self.error_flag = threading.Event()
        self.error_message = None

        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        self.workers = []
        for i in range(max_workers):
            t = threading.Thread(
                target=self._worker_loop, name=f"SegmentEncoder-{i}", daemon=True
            )
            t.start()
            self.workers.append(t)

        self.start_time = time.time()
        print(
            f"[PIPELINE] åˆå§‹åŒ–åˆ†æ®µç¼–ç å™¨: {len(cameras)} ç›¸æœº, {max_workers} å·¥ä½œçº¿ç¨‹"
        )

    def _worker_loop(self):
        """å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯"""
        while not self.stop_flag.is_set() and not self.error_flag.is_set():
            try:
                task = self.task_queue.get(timeout=0.5)
                if task is None:  # ç»“æŸä¿¡å·
                    break

                # å¦‚æœå·²ç»æœ‰é”™è¯¯ï¼Œè·³è¿‡å¤„ç†
                if self.error_flag.is_set():
                    self.task_queue.task_done()
                    continue

                batch_id = task

                # ä¸ºæ¯ä¸ªç›¸æœºç¼–ç è¯¥æ‰¹æ¬¡
                for camera in self.cameras:
                    if self.error_flag.is_set():
                        break

                    temp_dir = os.path.join(self.temp_base_dir, "color", camera)
                    segment_dir = os.path.join(self.segment_base_dir, camera)

                    segment_path = _encode_batch_segment_color(
                        batch_id,
                        camera,
                        temp_dir,
                        segment_dir,
                        self.train_hz,
                        self.chunk_size,
                    )

                    if segment_path:
                        with self.lock:
                            self.segments[camera].append((batch_id, segment_path))
                    else:
                        # ç¼–ç å¤±è´¥ï¼Œè®¾ç½®é”™è¯¯æ ‡å¿—å¹¶ç»ˆæ­¢
                        with self.lock:
                            self.error_flag.set()
                            self.error_message = (
                                f"Batch {batch_id} camera {camera} ç¼–ç å¤±è´¥"
                            )
                        print(f"[PIPELINE][ERROR] {self.error_message}ï¼Œç»ˆæ­¢æµæ°´çº¿")
                        break

                with self.lock:
                    self.batches_encoded += 1

                self.task_queue.task_done()

            except queue.Empty:
                continue
            except Exception as e:
                with self.lock:
                    self.error_flag.set()
                    self.error_message = f"å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}"
                print(f"[PIPELINE][ERROR] {self.error_message}")

    def submit_batch(self, batch_id: int):
        """æäº¤ä¸€ä¸ªæ‰¹æ¬¡çš„ç¼–ç ä»»åŠ¡"""
        self.task_queue.put(batch_id)
        self.batches_submitted += 1
        print(f"[PIPELINE] Batch {batch_id} å·²æäº¤ç¼–ç é˜Ÿåˆ—")

    def finalize(self, use_depth: bool = False) -> bool:
        """
        ç­‰å¾…æ‰€æœ‰ç¼–ç å®Œæˆï¼Œæ‹¼æ¥æœ€ç»ˆè§†é¢‘

        Returns:
            æˆåŠŸè¿”å› True

        Raises:
            RuntimeError: å¦‚æœç¼–ç è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯
        """
        import time as time_module

        print(f"[PIPELINE] ç­‰å¾… {self.batches_submitted} ä¸ªæ‰¹æ¬¡ç¼–ç å®Œæˆ...")

        # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º
        self.task_queue.join()

        # åœæ­¢å·¥ä½œçº¿ç¨‹
        self.stop_flag.set()
        for _ in self.workers:
            self.task_queue.put(None)
        for t in self.workers:
            t.join(timeout=5)

        encode_time = time.time() - self.start_time

        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯å‘ç”Ÿ
        if self.error_flag.is_set():
            error_msg = self.error_message or "æœªçŸ¥é”™è¯¯"
            print(f"[PIPELINE][FATAL] ç¼–ç å¤±è´¥: {error_msg}")
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if os.path.exists(self.segment_base_dir):
                shutil.rmtree(self.segment_base_dir)
            if os.path.exists(self.temp_base_dir):
                shutil.rmtree(self.temp_base_dir)
            raise RuntimeError(f"è§†é¢‘ç¼–ç æµæ°´çº¿å¤±è´¥: {error_msg}")

        print(
            f"[PIPELINE] æ‰€æœ‰æ‰¹æ¬¡ç¼–ç å®Œæˆ ({self.batches_encoded}/{self.batches_submitted}), è€—æ—¶ {encode_time:.1f}s"
        )

        # æ‹¼æ¥å„ç›¸æœºçš„è§†é¢‘
        concat_start = time.time()
        color_out_dir = os.path.join(self.video_output_dir, "videos", "chunk-000")

        success = True
        for camera in self.cameras:
            # æŒ‰ batch_id æ’åº
            with self.lock:
                sorted_segments = sorted(self.segments[camera], key=lambda x: x[0])
                segment_paths = [path for _, path in sorted_segments]

            if not segment_paths:
                print(f"[PIPELINE][{camera}] æ— ç‰‡æ®µå¯æ‹¼æ¥")
                continue

            output_path = os.path.join(
                color_out_dir, f"observation.images.{camera}", "episode_000000.mp4"
            )
            if not _concat_segments_ffmpeg(segment_paths, output_path, self.train_hz):
                success = False

        concat_time = time.time() - concat_start
        total_time = time.time() - self.start_time

        # æ¸…ç†ç‰‡æ®µä¸´æ—¶ç›®å½•
        if os.path.exists(self.segment_base_dir):
            shutil.rmtree(self.segment_base_dir)

        # æ¸…ç†åŸå§‹å¸§ä¸´æ—¶ç›®å½•
        if os.path.exists(self.temp_base_dir):
            shutil.rmtree(self.temp_base_dir)

        print(f"[PIPELINE] ========== æµæ°´çº¿å®Œæˆ ==========")
        print(
            f"[PIPELINE] ç¼–ç è€—æ—¶: {encode_time:.1f}s, æ‹¼æ¥è€—æ—¶: {concat_time:.1f}s, æ€»è®¡: {total_time:.1f}s"
        )

        return success


# ==================== åŸæœ‰è§†é¢‘ç¼–ç å‡½æ•° ====================


def encode_complete_videos_from_temp(
    temp_base_dir: str,
    video_output_dir: str,
    uuid: str,
    raw_config: Config,
    use_depth: bool = True,
):
    """
    ä»ä¸´æ—¶å¸§ç›®å½•åˆæˆå®Œæ•´è§†é¢‘ï¼ˆæ‰€æœ‰batchåˆå¹¶ä¸ºä¸€ä¸ªè§†é¢‘ï¼‰
    é€ä¸ªç›¸æœºå¤„ç†ï¼Œå¤„ç†å®Œç«‹å³æ¸…ç†ï¼Œæ§åˆ¶å†…å­˜å ç”¨

    Args:
        temp_base_dir: ä¸´æ—¶å¸§ç›®å½•
        video_output_dir: è§†é¢‘è¾“å‡ºç›®å½•
        uuid: æ•°æ®é›†UUID
        raw_config: é…ç½®å¯¹è±¡
    """
    import shutil
    import av
    from PIL import Image
    import glob

    print("[VIDEO] ========== å¼€å§‹åˆæˆå®Œæ•´è§†é¢‘ ==========")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    stats_output_dir = os.path.join(video_output_dir, "meta", "episodes_stats.jsonl")
    color_out_dir = os.path.join(video_output_dir, "videos", "chunk-000")

    os.makedirs(color_out_dir, exist_ok=True)

    # === å½©è‰²ï¼šæ¯ç›¸æœºä¸€ä¸ªå­è¿›ç¨‹ ===
    color_temp_dir = os.path.join(temp_base_dir, "color")
    color_procs = []
    if os.path.exists(color_temp_dir):
        for camera in os.listdir(color_temp_dir):
            camera_dir = os.path.join(color_temp_dir, camera)
            if not os.path.isdir(camera_dir):
                continue
            video_path = os.path.join(
                color_out_dir, f"observation.images.{camera}", "episode_000000.mp4"
            )
            os.makedirs(os.path.dirname(video_path), exist_ok=True)
            p = multiprocessing.Process(
                target=_encode_color_camera_worker,
                args=(
                    camera_dir,
                    camera,
                    video_path,
                    raw_config.train_hz,
                    stats_output_dir,
                ),
                daemon=False,
            )
            p.start()
            color_procs.append(p)

    # === æ·±åº¦ï¼šæ¯ç›¸æœºä¸€ä¸ªå­è¿›ç¨‹ï¼ˆå— use_depth æ§åˆ¶ï¼‰ ===
    depth_temp_dir = os.path.join(temp_base_dir, "depth")
    depth_procs = []
    if use_depth and os.path.exists(depth_temp_dir):
        depth_out_dir = os.path.join(video_output_dir, "depth", "chunk-000")
        os.makedirs(depth_out_dir, exist_ok=True)
        apply_denoise = getattr(raw_config, "denoise_enabled", True)
        apply_denoise = False  # ä¿æŒåŸé€»è¾‘å…³é—­
        for camera in os.listdir(depth_temp_dir):
            camera_dir = os.path.join(depth_temp_dir, camera)
            if not os.path.isdir(camera_dir):
                continue
            video_path = os.path.join(depth_out_dir, f"{camera}.mkv")
            p = multiprocessing.Process(
                target=_encode_depth_camera_worker,
                args=(
                    camera_dir,
                    camera,
                    video_path,
                    raw_config.train_hz,
                    apply_denoise,
                ),
                daemon=False,
            )
            p.start()
            depth_procs.append(p)
    elif not use_depth and os.path.exists(depth_temp_dir):
        shutil.rmtree(depth_temp_dir, ignore_errors=True)
        print("[VIDEO] è·³è¿‡æ·±åº¦è§†é¢‘å¤„ç†ï¼ˆuse_depth=falseï¼‰ï¼Œå·²æ¸…ç†æ·±åº¦ä¸´æ—¶ç›®å½•")

    # ç­‰å¾…æ‰€æœ‰å­è¿›ç¨‹å®Œæˆ
    for p in color_procs:
        p.join()
    for p in depth_procs:
        p.join()

    # æ¸…ç†æ•´ä¸ªä¸´æ—¶ç›®å½•
    if os.path.exists(temp_base_dir):
        shutil.rmtree(temp_base_dir)
        print("[VIDEO] ========== æ‰€æœ‰è§†é¢‘ç¼–ç å®Œæˆï¼Œä¸´æ—¶ç›®å½•å·²æ¸…ç† ==========")
        print(f"[VIDEO] è§†é¢‘ä¿å­˜ä½ç½®: {video_output_dir}/{uuid}")


def encode_complete_videos_from_temp1(
    temp_base_dir: str,
    video_output_dir: str,
    uuid: str,
    raw_config: Config,
):
    """
    ä»ä¸´æ—¶å¸§ç›®å½•åˆæˆå®Œæ•´è§†é¢‘ï¼ˆæ‰€æœ‰batchåˆå¹¶ä¸ºä¸€ä¸ªè§†é¢‘ï¼‰
    é€ä¸ªç›¸æœºå¤„ç†ï¼Œå¤„ç†å®Œç«‹å³æ¸…ç†ï¼Œæ§åˆ¶å†…å­˜å ç”¨

    Args:
        temp_base_dir: ä¸´æ—¶å¸§ç›®å½•
        video_output_dir: è§†é¢‘è¾“å‡ºç›®å½•
        uuid: æ•°æ®é›†UUID
        raw_config: é…ç½®å¯¹è±¡
    """
    import shutil
    import av
    from PIL import Image
    import glob
    import concurrent.futures
    import multiprocessing

    print("[VIDEO] ========== å¼€å§‹åˆæˆå®Œæ•´è§†é¢‘ ==========")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    color_out_dir = os.path.join(video_output_dir, uuid, "color")
    depth_out_dir = os.path.join(video_output_dir, uuid, "depth")
    os.makedirs(color_out_dir, exist_ok=True)
    os.makedirs(depth_out_dir, exist_ok=True)

    # å¹¶å‘çº¿ç¨‹æ•°
    max_workers = getattr(raw_config, "video_encoding_workers", None)
    if not isinstance(max_workers, int) or max_workers <= 0:
        max_workers = max(1, multiprocessing.cpu_count())

    # å½©è‰²ç›¸æœºç¼–ç çš„å·¥ä½œå‡½æ•°
    def _encode_color_camera(camera_dir: str, camera: str):
        print(f"[VIDEO] å¤„ç†å½©è‰²ç›¸æœº: {camera}")
        frame_files = sorted(glob.glob(os.path.join(camera_dir, "*.jpg")))
        print(f"[VIDEO]   å‘ç° {len(frame_files)} å¸§")
        if len(frame_files) == 0:
            # æ²¡å¸§ä¹Ÿæ¸…ç†ç›®å½•
            shutil.rmtree(camera_dir, ignore_errors=True)
            gc.collect()
            return f"{camera}: skipped(empty)"

        video_path = os.path.join(color_out_dir, f"{camera}.mp4")
        try:
            video_options = {
                "g": "2",
                "crf": "30",
                "svtav1-params": "threads=6:lp=4",
            }
            first_img = Image.open(frame_files[0])
            width, height = first_img.size
            with av.open(str(video_path), "w") as output:
                stream = output.add_stream(
                    "libx264", raw_config.train_hz, options=video_options
                )
                stream.pix_fmt = "yuv420p"
                stream.width = width
                stream.height = height

                for frame_file in frame_files:
                    img = Image.open(frame_file).convert("RGB")
                    frame = av.VideoFrame.from_image(img)
                    packet = stream.encode(frame)
                    if packet:
                        output.mux(packet)
                packet = stream.encode()
                if packet:
                    output.mux(packet)
            ret = f"{camera}: ok -> {video_path}"
        except Exception as e:
            ret = f"{camera}: fail -> {e}"
        finally:
            # æ— è®ºæˆåŠŸå¤±è´¥éƒ½æ¸…ç†ä¸´æ—¶å¸§ç›®å½•
            shutil.rmtree(camera_dir, ignore_errors=True)
            gc.collect()
            print(f"[VIDEO]   ğŸ—‘ï¸  {camera} ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        return ret

    # æ·±åº¦ç›¸æœºç¼–ç çš„å·¥ä½œå‡½æ•°ï¼ˆffmpegï¼‰
    def _encode_depth_camera(camera_dir: str, camera: str, apply_denoise: bool):
        print(f"[VIDEO] å¤„ç†æ·±åº¦ç›¸æœº: {camera}")
        frame_files = sorted(glob.glob(os.path.join(camera_dir, "*.png")))
        print(f"[VIDEO]   å‘ç° {len(frame_files)} æ·±åº¦å¸§")
        if len(frame_files) == 0:
            shutil.rmtree(camera_dir, ignore_errors=True)
            gc.collect()
            return f"{camera}: skipped(empty)"

        is_hand_camera = "wrist_cam" in camera
        if is_hand_camera and apply_denoise:
            print(f"[VIDEO]   å°†åº”ç”¨æ·±åº¦å»å™ª")

        import tempfile

        try:
            with tempfile.TemporaryDirectory() as processed_dir:
                for idx, frame_file in enumerate(frame_files):
                    img = cv2.imread(frame_file, cv2.IMREAD_UNCHANGED)
                    if img is None:
                        continue
                    if img.ndim > 2:
                        img = img[:, :, 0]
                    if img.dtype != np.uint16:
                        img = img.astype(np.uint16)

                    if is_hand_camera and apply_denoise:
                        try:
                            from video_denoising import repair_depth_noise_focused

                            img = repair_depth_noise_focused(
                                img,
                                max_valid_depth=10000,
                                median_kernel=5,
                                detect_white_spots=True,
                                spot_size_range=(10, 1000),
                            )
                        except Exception:
                            pass

                    processed_path = os.path.join(processed_dir, f"frame_{idx:06d}.png")
                    cv2.imwrite(processed_path, img)
                    if idx % 50 == 0:
                        gc.collect()

                video_path = os.path.join(depth_out_dir, f"{camera}.mkv")
                cmd = [
                    "ffmpeg",
                    "-y",
                    "-framerate",
                    str(raw_config.train_hz),
                    "-i",
                    os.path.join(processed_dir, "frame_%06d.png"),
                    "-c:v",
                    "ffv1",
                    "-pix_fmt",
                    "gray16le",
                    video_path,
                ]
                subprocess.run(cmd, check=True, capture_output=True)
                ret = f"{camera}: ok -> {video_path}"
        except Exception as e:
            ret = f"{camera}: fail -> {e}"
        finally:
            shutil.rmtree(camera_dir, ignore_errors=True)
            gc.collect()
            print(f"[VIDEO]   ğŸ—‘ï¸  {camera} ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        return ret

    # === å¹¶å‘å¤„ç†å½©è‰²è§†é¢‘ ===
    color_temp_dir = os.path.join(temp_base_dir, "color")
    if os.path.exists(color_temp_dir):
        futures = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            for camera in os.listdir(color_temp_dir):
                camera_dir = os.path.join(color_temp_dir, camera)
                if not os.path.isdir(camera_dir):
                    continue
                futures.append(
                    executor.submit(_encode_color_camera, camera_dir, camera)
                )
            for f in concurrent.futures.as_completed(futures):
                print(f"[VIDEO]   ç»“æœ: {f.result()}")

    # === å¹¶å‘å¤„ç†æ·±åº¦è§†é¢‘ ===
    depth_temp_dir = os.path.join(temp_base_dir, "depth")
    if os.path.exists(depth_temp_dir):
        apply_denoise = getattr(raw_config, "denoise_enabled", True)
        # ç°æœ‰é€»è¾‘å¼ºåˆ¶å…³é—­
        apply_denoise = False
        futures = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            for camera in os.listdir(depth_temp_dir):
                camera_dir = os.path.join(depth_temp_dir, camera)
                if not os.path.isdir(camera_dir):
                    continue
                futures.append(
                    executor.submit(
                        _encode_depth_camera, camera_dir, camera, apply_denoise
                    )
                )
            for f in concurrent.futures.as_completed(futures):
                print(f"[VIDEO]   ç»“æœ: {f.result()}")

    # æ¸…ç†æ•´ä¸ªä¸´æ—¶ç›®å½•
    if os.path.exists(temp_base_dir):
        shutil.rmtree(temp_base_dir)
        print("[VIDEO] ========== æ‰€æœ‰è§†é¢‘ç¼–ç å®Œæˆï¼Œä¸´æ—¶ç›®å½•å·²æ¸…ç† ==========")
        print(f"[VIDEO] è§†é¢‘ä¿å­˜ä½ç½®: {video_output_dir}/{uuid}")


def get_nested_value(data, path, i=None, default=None):
    """
    ä»åµŒå¥—å­—å…¸ä¸­é€šè¿‡è·¯å¾„å­—ç¬¦ä¸²æå–æ•°æ®ï¼Œå¹¶æ”¯æŒæŒ‰å¸§ç´¢å¼•å’Œé»˜è®¤å€¼ã€‚
    path: ä¾‹å¦‚ "state.head.position"
    i: å¸§ç´¢å¼•ï¼Œå¦‚æœä¸º None åˆ™è¿”å›æ•´ä¸ªæ•°ç»„
    default: é»˜è®¤å€¼ï¼ˆå¦‚ [0.0]*2ï¼‰
    """
    keys = path.split(".")
    v = data
    try:
        for k in keys:
            v = v[k]
        if i is not None:
            if v is not None and len(v) > i:
                v = v[i]
            else:
                v = default
        if v is None:
            v = default
        if isinstance(v, torch.Tensor):
            return v.float()
        else:
            return torch.tensor(v, dtype=torch.float32)
    except Exception:
        return torch.tensor(default, dtype=torch.float32)


# ç”¨æ³•ç¤ºä¾‹ï¼š
# get_nested_value(all_low_dim_data, "state.head.position", i, [0.0]*2)


def is_valid_hand_data(arr, expected_shape=None):
    arr = np.array(arr) if arr is not None else None
    if arr is None or arr.size == 0:
        return False
    if expected_shape is not None and arr.shape[1:] != expected_shape:
        return False
    return True


def calculate_action_frames(
    rosbag_actual_start_time,  # å®é™…æ•°æ®å¼€å§‹æ—¶é—´
    rosbag_actual_end_time,  # å®é™…æ•°æ®ç»“æŸæ—¶é—´
    rosbag_original_start_time,  # åŸå§‹bagå¼€å§‹æ—¶é—´
    rosbag_original_end_time,  # åŸå§‹bagç»“æŸæ—¶é—´
    action_original_start_time,  # åŠ¨ä½œåŸå§‹å¼€å§‹æ—¶é—´
    action_duration,  # åŠ¨ä½œæŒç»­æ—¶é—´
    frame_rate,  # å¸§ç‡
    total_frames,  # æ€»å¸§æ•°
):
    """
    è®¡ç®—åŠ¨ä½œçš„å¼€å§‹å¸§å’Œç»“æŸå¸§

    ç­–ç•¥ï¼š
    1. è®¡ç®—åŠ¨ä½œåœ¨åŸå§‹æ—¶é—´è½´ä¸Šçš„ç»å¯¹æ—¶é—´èŒƒå›´
    2. å°†è¿™ä¸ªæ—¶é—´èŒƒå›´æ˜ å°„åˆ°å®é™…æ•°æ®çš„æ—¶é—´èŒƒå›´
    3. æ ¹æ®å®é™…æ•°æ®çš„æ—¶é—´èŒƒå›´è®¡ç®—å¯¹åº”çš„å¸§æ•°
    """

    # 1. è®¡ç®—åŠ¨ä½œçš„ç»å¯¹æ—¶é—´èŒƒå›´
    action_start_time = action_original_start_time
    action_end_time = action_original_start_time + action_duration

    # 2. æ£€æŸ¥åŠ¨ä½œæ—¶é—´æ˜¯å¦åœ¨å®é™…æ•°æ®èŒƒå›´å†…
    if (
        action_end_time < rosbag_actual_start_time
        or action_start_time > rosbag_actual_end_time
    ):
        # åŠ¨ä½œå®Œå…¨åœ¨å®é™…æ•°æ®èŒƒå›´ä¹‹å¤–
        return None, None

    # 3. å°†åŠ¨ä½œæ—¶é—´èŒƒå›´é™åˆ¶åœ¨å®é™…æ•°æ®èŒƒå›´å†…
    clipped_action_start = max(action_start_time, rosbag_actual_start_time)
    clipped_action_end = min(action_end_time, rosbag_actual_end_time)

    # 4. è®¡ç®—ç›¸å¯¹äºå®é™…æ•°æ®å¼€å§‹æ—¶é—´çš„åç§»
    start_offset = clipped_action_start - rosbag_actual_start_time
    end_offset = clipped_action_end - rosbag_actual_start_time

    # 5. æ ¹æ®å®é™…æ•°æ®çš„æ—¶é—´èŒƒå›´è®¡ç®—å¸§æ•°
    actual_data_duration = rosbag_actual_end_time - rosbag_actual_start_time

    # æ–¹æ³•1ï¼šæŒ‰æ—¶é—´æ¯”ä¾‹è®¡ç®—
    start_frame = int((start_offset / actual_data_duration) * total_frames)
    end_frame = int((end_offset / actual_data_duration) * total_frames)

    # æ–¹æ³•2ï¼šæŒ‰å¸§ç‡è®¡ç®—ï¼ˆæ›´ç²¾ç¡®ï¼‰
    # start_frame = int(start_offset * frame_rate)
    # end_frame = int(end_offset * frame_rate)

    # 6. ç¡®ä¿å¸§æ•°åœ¨æœ‰æ•ˆèŒƒå›´å†…
    start_frame = max(0, min(start_frame, total_frames - 1))
    end_frame = max(start_frame, min(end_frame, total_frames - 1))

    return start_frame, end_frame


def merge_metadata_and_moment(
    metadata_path,
    moment_path,
    output_path,
    uuid,
    raw_config,
    bag_time_info=None,
    main_time_line_timestamps=None,
):
    """
    åˆå¹¶ metadata å’Œ moment æ•°æ®ï¼Œå¹¶æ·»åŠ  bag æ—¶é—´ä¿¡æ¯å’Œè®¡ç®—å¸§æ•°
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. æ—§æ ¼å¼ï¼šmetadata.json + moments.json ä¸¤ä¸ªæ–‡ä»¶
    2. æ–°æ ¼å¼ï¼šåªæœ‰ä¸€ä¸ª metadata.jsonï¼ŒåŒ…å« marks æ•°ç»„
    
    Args:
        metadata_path: metadata.json æ–‡ä»¶è·¯å¾„
        moment_path: moment.json æ–‡ä»¶è·¯å¾„ï¼ˆæ–°æ ¼å¼ä¸‹å¯ä¸º Noneï¼‰
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        uuid: å”¯ä¸€æ ‡è¯†ç¬¦
        raw_config: åŸå§‹é…ç½®å¯¹è±¡
        bag_time_info: bagæ—¶é—´ä¿¡æ¯å­—å…¸ï¼ˆå¯é€‰ï¼‰
        main_time_line_timestamps: ç»è¿‡å¸§ç‡å¯¹é½åçš„æ—¶é—´æˆ³æ•°ç»„ï¼ˆçº³ç§’ï¼‰
    """
    frequency = raw_config.train_hz if hasattr(raw_config, "train_hz") else 30

    # è¯»å– metadata.json
    with open(metadata_path, "r", encoding="utf-8") as f:
        raw_metadata = json.load(f)

    # æ£€æµ‹æ–°æ ¼å¼ï¼šå¦‚æœ metadata.json ä¸­æœ‰ marks å­—æ®µï¼Œä½¿ç”¨æ–°æ ¼å¼
    is_new_format = "marks" in raw_metadata and isinstance(raw_metadata.get("marks"), list)
    
    if is_new_format:
        print("[FORMAT] æ£€æµ‹åˆ°æ–°æ ¼å¼ metadata.jsonï¼ˆåŒ…å« marks æ•°ç»„ï¼‰")
        marks = raw_metadata.get("marks", [])
        moment = None  # æ–°æ ¼å¼ä¸éœ€è¦ moment.json
    else:
        print("[FORMAT] ä½¿ç”¨æ—§æ ¼å¼ï¼ˆmetadata.json + moments.jsonï¼‰")
        # è¯»å– moment.json
        if moment_path and os.path.exists(moment_path):
            with open(moment_path, "r", encoding="utf-8") as f:
                moment = json.load(f)
        else:
            print(f"[WARN] moment.json ä¸å­˜åœ¨: {moment_path}")
            moment = {"moments": []}

    # è½¬æ¢æ–°æ ¼å¼ metadata ä¸ºæ—§æ ¼å¼
    converted_metadata = {}
    
    if is_new_format:
        # æ–°æ ¼å¼å­—æ®µæ˜ å°„
        converted_metadata["scene_name"] = raw_metadata.get("primaryScene", "")
        converted_metadata["sub_scene_name"] = raw_metadata.get("tertiaryScene", "")
        converted_metadata["init_scene_text"] = raw_metadata.get("initSceneText", "")
        converted_metadata["english_init_scene_text"] = raw_metadata.get("englishInitSceneText", "")
        
        # task_name ä¼˜å…ˆ taskGroupNameï¼Œå…¶æ¬¡ taskName
        task_name = raw_metadata.get("taskGroupName")
        if not task_name:
            task_name = raw_metadata.get("taskName", "")
        converted_metadata["task_name"] = task_name
        
        # english_task_name ä¼˜å…ˆ taskGroupCodeï¼Œå…¶æ¬¡ taskCode
        english_task_name = raw_metadata.get("taskGroupCode")
        if not english_task_name:
            english_task_name = raw_metadata.get("taskCode", "")
        converted_metadata["english_task_name"] = english_task_name
        if isinstance(english_task_name, str) and "_" in english_task_name:
            english_task_name = english_task_name.replace("_", " ")
        converted_metadata["english_task_name"] = english_task_name
        
        converted_metadata["sn_code"] = raw_metadata.get("deviceSn", "")
    else:
        # æ—§æ ¼å¼å­—æ®µæ˜ å°„
        converted_metadata["scene_name"] = raw_metadata.get("scene_code", "")
        converted_metadata["sub_scene_name"] = raw_metadata.get("sub_scene_code", "")
        converted_metadata["init_scene_text"] = raw_metadata.get("sub_scene_zh_dec", "")
        converted_metadata["english_init_scene_text"] = raw_metadata.get("sub_scene_en_dec", "")
        
        task_name = raw_metadata.get("task_group_name")
        if not task_name:
            task_name = raw_metadata.get("task_name", "")
        converted_metadata["task_name"] = task_name
        
        english_task_name = raw_metadata.get("task_group_code")
        if not english_task_name:
            english_task_name = raw_metadata.get("task_code", "")
        converted_metadata["english_task_name"] = english_task_name
        if isinstance(english_task_name, str) and "_" in english_task_name:
            english_task_name = english_task_name.replace("_", " ")
        converted_metadata["english_task_name"] = english_task_name
        
        converted_metadata["sn_code"] = raw_metadata.get("device_sn", "")

    # é»˜è®¤å€¼å­—æ®µ
    converted_metadata["data_type"] = "å¸¸è§„"
    converted_metadata["episode_status"] = "approved"
    converted_metadata["data_gen_mode"] = "real_machine"
    converted_metadata["sn_name"] = "ä¹èšæœºå™¨äºº"

    print(f"Metadata å­—æ®µè½¬æ¢ç»“æœ:")
    for key, value in converted_metadata.items():
        print(f"  {key}: '{value}'")

    # ä½¿ç”¨è½¬æ¢åçš„ metadata
    metadata = converted_metadata

    # è·å–æ—¶é—´ä¿¡æ¯
    rosbag_actual_start_time = None
    rosbag_actual_end_time = None
    rosbag_original_start_time = None
    rosbag_original_end_time = None
    total_frames = 0

    # å®é™…æ•°æ®æ—¶é—´èŒƒå›´
    if main_time_line_timestamps is not None and len(main_time_line_timestamps) > 0:
        # è°ƒè¯•ï¼šæ‰“å°åŸå§‹æ—¶é—´æˆ³
        print(f"åŸå§‹æ—¶é—´æˆ³å‰3ä¸ª: {main_time_line_timestamps[:3]}")
        print(f"åŸå§‹æ—¶é—´æˆ³å3ä¸ª: {main_time_line_timestamps[-3:]}")

        # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦å·²ç»æ˜¯ç§’æ ¼å¼è¿˜æ˜¯çº³ç§’æ ¼å¼
        if main_time_line_timestamps[0] > 1e12:  # å¦‚æœå¤§äº1e12ï¼Œè®¤ä¸ºæ˜¯çº³ç§’æ ¼å¼
            timestamps_seconds = main_time_line_timestamps / 1e9
            print("æ—¶é—´æˆ³æ ¼å¼ï¼šçº³ç§’ -> ç§’")
        else:
            timestamps_seconds = main_time_line_timestamps
            print("æ—¶é—´æˆ³æ ¼å¼ï¼šå·²ç»æ˜¯ç§’")

        rosbag_actual_start_time = timestamps_seconds[0]
        rosbag_actual_end_time = timestamps_seconds[-1]
        total_frames = len(main_time_line_timestamps)

        # è°ƒè¯•ï¼šæ‰“å°è½¬æ¢åçš„æ—¶é—´æˆ³
        print(f"è½¬æ¢åæ—¶é—´æˆ³å‰3ä¸ª: {timestamps_seconds[:3]}")
        print(f"è½¬æ¢åæ—¶é—´æˆ³å3ä¸ª: {timestamps_seconds[-3:]}")

        # éªŒè¯æ—¶é—´æˆ³è½¬æ¢
        start_datetime = datetime.datetime.fromtimestamp(
            rosbag_actual_start_time, tz=datetime.timezone(datetime.timedelta(hours=8))
        )
        end_datetime = datetime.datetime.fromtimestamp(
            rosbag_actual_end_time, tz=datetime.timezone(datetime.timedelta(hours=8))
        )

        print(f"å®é™…å¼€å§‹æ—¶é—´éªŒè¯: {start_datetime.isoformat()}")
        print(f"å®é™…ç»“æŸæ—¶é—´éªŒè¯: {end_datetime.isoformat()}")

    # åŸå§‹bagæ—¶é—´èŒƒå›´
    if bag_time_info:
        rosbag_original_start_time = bag_time_info.get("unix_timestamp")
        rosbag_original_end_time = bag_time_info.get("end_time")

    # æ„é€  action_config
    print(f"æ—¶é—´ä¿¡æ¯:")
    if rosbag_original_start_time and rosbag_original_end_time:
        print(
            f"  åŸå§‹bagæ—¶é—´: {rosbag_original_start_time:.6f}s - {rosbag_original_end_time:.6f}s"
        )
    if rosbag_actual_start_time and rosbag_actual_end_time:
        print(
            f"  å®é™…æ•°æ®æ—¶é—´: {rosbag_actual_start_time:.6f}s - {rosbag_actual_end_time:.6f}s"
        )
    print(f"  æ€»å¸§æ•°: {total_frames}")

    action_config = []

    # æ ¹æ®æ ¼å¼é€‰æ‹©æ•°æ®æº
    if is_new_format:
        # æ–°æ ¼å¼ï¼šä» marks æ•°ç»„è¯»å–
        data_source = marks
    else:
        # æ—§æ ¼å¼ï¼šä» moments æ•°ç»„è¯»å–
        data_source = moment.get("moments", [])

    for m in data_source:
        if is_new_format:
            # æ–°æ ¼å¼ï¼šç›´æ¥ä» mark å¯¹è±¡è¯»å–
            mark_start = m.get("markStart", "")
            mark_end = m.get("markEnd", "")
            duration = m.get("duration", 0.0)  # å·²ç»æ˜¯æ•°å­—ï¼Œå•ä½ç§’
            
            # è½¬æ¢æ—¶é—´æ ¼å¼ï¼šä» "2026-01-06 09:41:20.781" è½¬ä¸º ISO æ ¼å¼
            try:
                # è§£æ markStart æ—¶é—´
                if mark_start:
                    # å°è¯•è§£æ "2026-01-06 09:41:20.781" æ ¼å¼
                    if " " in mark_start:
                        dt_str, time_str = mark_start.split(" ", 1)
                        # è½¬æ¢ä¸º ISO æ ¼å¼ï¼š2026-01-06T09:41:20.781+08:00
                        formatted_trigger_time = f"{dt_str}T{time_str}+08:00"
                    else:
                        formatted_trigger_time = mark_start
                else:
                    formatted_trigger_time = ""
            except Exception as e:
                print(f"[WARN] è§£æ markStart æ—¶é—´å¤±è´¥: {mark_start}, é”™è¯¯: {e}")
                formatted_trigger_time = ""
            
            skill_atomic = m.get("skillAtomic", "")
            skill_detail = m.get("skillDetail", "")
            en_skill_detail = m.get("enSkillDetail", "")
            mark_type = m.get("markType", "step")
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºé”™è¯¯åŠ¨ä½œï¼ˆretry ç±»å‹ï¼‰
            is_mistake = (mark_type == "retry")
            
            print(f"å¤„ç†åŠ¨ä½œæ•°æ®ï¼ˆæ–°æ ¼å¼ï¼‰:")
            print(f"  skill_atomic: {skill_atomic}")
            print(f"  skill_detail: {skill_detail}")
            print(f"  en_skill_detail: {en_skill_detail}")
            print(f"  markStart: {mark_start}")
            print(f"  markEnd: {mark_end}")
            print(f"  duration: {duration}s")
            print(f"  markType: {mark_type} (is_mistake={is_mistake})")
        else:
            # æ—§æ ¼å¼ï¼šä» customFieldValues ä¸­æå–æ•°æ®
            custom_fields = m.get("customFieldValues", {})
            trigger_time = m.get("triggerTime", "")
            duration_str = m.get("duration", "0s")
            
            # æ ¼å¼åŒ–æ—¶é—´æˆ³ï¼šå°† "Z" æ›¿æ¢ä¸º "+00:00"
            formatted_trigger_time = (
                trigger_time.replace("Z", "+00:00") if trigger_time else ""
            )
            
            skill_atomic = custom_fields.get("skill_atomic_en", "")
            skill_detail = custom_fields.get("skill_detail", "")
            en_skill_detail = custom_fields.get("en_skill_detail", "")
            is_mistake = False  # æ—§æ ¼å¼é»˜è®¤ä¸æ˜¯é”™è¯¯
            
            print(f"å¤„ç†åŠ¨ä½œæ•°æ®ï¼ˆæ—§æ ¼å¼ï¼‰:")
            print(f"  skill_atomic_en: {skill_atomic}")
            print(f"  skill_detail: {skill_detail}")
            print(f"  en_skill_detail: {en_skill_detail}")
            print(f"  åŸå§‹æ—¶é—´æˆ³: {trigger_time}")
            print(f"  æ ¼å¼åŒ–æ—¶é—´æˆ³: {formatted_trigger_time}")

        start_frame = None
        end_frame = None

        if (
            rosbag_actual_start_time is not None
            and rosbag_actual_end_time is not None
            and formatted_trigger_time
        ):
            try:
                if is_new_format:
                    # æ–°æ ¼å¼ï¼šä½¿ç”¨ markStart ä½œä¸ºè§¦å‘æ—¶é—´
                    # è§£æ markStart æ—¶é—´ï¼ˆæ ¼å¼ï¼š2026-01-06 09:41:20.781ï¼‰
                    if mark_start and " " in mark_start:
                        dt_str, time_str = mark_start.split(" ", 1)
                        # è½¬æ¢ä¸º datetime å¯¹è±¡ï¼ˆå‡è®¾æ˜¯æœ¬åœ°æ—¶é—´ï¼Œ+08:00ï¼‰
                        trigger_datetime = datetime.datetime.fromisoformat(
                            f"{dt_str}T{time_str}+08:00"
                        )
                    else:
                        trigger_datetime = datetime.datetime.fromisoformat(
                            formatted_trigger_time
                        )
                    action_original_start_time = trigger_datetime.timestamp()
                    action_duration = float(duration)  # å·²ç»æ˜¯æ•°å­—
                else:
                    # æ—§æ ¼å¼ï¼šä½¿ç”¨ triggerTime
                    trigger_datetime = datetime.datetime.fromisoformat(
                        formatted_trigger_time
                    )
                    action_original_start_time = trigger_datetime.timestamp()
                    # è§£ææŒç»­æ—¶é—´
                    action_duration = 0
                    if duration_str.endswith("s"):
                        action_duration = float(duration_str[:-1])

                # è®¡ç®—å¸§æ•°
                start_frame, end_frame = calculate_action_frames(
                    rosbag_actual_start_time=rosbag_actual_start_time,
                    rosbag_actual_end_time=rosbag_actual_end_time,
                    rosbag_original_start_time=rosbag_original_start_time,
                    rosbag_original_end_time=rosbag_original_end_time,
                    action_original_start_time=action_original_start_time,
                    action_duration=action_duration,
                    frame_rate=frequency,
                    total_frames=total_frames,
                )

                print(f"åŠ¨ä½œ: {skill_detail}")
                print(f"  åŠ¨ä½œæ—¶é—´: {trigger_datetime.isoformat()}")
                print(f"  åŸå§‹å¼€å§‹æ—¶é—´: {action_original_start_time:.6f}s")
                print(
                    f"  åŸå§‹ç»“æŸæ—¶é—´: {action_original_start_time + action_duration:.6f}s"
                )
                print(f"  æŒç»­æ—¶é—´: {action_duration:.3f}s")
                print(f"  è®¡ç®—å¾—åˆ°å¸§æ•°: {start_frame} - {end_frame}")

                # æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                if start_frame is None or end_frame is None:
                    print(f"  è°ƒè¯•ä¿¡æ¯:")
                    print(
                        f"    å®é™…æ•°æ®èŒƒå›´: {rosbag_actual_start_time:.6f}s - {rosbag_actual_end_time:.6f}s"
                    )
                    print(
                        f"    åŠ¨ä½œæ—¶é—´èŒƒå›´: {action_original_start_time:.6f}s - {action_original_start_time + action_duration:.6f}s"
                    )
                    print(
                        f"    åŠ¨ä½œæ˜¯å¦åœ¨æ•°æ®èŒƒå›´å†…: {action_original_start_time >= rosbag_actual_start_time and action_original_start_time + action_duration <= rosbag_actual_end_time}"
                    )
                    print(
                        f"    åŠ¨ä½œå¼€å§‹æ˜¯å¦åœ¨æ•°æ®èŒƒå›´å: {action_original_start_time > rosbag_actual_end_time}"
                    )
                    print(
                        f"    åŠ¨ä½œç»“æŸæ˜¯å¦åœ¨æ•°æ®èŒƒå›´å‰: {action_original_start_time + action_duration < rosbag_actual_start_time}"
                    )

                # éªŒè¯è®¡ç®—ç»“æœ
                if start_frame is not None and end_frame is not None:
                    actual_start_time = rosbag_actual_start_time + (
                        start_frame / total_frames
                    ) * (rosbag_actual_end_time - rosbag_actual_start_time)
                    actual_end_time = rosbag_actual_start_time + (
                        end_frame / total_frames
                    ) * (rosbag_actual_end_time - rosbag_actual_start_time)
                    print(f"  éªŒè¯-å®é™…å¼€å§‹æ—¶é—´: {actual_start_time:.6f}s")
                    print(f"  éªŒè¯-å®é™…ç»“æŸæ—¶é—´: {actual_end_time:.6f}s")

                print("-" * 50)

            except Exception as e:
                print(f"è®¡ç®—å¸§æ•°æ—¶å‡ºé”™: {e}")
                import traceback

                traceback.print_exc()

        # æ„é€ æ–°çš„ action å¯¹è±¡
        action = {
            "start_frame": start_frame,
            "end_frame": end_frame,
            "timestamp_utc": formatted_trigger_time,
            "is_mistake": is_mistake,
            "skill": skill_atomic,
            "action_text": skill_detail,
            "english_action_text": en_skill_detail,
        }
        action_config.append(action)

    # æŒ‰ç…§ timestamp_utc æ’åº
    action_config = sorted(
        action_config, key=lambda x: x["timestamp_utc"] if x["timestamp_utc"] else ""
    )

    # æ„é€ æ–°jsonï¼Œepisode_idæ”¾åœ¨æœ€å‰
    new_json = OrderedDict()
    new_json["episode_id"] = uuid

    # ä½¿ç”¨è½¬æ¢åçš„ metadata
    for k, v in metadata.items():
        new_json[k] = v

    if "label_info" not in new_json:
        new_json["label_info"] = {}
    new_json["label_info"]["action_config"] = action_config
    if "key_frame" not in new_json["label_info"]:
        new_json["label_info"]["key_frame"] = []

    # ä¿å­˜
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(new_json, f, ensure_ascii=False, indent=4)
    print(f"å·²ä¿å­˜åˆ° {output_path}")


def get_time_range_from_moments(moments_json_path, metadata_json_path=None):
    """
    ä» moments.json æˆ– metadata.jsonï¼ˆæ–°æ ¼å¼ï¼‰æ–‡ä»¶ä¸­è¯»å–æ—¶é—´èŒƒå›´
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. æ—§æ ¼å¼ï¼šä» moments.json çš„ moments æ•°ç»„ä¸­è¯»å– start_position/end_position
    2. æ–°æ ¼å¼ï¼šä» metadata.json çš„ marks æ•°ç»„ä¸­è¯»å– startPosition/endPosition

    Args:
        moments_json_path: moments.json æ–‡ä»¶è·¯å¾„ï¼ˆæ—§æ ¼å¼ï¼‰
        metadata_json_path: metadata.json æ–‡ä»¶è·¯å¾„ï¼ˆæ–°æ ¼å¼ï¼Œå¯é€‰ï¼‰

    Returns:
        tuple: (start_time, end_time) æˆ– (None, None) å¦‚æœå¤±è´¥
    """
    # ä¼˜å…ˆå°è¯•ä»æ–°æ ¼å¼çš„ metadata.json è¯»å–
    if metadata_json_path and os.path.exists(metadata_json_path):
        try:
            with open(metadata_json_path, "r", encoding="utf-8") as f:
                metadata_data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ ¼å¼ï¼ˆåŒ…å« marks å­—æ®µï¼‰
            if "marks" in metadata_data and isinstance(metadata_data.get("marks"), list):
                marks = metadata_data.get("marks", [])
                if not marks:
                    print(f"[MOMENTS] metadata.jsonä¸­æœªæ‰¾åˆ°marksæ•°æ®")
                else:
                    start_positions = []
                    end_positions = []
                    
                    for mark in marks:
                        start_pos = mark.get("startPosition")
                        end_pos = mark.get("endPosition")
                        
                        if start_pos is not None:
                            try:
                                start_positions.append(float(start_pos))
                            except (ValueError, TypeError):
                                print(f"[MOMENTS] æ— æ•ˆçš„startPositionå€¼: {start_pos}")
                                pass
                        
                        if end_pos is not None:
                            try:
                                end_positions.append(float(end_pos))
                            except (ValueError, TypeError):
                                print(f"[MOMENTS] æ— æ•ˆçš„endPositionå€¼: {end_pos}")
                                pass
                    
                    if start_positions and end_positions:
                        moments_start_time = min(start_positions)
                        moments_end_time = max(end_positions)
                        
                        print(
                            f"[MOMENTS] ä»metadata.jsonï¼ˆæ–°æ ¼å¼ï¼‰è·å–æ—¶é—´èŒƒå›´: {moments_start_time} - {moments_end_time}"
                        )
                        print(
                            f"[MOMENTS] æ‰¾åˆ° {len(start_positions)} ä¸ªstartPosition, {len(end_positions)} ä¸ªendPosition"
                        )
                        
                        return moments_start_time, moments_end_time
                    else:
                        print(f"[MOMENTS] metadata.jsonä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´ä½ç½®ä¿¡æ¯")
        except Exception as e:
            print(f"[MOMENTS] è¯»å–metadata.jsonæ—¶å‡ºé”™: {e}")
    
    # å›é€€åˆ°æ—§æ ¼å¼ï¼šä» moments.json è¯»å–
    if not moments_json_path or not os.path.exists(moments_json_path):
        return None, None

    try:
        with open(moments_json_path, "r", encoding="utf-8") as f:
            moments_data = json.load(f)

        moments = moments_data.get("moments", [])
        if not moments:
            print(f"[MOMENTS] moments.jsonä¸­æœªæ‰¾åˆ°momentsæ•°æ®")
            return None, None

        start_positions = []
        end_positions = []

        for moment in moments:
            custom_fields = moment.get("customFieldValues", {})
            start_pos = custom_fields.get("start_position")
            end_pos = custom_fields.get("end_position")

            if start_pos is not None:
                try:
                    start_positions.append(float(start_pos))
                except (ValueError, TypeError):
                    print(f"[MOMENTS] æ— æ•ˆçš„start_positionå€¼: {start_pos}")
                    pass

            if end_pos is not None:
                try:
                    end_positions.append(float(end_pos))
                except (ValueError, TypeError):
                    print(f"[MOMENTS] æ— æ•ˆçš„end_positionå€¼: {end_pos}")
                    pass

        # ä½¿ç”¨æœ€æ—©çš„start_positionå’Œæœ€æ™šçš„end_position
        if start_positions and end_positions:
            moments_start_time = min(start_positions)
            moments_end_time = max(end_positions)

            print(
                f"[MOMENTS] ä»moments.jsonè·å–æ—¶é—´èŒƒå›´: {moments_start_time} - {moments_end_time}"
            )
            print(
                f"[MOMENTS] æ‰¾åˆ° {len(start_positions)} ä¸ªstart_position, {len(end_positions)} ä¸ªend_position"
            )

            return moments_start_time, moments_end_time
        else:
            print(f"[MOMENTS] moments.jsonä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´ä½ç½®ä¿¡æ¯")
            return None, None

    except Exception as e:
        print(f"[MOMENTS] è¯»å–moments.jsonæ—¶å‡ºé”™: {e}")
        return None, None


def get_bag_time_info(bag_path: str) -> dict:
    """
    è·å– rosbag åŒ…çš„æ—¶é—´ä¿¡æ¯

    Args:
        bag_path: rosbag æ–‡ä»¶è·¯å¾„

    Returns:
        dict: åŒ…å«æ—¶é—´ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…æ‹¬ï¼š
            - unix_timestamp: Unixæ—¶é—´æˆ³ï¼ˆç§’ï¼‰
            - iso_format: ISOæ ¼å¼æ—¶é—´å­—ç¬¦ä¸²ï¼ˆä¸œå…«åŒºï¼‰
            - nanoseconds: çº³ç§’æ ¼å¼æ—¶é—´æˆ³
            - duration: bagæŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
            - end_time: ç»“æŸæ—¶é—´Unixæ—¶é—´æˆ³
    """
    try:
        bag = rosbag.Bag(bag_path, "r")
        bag_start_time = bag.get_start_time()
        bag_end_time = bag.get_end_time()
        bag_duration = bag_end_time - bag_start_time
        bag.close()

        # è½¬æ¢ä¸ºå¸¦æ—¶åŒºçš„ISOæ ¼å¼ï¼ˆä¸œå…«åŒºï¼‰
        start_datetime = datetime.datetime.fromtimestamp(
            bag_start_time, tz=datetime.timezone(datetime.timedelta(hours=8))
        )
        start_iso = start_datetime.isoformat()

        # è½¬æ¢ä¸ºçº³ç§’
        start_nanoseconds = int(bag_start_time * 1e9)

        return {
            "unix_timestamp": bag_start_time,
            "iso_format": start_iso,
            "nanoseconds": start_nanoseconds,
            "duration": bag_duration,
            "end_time": bag_end_time,
        }

    except Exception as e:
        print(f"è·å–bagæ—¶é—´ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "unix_timestamp": None,
            "iso_format": None,
            "nanoseconds": None,
            "duration": None,
            "end_time": None,
        }


def list_bag_files_auto(raw_dir):
    bag_files = []
    for i, fname in enumerate(sorted(os.listdir(raw_dir))):
        if fname.endswith(".bag"):
            bag_files.append(
                {
                    "link": "",  # ä¿æŒä¸ºç©º
                    "start": 0,  # æ‰¹é‡è®¾ç½®ä¸º0
                    "end": 1,  # æ‰¹é‡è®¾ç½®ä¸º1
                    "local_path": os.path.join(raw_dir, fname),
                }
            )
    return bag_files


def load_raw_depth_lerobot(
    bag_data: dict, default_camera_names: list[str]
) -> dict[str, np.ndarray]:
    imgs_per_cam = {}
    for camera in default_camera_names:
        key = f"{camera}_depth"
        imgs_per_cam[camera] = np.array([msg["data"] for msg in bag_data[key]])
        # print(f"camera {camera} image", imgs_per_cam[camera].shape)

    return imgs_per_cam


def load_raw_depth_images_per_camera(bag_data: dict, default_camera_names: list[str]):
    imgs_per_cam = {}
    compressed_per_cam = {}
    for camera in default_camera_names:
        key = f"{camera}_depth"
        imgs_per_cam[camera] = [msg["data"] for msg in bag_data[key]]
        # åªå–ç¬¬ä¸€å¸§çš„å‹ç¼©çŠ¶æ€ï¼ˆå‡è®¾æ‰€æœ‰å¸§ä¸€è‡´ï¼‰
        if bag_data[key]:
            compressed_per_cam[camera] = bag_data[key][0].get("compressed", None)
        else:
            compressed_per_cam[camera] = None
    print("+" * 20, compressed_per_cam)
    return imgs_per_cam, compressed_per_cam


def load_camera_info_per_camera(
    bag_data: dict, default_camera_names: list[str]
) -> dict:
    info_per_cam = {}
    distortion_model = {}
    for camera in default_camera_names:
        info_per_cam[camera] = np.array(
            [msg["data"] for msg in bag_data[f"{camera}_camera_info"]], dtype=np.float32
        )
        distortion_model[camera] = [
            msg["distortion_model"] for msg in bag_data[f"{camera}_camera_info"]
        ]
    return info_per_cam, distortion_model


def load_raw_images_per_camera(
    bag_data: dict, default_camera_names: list[str]
) -> dict[str, list]:
    imgs_per_cam = {}
    for camera in default_camera_names:
        imgs_per_cam[camera] = [msg["data"] for msg in bag_data[camera]]
    return imgs_per_cam


def load_raw_episode_data(
    raw_config: Config,
    ep_path: Path,
    start_time: float = 0,
    end_time: float = 1,
    action_config=None,
    min_duration: float = 5.0,
    metadata_json_dir: str = None,
):
    sn_code = None
    if metadata_json_dir and os.path.exists(metadata_json_dir):
        try:
            with open(metadata_json_dir, "r", encoding="utf-8") as f:
                raw_metadata = json.load(f)
            sn_code = raw_metadata.get("device_sn", "")
        except Exception as e:
            print(f"[WARN] è¯»å–metadata.jsonå¤±è´¥: {e})")
    bag_reader = KuavoRosbagReader(raw_config)
    bag_data = bag_reader.process_rosbag(
        ep_path, start_time=start_time, end_time=end_time, action_config=action_config
    )
    if sn_code is not None:
        main_time_line_timestamps = None
        if "camera_top" in bag_data and len(bag_data["camera_top"]) > 0:
            main_time_line_timestamps = np.array(
                [msg["timestamp"] for msg in bag_data["camera_top"]]
            )
        else:
            main_time_line_timestamps = None
        swap_left_right_data_if_needed(bag_data, sn_code, main_time_line_timestamps)
    # 1. å¤„ç†å®Œ bag_data åç«‹å³æå–æ‰€éœ€æ•°æ®å¹¶æ¸…ç†
    sensors_data_raw__joint_q = state = np.array(
        [msg["data"] for msg in bag_data["observation.sensorsData.joint_q"]],
        dtype=np.float32,
    )
    joint_cmd__joint_q = action = np.array(
        [msg["data"] for msg in bag_data["action.joint_cmd.joint_q"]],
        dtype=np.float32,
    )
    kuavo_arm_traj__position = action_kuavo_arm_traj = np.array(
        [msg["data"] for msg in bag_data["action.kuavo_arm_traj"]],
        dtype=np.float32,
    )

    # æ‰‹éƒ¨æ•°æ®
    leju_claw_state__position = claw_state = np.array(
        [msg["data"] for msg in bag_data["observation.claw"]],
        dtype=np.float32,
    )
    leju_claw_command__position = claw_action = np.array(
        [msg["data"] for msg in bag_data["action.claw"]],
        dtype=np.float32,
    )

    # control_robot_hand_position_state_both = qiangnao_state = np.array(
    #     [msg["data"] for msg in bag_data["observation.qiangnao"]], dtype=np.float32,
    # )
    # control_robot_hand_position_both = qiangnao_action = np.array(
    #     [msg["data"] for msg in bag_data["action.qiangnao"]], dtype=np.float32,
    # )
    qiangnao_state = None
    try:
        qiangnao_state = np.array(
            [msg["data"] for msg in bag_data["observation.qiangnao"]],
            dtype=np.float32,
        )
    except KeyError:
        print("[WARN] æœªæ‰¾åˆ° 'observation.qiangnao' æ•°æ®")
    qiangnao_action = None
    try:
        qiangnao_action = np.array(
            [msg["data"] for msg in bag_data["action.qiangnao"]],
            dtype=np.float32,
        )
    except KeyError:
        print("[WARN] æœªæ‰¾åˆ° 'action.qiangnao' æ•°æ®")

    hand_state_left = None
    hand_state_right = None
    hand_action_left = None
    hand_action_right = None

    if "observation.qiangnao_left" in bag_data:
        hand_state_left = np.array(
            [msg["data"] for msg in bag_data["observation.qiangnao_left"]],
            dtype=np.float32,
        )
    if "observation.qiangnao_right" in bag_data:
        hand_state_right = np.array(
            [msg["data"] for msg in bag_data["observation.qiangnao_right"]],
            dtype=np.float32,
        )
    if "action.qiangnao_left" in bag_data:
        hand_action_left = np.array(
            [msg["data"] for msg in bag_data["action.qiangnao_left"]],
            dtype=np.float32,
        )
    if "action.qiangnao_right" in bag_data:
        hand_action_right = np.array(
            [msg["data"] for msg in bag_data["action.qiangnao_right"]],
            dtype=np.float32,
        )

    if (
        (hand_state_left is None or hand_state_right is None)
        and qiangnao_state is not None
    ):
        split_left, split_right = _split_dexhand_lr(qiangnao_state)
        if split_left is not None:
            hand_state_left = split_left
        if split_right is not None:
            hand_state_right = split_right
    if (
        (hand_action_left is None or hand_action_right is None)
        and qiangnao_action is not None
    ):
        split_left, split_right = _split_dexhand_lr(qiangnao_action)
        if split_left is not None:
            hand_action_left = split_left
        if split_right is not None:
            hand_action_right = split_right

    # é€Ÿåº¦å’Œç”µæµæ•°æ®
    sensors_data_raw__joint_v = state_joint_v = np.array(
        [msg["data"] for msg in bag_data["observation.sensorsData.joint_v"]],
        dtype=np.float32,
    )
    state_joint_current = np.array(
        [msg["data"] for msg in bag_data["observation.sensorsData.joint_current"]],
        dtype=np.float32,
    )

    # å›¾åƒæ•°æ®
    import psutil

    process = psutil.Process()
    mem_before = process.memory_info().rss / 1024 / 1024
    print(f"[å†…å­˜] æå–å›¾åƒå‰: {mem_before:.1f} MB")

    imgs_per_cam = load_raw_images_per_camera(bag_data, raw_config.default_camera_names)
    mem_after_color = process.memory_info().rss / 1024 / 1024
    print(
        f"[å†…å­˜] å½©è‰²å›¾åƒæå–å: {mem_after_color:.1f} MB (å¢é•¿ {mem_after_color - mem_before:.1f} MB)"
    )

    imgs_per_cam_depth, compressed = load_raw_depth_images_per_camera(
        bag_data, raw_config.default_camera_names
    )
    mem_after_depth = process.memory_info().rss / 1024 / 1024
    print(
        f"[å†…å­˜] æ·±åº¦å›¾åƒæå–å: {mem_after_depth:.1f} MB (å¢é•¿ {mem_after_depth - mem_after_color:.1f} MB)"
    )

    info_per_cam, distortion_model = load_camera_info_per_camera(
        bag_data, raw_config.default_camera_names
    )
    mem_after_info = process.memory_info().rss / 1024 / 1024
    print(
        f"[å†…å­˜] ç›¸æœºä¿¡æ¯æå–å: {mem_after_info:.1f} MB (å¢é•¿ {mem_after_info - mem_after_depth:.1f} MB)"
    )
    main_time_line_timestamps = np.array(
        [msg["timestamp"] for msg in bag_data["camera_top"]]
    )
    if sn_code is not None:
        imgs_per_cam, imgs_per_cam_depth = flip_camera_arrays_if_needed(
            imgs_per_cam, imgs_per_cam_depth, sn_code, main_time_line_timestamps[0]
        )
    else:
        print("[WARN] æœªæä¾›sn_codeï¼Œè·³è¿‡ç›¸æœºç¿»è½¬æ£€æµ‹")
    # æ—¶é—´æˆ³å’Œç›¸æœºå¤–å‚

    head_extrinsics = bag_data.get("head_camera_extrinsics", [])
    left_extrinsics = bag_data.get("left_hand_camera_extrinsics", [])
    right_extrinsics = bag_data.get("right_hand_camera_extrinsics", [])
    end_position = np.array(
        [msg["data"] for msg in bag_data["end.position"]],
        dtype=np.float32,
    )
    end_orientation = np.array(
        [msg["data"] for msg in bag_data["end.orientation"]],
        dtype=np.float32,
    )
    sensors_data_raw__imu_data = state_joint_imu = np.array(
        [msg["data"] for msg in bag_data["observation.sensorsData.imu"]],
        dtype=np.float32,
    )

    # 2. ç«‹å³æ¸…ç† bag_data å’Œ bag_reader
    mem_before_del = process.memory_info().rss / 1024 / 1024
    print(f"[å†…å­˜] åˆ é™¤ bag_data å‰: {mem_before_del:.1f} MB")

    del bag_data
    del bag_reader
    gc.collect()

    mem_after_del = process.memory_info().rss / 1024 / 1024
    print(
        f"[å†…å­˜] åˆ é™¤ bag_data å: {mem_after_del:.1f} MB (é‡Šæ”¾ {mem_before_del - mem_after_del:.1f} MB)"
    )

    # 3. å¤„ç†ç”µæœºæ•°æ®ï¼ˆè¿™äº›è®¡ç®—æ¯”è¾ƒæ¶ˆè€—å†…å­˜ï¼‰
    action[:, 12:26] = action_kuavo_arm_traj
    del action_kuavo_arm_traj  # ç«‹å³åˆ é™¤ä¸´æ—¶å˜é‡

    sensors_data_raw__joint_effort = state_joint_effort = (
        PostProcessorUtils.current_to_torque_batch(
            state_joint_current,
            MOTOR_C2T=[
                2,
                1.05,
                1.05,
                2,
                2.1,
                2.1,
                2,
                1.05,
                1.05,
                2,
                2.1,
                2.1,
                1.05,
                5,
                2.3,
                5,
                4.7,
                4.7,
                4.7,
                1.05,
                5,
                2.3,
                5,
                4.7,
                4.7,
                4.7,
                0.21,
                4.7,
            ],
        )
    )

    sensors_data_raw__joint_current = PostProcessorUtils.torque_to_current_batch(
        state_joint_current,
        MOTOR_C2T=[
            2,
            1.05,
            1.05,
            2,
            2.1,
            2.1,
            2,
            1.05,
            1.05,
            2,
            2.1,
            2.1,
            1.05,
            5,
            2.3,
            5,
            4.7,
            4.7,
            4.7,
            1.05,
            5,
            2.3,
            5,
            4.7,
            4.7,
            4.7,
            0.21,
            4.7,
        ],
    )

    # 4. æå–å­æ•°ç»„å¹¶æ¸…ç†åŸå§‹æ•°ç»„
    head_effort = sensors_data_raw__joint_effort[:, 26:28]
    head_current = sensors_data_raw__joint_current[:, 26:28]
    joint_effort = sensors_data_raw__joint_effort[:, 12:26]
    joint_current = sensors_data_raw__joint_current[:, 12:26]

    # æ¸…ç†ä¸€äº›ä¸å†éœ€è¦çš„ä¸´æ—¶å˜é‡
    del state_joint_current
    gc.collect()

    # 5. å¤„ç†æ—¶é—´æˆ³
    main_time_line_timestamps_ns = (main_time_line_timestamps * 1e9).astype(np.int64)

    velocity = None
    effort = None

    # 6. æ„å»º all_low_dim_dataï¼ˆè¿™æ˜¯è¿”å›çš„ä¸»è¦æ•°æ®ç»“æ„ï¼‰
    all_low_dim_data = {
        "timestamps": main_time_line_timestamps_ns,
        "action": {
            "effector": {
                "position(gripper)": leju_claw_command__position,
                "index": main_time_line_timestamps_ns,
            },
            "hand_left": {
                "position": hand_action_left,
                "index": main_time_line_timestamps_ns,
            },
            "hand_right": {
                "position": hand_action_right,
                "index": main_time_line_timestamps_ns,
            },
            "joint": {
                "position": kuavo_arm_traj__position,
                "index": main_time_line_timestamps_ns,
            },
            "head": {
                "position": joint_cmd__joint_q[:, 26:28],
                "index": main_time_line_timestamps_ns,
            },
            "leg": {
                "position": joint_cmd__joint_q[:, :12],
                "index": main_time_line_timestamps_ns,
            },
        },
        "state": {
            "effector": {
                "position(gripper)": leju_claw_state__position,
            },
            "hand_left": {
                "position": hand_state_left,
            },
            "hand_right": {
                "position": hand_state_right,
            },
            "head": {
                "current_value": head_current,
                "effort": head_effort,
                "position": sensors_data_raw__joint_q[:, 26:28],
                "velocity": sensors_data_raw__joint_v[:, 26:28],
            },
            "joint": {
                "current_value": joint_current,
                "effort": joint_effort,
                "position": sensors_data_raw__joint_q[:, 12:26],
                "velocity": sensors_data_raw__joint_v[:, 12:26],
            },
            "end": {
                "position": end_position,
                "orientation": end_orientation,
            },
            "leg": {
                "current_value": sensors_data_raw__joint_current[:, :12],
                "effort": sensors_data_raw__joint_effort[:, :12],
                "position": sensors_data_raw__joint_q[:, 0:12],
                "velocity": sensors_data_raw__joint_v[:, 0:12],
            },
        },
        "imu": {
            "gyro_xyz": sensors_data_raw__imu_data[:, 0:3],
            "acc_xyz": sensors_data_raw__imu_data[:, 3:6],
            "free_acc_xyz": sensors_data_raw__imu_data[:, 6:9],
            "quat_xyzw": sensors_data_raw__imu_data[:, 9:13],
        },
    }

    # 7. è¿”å›å‰æœ€åä¸€æ¬¡å†…å­˜æ¸…ç†
    del kuavo_arm_traj__position
    gc.collect()

    return (
        imgs_per_cam,
        imgs_per_cam_depth,
        info_per_cam,
        all_low_dim_data,
        main_time_line_timestamps,
        distortion_model,
        head_extrinsics,
        left_extrinsics,
        right_extrinsics,
        compressed,
        state,
        action,
        claw_state,
        claw_action,
        qiangnao_state,
        qiangnao_action,
    )


import multiprocessing


def load_raw_episode_worker(raw_config, ep_path, start_time, end_time, queue):
    try:
        result = load_raw_episode_data(
            raw_config=raw_config,
            ep_path=ep_path,
            start_time=start_time,
            end_time=end_time,
        )
        queue.put({"ok": True, "data": result})
    except Exception as e:
        import traceback

        queue.put({"ok": False, "error": str(e), "traceback": traceback.format_exc()})


def load_hand_data_worker(config, first_bag_path, first_start, first_end, queue):
    try:
        claw_state, claw_action, qiangnao_state, qiangnao_action = process_rosbag_eef(
            config, first_bag_path, start_time=first_start, end_time=first_end
        )
        queue.put(
            {
                "ok": True,
                "data": (claw_state, claw_action, qiangnao_state, qiangnao_action),
            }
        )
    except Exception as e:
        import traceback

        queue.put({"ok": False, "error": str(e), "traceback": traceback.format_exc()})


def process_rosbag_eef(config, bag_path, start_time=0, end_time=1):
    """
    åªè¯»å–æ‰‹éƒ¨ç›¸å…³æ•°æ®ï¼Œä¸åšæ—¶é—´æˆ³å¯¹é½å’Œè¯é¢˜ç­›é€‰ã€‚
    åªéå†éœ€è¦çš„è¯é¢˜ï¼Œè¿”å› claw_state, claw_action, qiangnao_state, qiangnao_action
    """
    import rosbag
    import numpy as np

    claw_state = []
    claw_action = []
    qiangnao_state = []
    qiangnao_action = []
    cb_left_state = []
    cb_right_state = []
    cb_left_action = []
    cb_right_action = []

    # è¯é¢˜åæ ¹æ®ä½ çš„å®é™…å®šä¹‰
    topic_claw_state = "/leju_claw_state"
    topic_claw_action = "/leju_claw_command"
    topic_qiangnao_state = "/control_robot_hand_position_state"
    topic_qiangnao_action = "/control_robot_hand_position"
    topic_cb_left_state = "/cb_left_hand_state"
    topic_cb_right_state = "/cb_right_hand_state"
    topic_cb_left_action = "/cb_left_hand_control_cmd"
    topic_cb_right_action = "/cb_right_hand_control_cmd"

    bag = rosbag.Bag(bag_path, "r")
    bag_start = bag.get_start_time()
    bag_end = bag.get_end_time()
    bag_duration = bag_end - bag_start

    abs_start = bag_start + start_time * bag_duration
    abs_end = bag_start + end_time * bag_duration

    # åªéå†éœ€è¦çš„è¯é¢˜
    for topic, msg, t in bag.read_messages(
        topics=[
            topic_claw_state,
            topic_claw_action,
            topic_qiangnao_state,
            topic_qiangnao_action,
            topic_cb_left_state,
            topic_cb_right_state,
            topic_cb_left_action,
            topic_cb_right_action,
        ]
    ):
        if t.to_sec() < abs_start or t.to_sec() > abs_end:
            continue
        if topic == topic_claw_state:
            try:
                claw_state.append(np.array(msg.data.position, dtype=np.float64))
            except Exception:
                pass
        elif topic == topic_claw_action:
            try:
                claw_action.append(np.array(msg.data.position, dtype=np.float64))
            except Exception:
                pass
        elif topic == topic_qiangnao_state:
            try:
                state = list(msg.left_hand_position) + list(msg.right_hand_position)
                qiangnao_state.append(np.array(state, dtype=np.float64))
            except Exception:
                pass
        elif topic == topic_qiangnao_action:
            try:
                position = list(msg.left_hand_position) + list(msg.right_hand_position)
                qiangnao_action.append(np.array(position, dtype=np.float64))
            except Exception:
                pass
        elif topic == topic_cb_left_state:
            try:
                cb_left_state.append(np.array(msg.position, dtype=np.float64))
            except Exception:
                pass
        elif topic == topic_cb_right_state:
            try:
                cb_right_state.append(np.array(msg.position, dtype=np.float64))
            except Exception:
                pass
        elif topic == topic_cb_left_action:
            try:
                cb_left_action.append(np.array(msg.position, dtype=np.float64))
            except Exception:
                pass
        elif topic == topic_cb_right_action:
            try:
                cb_right_action.append(np.array(msg.position, dtype=np.float64))
            except Exception:
                pass

    bag.close()

    claw_state = np.array(claw_state)
    claw_action = np.array(claw_action)
    qiangnao_state = np.array(qiangnao_state)
    qiangnao_action = np.array(qiangnao_action)
    cb_left_state = np.array(cb_left_state)
    cb_right_state = np.array(cb_right_state)
    cb_left_action = np.array(cb_left_action)
    cb_right_action = np.array(cb_right_action)

    if qiangnao_state.size == 0:
        if cb_left_state.size > 0:
            qiangnao_state = cb_left_state
        elif cb_right_state.size > 0:
            qiangnao_state = cb_right_state
    if qiangnao_action.size == 0:
        if cb_left_action.size > 0:
            qiangnao_action = cb_left_action
        elif cb_right_action.size > 0:
            qiangnao_action = cb_right_action

    return claw_state, claw_action, qiangnao_state, qiangnao_action


def _split_dexhand_lr(arr):
    if arr is None:
        return None, None
    arr = np.array(arr)
    if arr.ndim != 2 or arr.shape[1] < 12:
        return None, None
    left = arr[:, :6]
    right = arr[:, 6:12]
    return left, right


def port_kuavo_rosbag(
    raw_config: Config,
    repo_id: str = "lerobot/kuavo",
    raw_repo_id: str | None = None,
    task: str = "DEBUG",
    *,
    episodes: list[int] | None = None,
    mode: Literal["video", "image"] = "video",
    processed_files: list[dict[str, str]] | list[str] = [],
    moment_json_DIR: str | None = None,
    metadata_json_DIR: str | None = None,
    lerobot_dir: str | None = None,
    use_depth: bool = True,
):

    from kuavo_dataset_slave_s import (
        KuavoRosbagReader,
        DEFAULT_JOINT_NAMES_LIST,
        DEFAULT_LEG_JOINT_NAMES,
        DEFAULT_ARM_JOINT_NAMES,
        DEFAULT_HEAD_JOINT_NAMES,
        DEFAULT_JOINT_NAMES,
        DEFAULT_LEJUCLAW_JOINT_NAMES,
        DEFAULT_DEXHAND_JOINT_NAMES,
        PostProcessorUtils,
    )

    config = raw_config

    # å¤„ç†å¹¶è¡Œ ROSbag è¯»å–ç¯å¢ƒå˜é‡
    env_parallel = os.environ.get("USE_PARALLEL_ROSBAG_READ", "").lower()
    if env_parallel in ("true", "1", "yes"):
        config.use_parallel_rosbag_read = True
        print(
            "[CONFIG] å¹¶è¡Œ ROSbag è¯»å–å·²é€šè¿‡ç¯å¢ƒå˜é‡å¯ç”¨ (USE_PARALLEL_ROSBAG_READ=true)"
        )
    elif env_parallel in ("false", "0", "no"):
        config.use_parallel_rosbag_read = False

    env_workers = os.environ.get("PARALLEL_ROSBAG_WORKERS", "")
    if env_workers.isdigit():
        config.parallel_rosbag_workers = int(env_workers)
        print(f"[CONFIG] å¹¶è¡Œ worker æ•°é‡: {config.parallel_rosbag_workers}")

    RAW_DIR = config.raw_dir
    ID = config.id
    CONTROL_HAND_SIDE = config.which_arm
    SLICE_ROBOT = config.slice_robot
    SLICE_DEX = config.dex_slice
    SLICE_CLAW = config.claw_slice
    IS_BINARY = config.is_binary
    DELTA_ACTION = config.delta_action
    RELATIVE_START = config.relative_start
    ONLY_HALF_UP_BODY = config.only_arm
    USE_LEJU_CLAW = config.use_leju_claw
    USE_QIANGNAO = config.use_qiangnao
    SEPARATE_HAND_FIELDS = getattr(config, "separate_hand_fields", False)
    MERGE_HAND_POSITION = getattr(config, "merge_hand_position", False)

    DEFAULT_JOINT_NAMES_LIST_ORIGIN = DEFAULT_JOINT_NAMES_LIST
    DEFAULT_ARM_JOINT_NAMES_ORIGIN = DEFAULT_ARM_JOINT_NAMES

    # ä¸ºæ•´æ¬¡å¯¼å‡ºåˆ›å»º uuid æ ¹ç›®å½•
    episode_uuid = str(uuid.uuid4())
    base_root = os.path.join(lerobot_dir, episode_uuid)
    if os.path.exists(base_root):
        shutil.rmtree(base_root)
    os.makedirs(base_root, exist_ok=True)

    # 1) è¯»å–ç¬¬ä¸€ä¸ª bagï¼Œæ£€æµ‹å®é™…æ‰‹å‹ï¼ˆä¸åŸé€»è¾‘ä¸€è‡´ï¼‰
    first_bag_info = processed_files[0]
    first_bag_path = (
        first_bag_info["local_path"]
        if isinstance(first_bag_info, dict)
        else first_bag_info
    )
    first_start = (
        first_bag_info.get("start", 0) if isinstance(first_bag_info, dict) else 0
    )
    first_end = first_bag_info.get("end", 1) if isinstance(first_bag_info, dict) else 1

    queue = multiprocessing.Queue()
    p = multiprocessing.Process(
        target=load_hand_data_worker,
        args=(config, first_bag_path, first_start, first_end, queue),
    )
    p.start()
    result = queue.get()
    p.join()
    if not result.get("ok"):
        print("å­è¿›ç¨‹å¼‚å¸¸é€€å‡ºï¼")
        print(result.get("error"))
        print(result.get("traceback"))
        sys.exit(1)

    (
        claw_state_probe,
        claw_action_probe,
        qiangnao_state_probe,
        qiangnao_action_probe,
    ) = result["data"]
    USE_LEJU_CLAW = is_valid_hand_data(claw_state_probe) or is_valid_hand_data(
        claw_action_probe
    )
    USE_QIANGNAO = is_valid_hand_data(qiangnao_state_probe) or is_valid_hand_data(
        qiangnao_action_probe
    )
    print(f"æ£€æµ‹åˆ°æ‰‹éƒ¨ç±»å‹: USE_LEJU_CLAW={USE_LEJU_CLAW}, USE_QIANGNAO={USE_QIANGNAO}")

    half_arm = len(DEFAULT_ARM_JOINT_NAMES) // 2
    half_claw = len(DEFAULT_LEJUCLAW_JOINT_NAMES) // 2
    half_dexhand = len(DEFAULT_DEXHAND_JOINT_NAMES) // 2
    UP_START_INDEX = 12
    if ONLY_HALF_UP_BODY:
        if SEPARATE_HAND_FIELDS:
            DEFAULT_ARM_JOINT_NAMES = DEFAULT_ARM_JOINT_NAMES_ORIGIN
        if USE_LEJU_CLAW:
            DEFAULT_ARM_JOINT_NAMES = (
                DEFAULT_ARM_JOINT_NAMES[:half_arm]
                + DEFAULT_LEJUCLAW_JOINT_NAMES[:half_claw]
                + DEFAULT_ARM_JOINT_NAMES[half_arm:]
                + DEFAULT_LEJUCLAW_JOINT_NAMES[half_claw:]
            )
            arm_slice = [
                (
                    SLICE_ROBOT[0][0] - UP_START_INDEX,
                    SLICE_ROBOT[0][-1] - UP_START_INDEX,
                ),
                (SLICE_CLAW[0][0] + half_arm, SLICE_CLAW[0][-1] + half_arm),
                (
                    SLICE_ROBOT[1][0] - UP_START_INDEX + half_claw,
                    SLICE_ROBOT[1][-1] - UP_START_INDEX + half_claw,
                ),
                (SLICE_CLAW[1][0] + half_arm * 2, SLICE_CLAW[1][-1] + half_arm * 2),
            ]
        elif USE_QIANGNAO and not SEPARATE_HAND_FIELDS:
            DEFAULT_ARM_JOINT_NAMES = (
                DEFAULT_ARM_JOINT_NAMES[:half_arm]
                + DEFAULT_DEXHAND_JOINT_NAMES[:half_dexhand]
                + DEFAULT_ARM_JOINT_NAMES[half_arm:]
                + DEFAULT_DEXHAND_JOINT_NAMES[half_dexhand:]
            )
            arm_slice = [
                (
                    SLICE_ROBOT[0][0] - UP_START_INDEX,
                    SLICE_ROBOT[0][-1] - UP_START_INDEX,
                ),
                (SLICE_DEX[0][0] + half_arm, SLICE_DEX[0][-1] + half_arm),
                (
                    SLICE_ROBOT[1][0] - UP_START_INDEX + half_dexhand,
                    SLICE_ROBOT[1][-1] - UP_START_INDEX + half_dexhand,
                ),
                (SLICE_DEX[1][0] + half_arm * 2, SLICE_DEX[1][-1] + half_arm * 2),
            ]
        if USE_QIANGNAO and SEPARATE_HAND_FIELDS:
            DEFAULT_JOINT_NAMES_LIST = DEFAULT_ARM_JOINT_NAMES
        else:
            DEFAULT_JOINT_NAMES_LIST = [
                DEFAULT_ARM_JOINT_NAMES[k] for l, r in arm_slice for k in range(l, r)
            ]
    else:
        if SEPARATE_HAND_FIELDS:
            DEFAULT_ARM_JOINT_NAMES = DEFAULT_ARM_JOINT_NAMES_ORIGIN
        if USE_LEJU_CLAW:
            DEFAULT_ARM_JOINT_NAMES = (
                DEFAULT_ARM_JOINT_NAMES[:half_arm]
                + DEFAULT_LEJUCLAW_JOINT_NAMES[:half_claw]
                + DEFAULT_ARM_JOINT_NAMES[half_arm:]
                + DEFAULT_LEJUCLAW_JOINT_NAMES[half_claw:]
            )
        elif USE_QIANGNAO and not SEPARATE_HAND_FIELDS:
            DEFAULT_ARM_JOINT_NAMES = (
                DEFAULT_ARM_JOINT_NAMES[:half_arm]
                + DEFAULT_DEXHAND_JOINT_NAMES[:half_dexhand]
                + DEFAULT_ARM_JOINT_NAMES[half_arm:]
                + DEFAULT_DEXHAND_JOINT_NAMES[half_dexhand:]
            )
        DEFAULT_JOINT_NAMES_LIST = (
            DEFAULT_LEG_JOINT_NAMES + DEFAULT_ARM_JOINT_NAMES + DEFAULT_HEAD_JOINT_NAMES
        )
    if MERGE_HAND_POSITION:
        DEFAULT_JOINT_NAMES_LIST = (
            list(DEFAULT_JOINT_NAMES_LIST)
            + DEFAULT_DEXHAND_JOINT_NAMES[:6]
            + DEFAULT_DEXHAND_JOINT_NAMES[6:12]
        )

    @dataclasses.dataclass(frozen=True)
    class DatasetConfig:
        use_videos: bool = True
        tolerance_s: float = 0.0001
        image_writer_processes: int = 6
        image_writer_threads: int = 12
        video_backend: str | None = None

    DEFAULT_DATASET_CONFIG = DatasetConfig()
    dataset_config = DEFAULT_DATASET_CONFIG

    def create_empty_dataset(
        repo_id: str,
        robot_type: str,
        mode: Literal["video", "image"] = "video",
        eef_type: Literal["leju_claw", "dex_hand"] = "dex_hand",
        *,
        has_depth_image: bool = False,
        dataset_config: DatasetConfig = DEFAULT_DATASET_CONFIG,
        root: str,
        extra_features: bool = True,
        raw_config: Config,
    ) -> LeRobotDataset:
        dexhand = [
            "left_linkerhand_1",
            "left_linkerhand_2",
            "left_linkerhand_3",
            "left_linkerhand_4",
            "left_linkerhand_5",
            "left_linkerhand_6",
            "right_linkerhand_1",
            "right_linkerhand_2",
            "right_linkerhand_3",
            "right_linkerhand_4",
            "right_linkerhand_5",
            "right_linkerhand_6",
        ]
        lejuclaw = [
            "left_claw",
            "right_claw",
        ]
        leg = [
            "l_leg_roll",
            "l_leg_yaw",
            "l_leg_pitch",
            "l_knee",
            "l_foot_pitch",
            "l_foot_roll",
            "r_leg_roll",
            "r_leg_yaw",
            "r_leg_pitch",
            "r_knee",
            "r_foot_pitch",
            "r_foot_roll",
        ]
        arm = [
            "zarm_l1_link",
            "zarm_l2_link",
            "zarm_l3_link",
            "zarm_l4_link",
            "zarm_l5_link",
            "zarm_l6_link",
            "zarm_l7_link",
            "zarm_r1_link",
            "zarm_r2_link",
            "zarm_r3_link",
            "zarm_r4_link",
            "zarm_r5_link",
            "zarm_r6_link",
            "zarm_r7_link",
        ]
        head = ["head_yaw", "head_pitch"]
        cameras = raw_config.default_camera_names
        imu_acc = ["acc_x", "acc_y", "acc_z"]
        imu_free_acc = ["free_acc_x", "ree_acc_y", "free_acc_z"]
        imu_gyro_acc = ["gyro_x", "gyro_y", "gyro_z"]
        imu_quat_acc = ["quat_x", "quat_y", "quat_z", "quat_w"]
        end_orientation = [
            "left_x",
            "left_y",
            "left_z",
            "left_w",
            "right_x",
            "right_y",
            "right_z",
            "right_w",
        ]
        end_position = ["left_x", "left_y", "left_z", "right_x", "right_y", "right_z"]
        # æ ¹æ®æœ«ç«¯æ‰§è¡Œå™¨ç±»å‹å®šä¹‰ç‰¹å¾
        features = {
            "observation.state.arm.position": {
                "dtype": "float32",
                "shape": (14,),
                "names": arm,
            },
            "observation.state.arm.effort": {
                "dtype": "float32",
                "shape": (14,),
                "names": arm,
            },
            "observation.state.arm.velocity": {
                "dtype": "float32",
                "shape": (14,),
                "names": arm,
            },
            "observation.state.arm.current_value": {
                "dtype": "float32",
                "shape": (14,),
                "names": arm,
            },
            "observation.state.end.position": {
                "dtype": "float32",
                "shape": (6,),
                "names": end_position,
            },
            "observation.state.end.orientation": {
                "dtype": "float32",
                "shape": (8,),
                "names": end_orientation,
            },
            # "observation.state.head.position" : {"dtype": "float32", "shape": (2,), "names": head},
            "observation.state.head.effort": {
                "dtype": "float32",
                "shape": (2,),
                "names": head,
            },
            "observation.state.head.position": {
                "dtype": "float32",
                "shape": (2,),
                "names": head,
            },
            "observation.state.head.velocity": {
                "dtype": "float32",
                "shape": (2,),
                "names": head,
            },
            "observation.state.leg.effort": {
                "dtype": "float32",
                "shape": (12,),
                "names": leg,
            },
            "observation.state.leg.position": {
                "dtype": "float32",
                "shape": (12,),
                "names": leg,
            },
            "observation.state.leg.velocity": {
                "dtype": "float32",
                "shape": (12,),
                "names": leg,
            },
            "observation.state.leg.current_value": {
                "dtype": "float32",
                "shape": (12,),
                "names": leg,
            },
            "action.head.position": {"dtype": "float32", "shape": (2,), "names": head},
            "action.arm.position": {"dtype": "float32", "shape": (14,), "names": arm},
            "action.leg.position": {"dtype": "float32", "shape": (12,), "names": leg},
            "imu.acc_xyz": {"dtype": "float32", "shape": (3,), "names": imu_acc},
            "imu.free_acc_xyz": {
                "dtype": "float32",
                "shape": (3,),
                "names": imu_free_acc,
            },
            "imu.gyro_xyz": {
                "dtype": "float32",
                "shape": (3,),
                "names": imu_gyro_acc,
            },
            "imu.quat_xyzw": {
                "dtype": "float32",
                "shape": (4,),
                "names": imu_quat_acc,
            },
        }

        # æ ¹æ®æœ«ç«¯æ‰§è¡Œå™¨ç±»å‹æ·»åŠ ç›¸åº”çš„ç‰¹å¾
        if eef_type == "leju_claw":
            features.update(
                {
                    "action.effector.position": {
                        "dtype": "float32",
                        "shape": (2,),
                        "names": lejuclaw,
                    },
                    "observation.state.effector.position": {
                        "dtype": "float32",
                        "shape": (2,),
                        "names": lejuclaw,
                    },
                }
            )
        elif eef_type == "dex_hand":
            features.update(
                {
                    "action.hand_left.position": {
                        "dtype": "float32",
                        "shape": (6,),
                        "names": dexhand[:6],
                    },
                    "action.hand_right.position": {
                        "dtype": "float32",
                        "shape": (6,),
                        "names": dexhand[6:],
                    },
                    "observation.state.hand_left.position": {
                        "dtype": "float32",
                        "shape": (6,),
                        "names": dexhand[:6],
                    },
                    "observation.state.hand_right.position": {
                        "dtype": "float32",
                        "shape": (6,),
                        "names": dexhand[6:],
                    },
                    "observation.state.hand_left.force_torque": {
                        "dtype": "float32",
                        "shape": (6,),
                        "names": [
                            "force_x",
                            "force_y",
                            "force_z",
                            "torque_x",
                            "torque_y",
                            "torque_z",
                        ],
                    },
                    "observation.state.hand_right.force_torque": {
                        "dtype": "float32",
                        "shape": (6,),
                        "names": [
                            "force_x",
                            "force_y",
                            "force_z",
                            "torque_x",
                            "torque_y",
                            "torque_z",
                        ],
                    },
                    "observation.state.hand_left.touch_matrix": {
                        "dtype": "float32",
                        "shape": (360,),
                        "names": None,
                    },
                    "observation.state.hand_right.touch_matrix": {
                        "dtype": "float32",
                        "shape": (360,),
                        "names": None,
                    },
                }
            )

        # ç›¸æœºç‰¹å¾ï¼šå¦‚æœè§†é¢‘å•ç‹¬å­˜å‚¨ï¼Œä¸æ·»åŠ å›¾åƒfeatures
        separate_video_storage = getattr(raw_config, "separate_video_storage", False)

        if not separate_video_storage:
            # åŸæœ‰é€»è¾‘ï¼šæ·»åŠ å›¾åƒ/è§†é¢‘features
            for cam in cameras:
                features[f"observation.images.{cam}"] = {
                    "dtype": mode,
                    "shape": (3, 480, 848),
                    "names": ["channels", "height", "width"],
                }
                if has_depth_image:
                    features[f"observation.images.depth.{cam}"] = {
                        "dtype": mode,
                        "shape": (480, 848),
                        "names": ["height", "width"],
                    }

        for cam in cameras:
            features[f"observation.camera_params.rotation_matrix_flat.{cam}"] = {
                "dtype": "float32",
                "shape": (9,),
                "names": None,
            }
            features[f"observation.camera_params.translation_vector.{cam}"] = {
                "dtype": "float32",
                "shape": (3,),
                "names": None,
            }
        if extra_features:
            features["observation.state"] = {
                "dtype": "float32",
                "shape": (len(DEFAULT_JOINT_NAMES_LIST),),
                "names": DEFAULT_JOINT_NAMES_LIST,
            }
            features["action"] = {
                "dtype": "float32",
                "shape": (len(DEFAULT_JOINT_NAMES_LIST),),
                "names": DEFAULT_JOINT_NAMES_LIST,
            }
            print("DEFAULT_JOINT_NAMES_LIST", DEFAULT_JOINT_NAMES_LIST)

        if Path(LEROBOT_HOME / repo_id).exists():
            shutil.rmtree(LEROBOT_HOME / repo_id)

        # å¦‚æœè§†é¢‘å•ç‹¬å­˜å‚¨ï¼Œfeaturesä¸­å·²ç»æ²¡æœ‰videoç±»å‹ï¼Œuse_videosä¿æŒåŸå€¼å³å¯
        return LeRobotDataset.create(
            repo_id=repo_id,
            fps=raw_config.train_hz,
            robot_type=robot_type,
            features=features,
            use_videos=dataset_config.use_videos,
            tolerance_s=dataset_config.tolerance_s,
            image_writer_processes=dataset_config.image_writer_processes,
            image_writer_threads=dataset_config.image_writer_threads,
            video_backend=dataset_config.video_backend,
            root=root,
        )

    def populate_dataset_stream(
        raw_config: Config,
        bag_files: list,
        task: str,
        moment_json_dir: str | None,
        base_root: str,
        metadata_json_dir: str | None = None,
        pipeline_encoder: "BatchSegmentEncoder | None" = None,
        streaming_encoder: "StreamingVideoEncoderManager | None" = None,
    ):
        import psutil

        process = psutil.Process()

        # è¯»å– metadata.json è·å– sn_codeï¼ˆç›¸æœºå·¦å³ç¿»è½¬åˆ¤å®šï¼‰
        sn_code = None
        if metadata_json_dir and os.path.exists(metadata_json_dir):
            try:
                with open(metadata_json_dir, "r", encoding="utf-8") as f:
                    raw_metadata = json.load(f)
                # æ”¯æŒæ–°æ ¼å¼ï¼ˆdeviceSnï¼‰å’Œæ—§æ ¼å¼ï¼ˆdevice_snï¼‰
                sn_code = raw_metadata.get("deviceSn") or raw_metadata.get("device_sn", "")
            except Exception as e:
                print(f"[WARN] è¯»å–metadata.jsonå¤±è´¥: {e})")

        if len(bag_files) == 0:
            print("[WARN] æ—  bag æ–‡ä»¶")
            return None, None

        # éå†æ¯ä¸ª bag
        for ep_idx, bag_info in enumerate(bag_files):
            if isinstance(bag_info, dict):
                ep_path = bag_info["local_path"]
                start_time = bag_info.get("start", 0)
                end_time = bag_info.get("end", 1)
            else:
                ep_path = bag_info
                start_time = 0
                end_time = 1

            # moments.json æˆ– metadata.jsonï¼ˆæ–°æ ¼å¼ï¼‰è¦†ç›–æ—¶é—´çª—
            moments_start_time, moments_end_time = get_time_range_from_moments(
                moment_json_dir, metadata_json_path=metadata_json_dir
            )
            if moments_start_time is not None and moments_end_time is not None:
                print(
                    f"[MOMENTS] è¦†ç›–ä½¿ç”¨æ ‡æ³¨æ–‡ä»¶æ—¶é—´èŒƒå›´: {moments_start_time} - {moments_end_time}"
                )
                start_time = moments_start_time
                end_time = moments_end_time

            # bag æ—¶é—´ä¿¡æ¯ï¼ˆç”¨äº metadata åˆå¹¶ï¼‰
            bag_time_info = get_bag_time_info(ep_path)
            if bag_time_info["iso_format"]:
                print(f"Bagå¼€å§‹æ—¶é—´: {bag_time_info['iso_format']}")
                print(f"BagæŒç»­æ—¶é—´: {bag_time_info['duration']:.2f}ç§’")

            # æµå¼ reader
            reader = KuavoRosbagReader(raw_config, use_depth)
            extrinsics_dict = {}
            # é€æ‰¹æ¶ˆè´¹
            batch_id = 0
            _t_prev_batch_end = time.time()  # ç”¨äºè®¡ç®— generator yield è€—æ—¶

            # æå‰è·å–é…ç½®ï¼Œé¿å…å¾ªç¯ä½“å†…æœªå®šä¹‰
            separate_video_storage = getattr(
                raw_config, "separate_video_storage", False
            )
            cam_stats = {}  # åˆå§‹åŒ–ï¼Œé¿å…æ—  batch æ—¶æœªå®šä¹‰

            # é€‰æ‹©ä¸²è¡Œæˆ–å¹¶è¡Œè¯»å–
            use_parallel = getattr(raw_config, "use_parallel_rosbag_read", False)
            num_workers = getattr(raw_config, "parallel_rosbag_workers", 2)

            if use_parallel:
                print(f"[STREAM] å¯ç”¨å¹¶è¡Œ ROSbag è¯»å– ({num_workers} workers)")
                batch_iter = reader.process_rosbag_parallel(
                    str(ep_path),
                    start_time=start_time,
                    end_time=end_time,
                    action_config=None,
                    chunk_size=800,
                    num_workers=num_workers,
                )
            else:
                batch_iter = reader.process_rosbag(
                    str(ep_path),
                    start_time=start_time,
                    end_time=end_time,
                    action_config=None,
                    chunk_size=800,
                )
            for aligned_batch in batch_iter:
                batch_id += 1
                _t_batch_start = time.time()
                _t_rosbag_read = (
                    _t_batch_start - _t_prev_batch_end
                )  # ROSbagè¯»å–+å¯¹é½æ—¶é—´
                main_key = getattr(reader, "MAIN_TIMESTAMP_TOPIC", "camera_top")
                if main_key not in aligned_batch or len(aligned_batch[main_key]) == 0:
                    print(f"[STREAM][WARN] æ‰¹æ¬¡{batch_id} æ— ä¸»æ—¶é—´çº¿ï¼Œè·³è¿‡")
                    continue

                # ä¸»æ—¶é—´æˆ³
                main_ts = np.array(
                    [it["timestamp"] for it in aligned_batch[main_key]],
                    dtype=np.float64,
                )

                first_ts = float(main_ts[0])
                last_ts = float(main_ts[-1])

                # æ¯æ‰¹æå–ç›¸æœºå¤–å‚ï¼ˆæŒ‰æ—¶é—´çª—ï¼‰
                if batch_id == 1:
                    try:
                        extrinsics = reader.extract_and_format_camera_extrinsics(
                            str(ep_path), abs_start=first_ts, abs_end=last_ts
                        )
                        head_extrinsics = extrinsics.get("head_camera_extrinsics", [])
                        left_extrinsics = extrinsics.get(
                            "left_hand_camera_extrinsics", []
                        )
                        right_extrinsics = extrinsics.get(
                            "right_hand_camera_extrinsics", []
                        )
                    except Exception as e:
                        print(f"[WARN] æ‰¹æ¬¡{batch_id} å¤–å‚æå–å¤±è´¥: {e}")
                        head_extrinsics, left_extrinsics, right_extrinsics = [], [], []

                # é¢œè‰²/æ·±åº¦/ç›¸æœºä¿¡æ¯
                _t_extract_start = time.time()
                cameras = raw_config.default_camera_names
                imgs_per_cam = {
                    cam: [x["data"] for x in aligned_batch.get(cam, [])]
                    for cam in cameras
                }
                if use_depth:
                    imgs_per_cam_depth = {
                        cam: [x["data"] for x in aligned_batch.get(f"{cam}_depth", [])]
                        for cam in cameras
                    }
                    compressed = {
                        cam: (
                            aligned_batch.get(f"{cam}_depth", [])[0].get(
                                "compressed", None
                            )
                            if len(aligned_batch.get(f"{cam}_depth", [])) > 0
                            else None
                        )
                        for cam in cameras
                    }
                else:
                    imgs_per_cam_depth = None
                    compressed = None
                info_per_cam = {
                    cam: [
                        np.array(x["data"], dtype=np.float32)
                        for x in aligned_batch.get(f"{cam}_camera_info", [])
                    ]
                    for cam in cameras
                }
                distortion_model = {
                    cam: [
                        x.get("distortion_model", None)
                        for x in aligned_batch.get(f"{cam}_camera_info", [])
                    ]
                    for cam in cameras
                }

                # ç›¸æœºç¿»è½¬ï¼ˆåŸºäº sn_codeï¼‰
                # if sn_code is not None and len(main_ts) > 0:
                #     imgs_per_cam, imgs_per_cam_depth = flip_camera_arrays_if_needed(
                #         imgs_per_cam, imgs_per_cam_depth, sn_code, main_ts[0]
                #     )

                # ä½ç»´æ•°æ®/æœ«ç«¯ä½å§¿
                def get_arr(key, dflt_shape=None):
                    items = aligned_batch.get(key, [])
                    if not items:
                        return None
                    return np.array([x["data"] for x in items], dtype=np.float32)

                # print(get_arr("observation.sensorsData.joint_q").shape)
                state = get_arr(
                    "observation.sensorsData.joint_q"
                )  # or np.zeros((0, 28), dtype=np.float32)
                # sensors_data_raw__joint_v = get_arr("observation.sensorsData.joint_v") #or np.zeros((len(state), 28), dtype=np.float32)
                state_joint_current = get_arr(
                    "observation.sensorsData.joint_current"
                )  # or np.zeros((len(state), 28), dtype=np.float32)
                action = get_arr(
                    "action.joint_cmd.joint_q"
                )  # or np.zeros((0, 28), dtype=np.float32)
                action_kuavo_arm_traj = get_arr(
                    "action.kuavo_arm_traj"
                )  # or np.zeros((0, 14), dtype=np.float32)
                sensors_data_raw__joint_v = get_arr(
                    "observation.sensorsData.joint_v"
                )  # or np.zeros((len(state), 28), dtype=np.float32)
                state_joint_current_arr = get_arr(
                    "observation.sensorsData.joint_current"
                )  # or np.zeros((len(state), 28), dtype=np.float32)
                sensors_data_raw__imu_data = get_arr(
                    "observation.sensorsData.imu"
                )  # or np.zeros((len(state), 13), dtype=np.float32)

                claw_state = get_arr(
                    "observation.claw"
                )  # or np.zeros((len(state), 2), dtype=np.float32)
                claw_action = get_arr(
                    "action.claw"
                )  # or np.zeros((len(state), 2), dtype=np.float32)
                qiangnao_state = get_arr("observation.qiangnao")
                qiangnao_action = get_arr("action.qiangnao")
                hand_state_left = get_arr("observation.qiangnao_left")
                hand_state_right = get_arr("observation.qiangnao_right")
                hand_action_left = get_arr("action.qiangnao_left")
                hand_action_right = get_arr("action.qiangnao_right")
                hand_force_left = get_arr("observation.state.hand_left.force_torque")
                hand_force_right = get_arr("observation.state.hand_right.force_torque")
                hand_touch_left = get_arr("observation.state.hand_left.touch_matrix")
                hand_touch_right = get_arr("observation.state.hand_right.touch_matrix")
                if (
                    (hand_state_left is None or hand_state_right is None)
                    and qiangnao_state is not None
                ):
                    split_left, split_right = _split_dexhand_lr(qiangnao_state)
                    if split_left is not None:
                        hand_state_left = split_left
                    if split_right is not None:
                        hand_state_right = split_right
                if (
                    (hand_action_left is None or hand_action_right is None)
                    and qiangnao_action is not None
                ):
                    split_left, split_right = _split_dexhand_lr(qiangnao_action)
                    if split_left is not None:
                        hand_action_left = split_left
                    if split_right is not None:
                        hand_action_right = split_right

                end_position = get_arr(
                    "end.position"
                )  # or np.zeros((len(state), 6), dtype=np.float32)
                end_orientation = get_arr(
                    "end.orientation"
                )  # or np.zeros((len(state), 8), dtype=np.float32)

                # å¡«å…… action çš„å…³èŠ‚å­æ®µï¼ˆ12:26ï¼‰ä¸º kuavo_arm_traj
                if action.size > 0 and action_kuavo_arm_traj.size > 0:
                    min_rows = min(len(action), len(action_kuavo_arm_traj))
                    action[:min_rows, 12:26] = action_kuavo_arm_traj[:min_rows]

                sensors_data_raw__joint_effort = state_joint_effort = (
                    PostProcessorUtils.current_to_torque_batch(
                        state_joint_current,
                        MOTOR_C2T=[
                            2,
                            1.05,
                            1.05,
                            2,
                            2.1,
                            2.1,
                            2,
                            1.05,
                            1.05,
                            2,
                            2.1,
                            2.1,
                            1.05,
                            5,
                            2.3,
                            5,
                            4.7,
                            4.7,
                            4.7,
                            1.05,
                            5,
                            2.3,
                            5,
                            4.7,
                            4.7,
                            4.7,
                            0.21,
                            4.7,
                        ],
                    )
                )

                sensors_data_raw__joint_current = (
                    PostProcessorUtils.torque_to_current_batch(
                        state_joint_current,
                        MOTOR_C2T=[
                            2,
                            1.05,
                            1.05,
                            2,
                            2.1,
                            2.1,
                            2,
                            1.05,
                            1.05,
                            2,
                            2.1,
                            2.1,
                            1.05,
                            5,
                            2.3,
                            5,
                            4.7,
                            4.7,
                            4.7,
                            1.05,
                            5,
                            2.3,
                            5,
                            4.7,
                            4.7,
                            4.7,
                            0.21,
                            4.7,
                        ],
                    )
                )

                # 4. æå–å­æ•°ç»„å¹¶æ¸…ç†åŸå§‹æ•°ç»„
                head_effort = sensors_data_raw__joint_effort[:, 26:28]
                head_current = sensors_data_raw__joint_current[:, 26:28]
                joint_effort = sensors_data_raw__joint_effort[:, 12:26]
                joint_current = sensors_data_raw__joint_current[:, 12:26]

                # all_low_dim_dataï¼ˆæŒ‰æ‰¹æ¬¡ï¼‰
                main_ts_ns = (main_ts * 1e9).astype(np.int64)
                all_low_dim_data = {
                    "timestamps": main_ts_ns,
                    "action": {
                        "effector": {
                            "position": claw_action,
                            "index": main_ts_ns,
                        },
                        "hand_left": {
                            "position": hand_action_left,
                            "index": main_ts_ns,
                        },
                        "hand_right": {
                            "position": hand_action_right,
                            "index": main_ts_ns,
                        },
                        "arm": {
                            "position": action_kuavo_arm_traj,
                            "index": main_ts_ns,
                        },
                        "head": {
                            "position": action[:, 26:28],
                            "index": main_ts_ns,
                        },
                        "leg": {
                            "position": action[:, :12],
                            "index": main_ts_ns,
                        },
                    },
                    "state": {
                        "effector": {
                            "position": claw_state,
                        },
                        "hand_left": {
                            "position": hand_state_left,
                            "force_torque": hand_force_left,
                            "touch_matrix": hand_touch_left,
                        },
                        "hand_right": {
                            "position": hand_state_right,
                            "force_torque": hand_force_right,
                            "touch_matrix": hand_touch_right,
                        },
                        "head": {
                            "current_value": head_current,
                            "effort": head_effort,
                            "position": state[:, 26:28],
                            "velocity": sensors_data_raw__joint_v[:, 26:28],
                        },
                        "arm": {
                            "current_value": joint_current,
                            "effort": joint_effort,
                            "position": state[:, 12:26],
                            "velocity": sensors_data_raw__joint_v[:, 12:26],
                        },
                        "end": {
                            "position": end_position,
                            "orientation": end_orientation,
                        },
                        "leg": {
                            "current_value": sensors_data_raw__joint_current[:, :12],
                            "effort": sensors_data_raw__joint_effort[:, :12],
                            "position": state[:, 0:12],
                            "velocity": sensors_data_raw__joint_v[:, 0:12],
                        },
                    },
                    "imu": {
                        "gyro_xyz": sensors_data_raw__imu_data[:, 0:3],
                        "acc_xyz": sensors_data_raw__imu_data[:, 3:6],
                        "free_acc_xyz": sensors_data_raw__imu_data[:, 6:9],
                        "quat_xyzw": sensors_data_raw__imu_data[:, 9:13],
                    },
                }
                _t_extract_end = time.time()

                # ä¸ºè¯¥æ‰¹åˆ›å»ºç‹¬ç«‹æ•°æ®é›† root: {base_root}/batch_{id}
                batch_root = os.path.join(base_root, f"batch_{batch_id:04d}")
                # os.makedirs(batch_root, exist_ok=True)
                use_leju_claw_batch = (
                    USE_LEJU_CLAW
                    and claw_state is not None
                    and claw_action is not None
                    and len(claw_state) > 0
                    and len(claw_action) > 0
                )
                use_qiangnao_batch = (
                    USE_QIANGNAO
                    and hand_state_left is not None
                    and hand_state_right is not None
                    and hand_action_left is not None
                    and hand_action_right is not None
                    and len(hand_state_left) > 0
                    and len(hand_state_right) > 0
                    and len(hand_action_left) > 0
                    and len(hand_action_right) > 0
                )
                eef_type = "leju_claw" if use_leju_claw_batch else "dex_hand"
                _t_create_dataset_start = time.time()
                dataset = create_empty_dataset(
                    repo_id=f"lerobot/kuavo",
                    robot_type="kuavo4pro",
                    mode=mode,
                    eef_type=eef_type,
                    dataset_config=dataset_config,
                    has_depth_image=use_depth,
                    root=batch_root,
                    raw_config=raw_config,
                )
                _t_create_dataset_end = time.time()

                # å¸§å†™å…¥ï¼ˆä¸åŸé€»è¾‘ä¸€è‡´ï¼‰
                if batch_id == 1:
                    extrinsics_map = {
                        "camera_top": head_extrinsics,
                        "camera_wrist_left": left_extrinsics,
                        "camera_wrist_right": right_extrinsics,
                        "head_cam_h": head_extrinsics,
                        "wrist_cam_l": left_extrinsics,
                        "wrist_cam_r": right_extrinsics,
                    }
                    extrinsics_dict = {
                        cam: extrinsics_map[cam]
                        for cam in cameras
                        if cam in extrinsics_map
                    }

                num_frames = state.shape[0]
                print(f"[STREAM] æ‰¹æ¬¡{batch_id} å†™å…¥ {num_frames} å¸§")

                _t_frame_loop_start = time.time()
                for i in range(num_frames):
                    # é«˜æ•ˆæ„é€  output_state / output_actionï¼šé¢„åˆ†é… + åˆ‡ç‰‡èµ‹å€¼ï¼Œé¿å…å¤šæ¬¡ concatenate/insert
                    if ONLY_HALF_UP_BODY:
                        if use_leju_claw_batch:
                            if CONTROL_HAND_SIDE in ("left", "both"):
                                l0, l1 = SLICE_ROBOT[0][0], SLICE_ROBOT[0][-1]
                                c0, c1 = SLICE_CLAW[0][0], SLICE_CLAW[0][-1]
                                left_len = (l1 - l0) + (c1 - c0)
                                output_state = np.empty((left_len,), dtype=np.float32)
                                output_action = np.empty((left_len,), dtype=np.float32)
                                output_state[: (l1 - l0)] = state[i, l0:l1]
                                output_state[(l1 - l0) :] = claw_state[i, c0:c1]
                                output_action[: (l1 - l0)] = action[i, l0:l1]
                                output_action[(l1 - l0) :] = claw_action[i, c0:c1]
                            if CONTROL_HAND_SIDE in ("right", "both"):
                                r0, r1 = SLICE_ROBOT[1][0], SLICE_ROBOT[1][-1]
                                rc0, rc1 = SLICE_CLAW[1][0], SLICE_CLAW[1][-1]
                                right_len = (r1 - r0) + (rc1 - rc0)
                                right_state = np.empty((right_len,), dtype=np.float32)
                                right_action = np.empty((right_len,), dtype=np.float32)
                                right_state[: (r1 - r0)] = state[i, r0:r1]
                                right_state[(r1 - r0) :] = claw_state[i, rc0:rc1]
                                right_action[: (r1 - r0)] = action[i, r0:r1]
                                right_action[(r1 - r0) :] = claw_action[i, rc0:rc1]
                                if CONTROL_HAND_SIDE == "both":
                                    # ä»…ä¸€æ¬¡æ‹¼æ¥
                                    output_state = np.concatenate(
                                        (output_state, right_state), axis=0
                                    )
                                    output_action = np.concatenate(
                                        (output_action, right_action), axis=0
                                    )
                                else:
                                    output_state = right_state
                                    output_action = right_action

                        else:
                            if CONTROL_HAND_SIDE in ("left", "both"):
                                l0, l1 = SLICE_ROBOT[0][0], SLICE_ROBOT[0][-1]
                                output_state = np.array(state[i, l0:l1], dtype=np.float32)
                                output_action = np.array(action[i, l0:l1], dtype=np.float32)
                            if CONTROL_HAND_SIDE in ("right", "both"):
                                r0, r1 = SLICE_ROBOT[1][0], SLICE_ROBOT[1][-1]
                                right_state = np.array(state[i, r0:r1], dtype=np.float32)
                                right_action = np.array(action[i, r0:r1], dtype=np.float32)
                                if CONTROL_HAND_SIDE == "both":
                                    output_state = np.concatenate((output_state, right_state), axis=0)
                                    output_action = np.concatenate((output_action, right_action), axis=0)
                                else:
                                    output_state = right_state
                                    output_action = right_action

                    else:
                        if use_leju_claw_batch:
                            # å…¨èº« + çˆªï¼šç›®æ ‡é•¿åº¦ = 28 åŸå…³èŠ‚ + 2 çˆª = 30
                            output_state = np.empty((30,), dtype=np.float32)
                            output_action = np.empty((30,), dtype=np.float32)
                            # 0:19 åŸå§‹
                            output_state[0:19] = state[i, 0:19]
                            output_action[0:19] = action[i, 0:19]
                            # å·¦çˆªæ”¾åœ¨ç´¢å¼•19
                            output_state[19] = float(claw_state[i, 0])
                            output_action[19] = float(claw_action[i, 0])
                            # 20:27 åŸ 19:26
                            output_state[20:27] = state[i, 19:26]
                            output_action[20:27] = action[i, 19:26]
                            # å³çˆªæ”¾åœ¨ç´¢å¼•27
                            output_state[27] = float(claw_state[i, 1])
                            output_action[27] = float(claw_action[i, 1])
                            # 28:30 å¤´éƒ¨
                            output_state[28:30] = state[i, 26:28]
                            output_action[28:30] = action[i, 26:28]

                        else:
                            output_state = np.array(state[i, :], dtype=np.float32)
                            output_action = np.array(action[i, :], dtype=np.float32)

                    if MERGE_HAND_POSITION:
                        left_pos = (
                            hand_state_left[i]
                            if hand_state_left is not None and len(hand_state_left) > i
                            else np.zeros((6,), dtype=np.float32)
                        )
                        right_pos = (
                            hand_state_right[i]
                            if hand_state_right is not None and len(hand_state_right) > i
                            else np.zeros((6,), dtype=np.float32)
                        )
                        left_act = (
                            hand_action_left[i]
                            if hand_action_left is not None and len(hand_action_left) > i
                            else np.zeros((6,), dtype=np.float32)
                        )
                        right_act = (
                            hand_action_right[i]
                            if hand_action_right is not None and len(hand_action_right) > i
                            else np.zeros((6,), dtype=np.float32)
                        )
                        output_state = np.concatenate((output_state, left_pos, right_pos), axis=0)
                        output_action = np.concatenate((output_action, left_act, right_act), axis=0)

                    frame = {
                        "observation.state": torch.from_numpy(output_state).type(
                            torch.float32
                        ),
                        "action": torch.from_numpy(output_action).type(torch.float32),
                        "action.head.position": get_nested_value(
                            all_low_dim_data, "action.head.position", i, [0.0] * 2
                        ),
                        "action.arm.position": get_nested_value(
                            all_low_dim_data, "action.arm.position", i, [0.0] * 14
                        ),
                        "action.leg.position": get_nested_value(
                            all_low_dim_data, "action.leg.position", i, [0.0] * 12
                        ),
                        "observation.state.head.effort": get_nested_value(
                            all_low_dim_data, "state.head.effort", i, [0.0] * 2
                        ),
                        "observation.state.head.position": get_nested_value(
                            all_low_dim_data, "state.head.position", i, [0.0] * 2
                        ),
                        "observation.state.head.velocity": get_nested_value(
                            all_low_dim_data, "state.head.velocity", i, [0.0] * 2
                        ),
                        "observation.state.arm.current_value": get_nested_value(
                            all_low_dim_data, "state.arm.current_value", i, [0.0] * 14
                        ),
                        "observation.state.arm.effort": get_nested_value(
                            all_low_dim_data, "state.arm.effort", i, [0.0] * 14
                        ),
                        "observation.state.arm.position": get_nested_value(
                            all_low_dim_data, "state.arm.position", i, [0.0] * 14
                        ),
                        "observation.state.arm.velocity": get_nested_value(
                            all_low_dim_data, "state.arm.velocity", i, [0.0] * 14
                        ),
                        # å±•å¹³æœ«ç«¯å·¦å³æ‰‹å§¿æ€å’Œä½ç½®
                        "observation.state.end.orientation": (
                            get_nested_value(
                                all_low_dim_data, "state.end.orientation", i, [0.0] * 8
                            )
                        ).flatten(),
                        "observation.state.end.position": (
                            get_nested_value(
                                all_low_dim_data, "state.end.position", i, [0.0] * 6
                            )
                        ).flatten(),
                        "observation.state.leg.current_value": get_nested_value(
                            all_low_dim_data, "state.leg.current_value", i, [0.0] * 12
                        ),
                        "observation.state.leg.effort": get_nested_value(
                            all_low_dim_data, "state.leg.effort", i, [0.0] * 12
                        ),
                        "observation.state.leg.position": get_nested_value(
                            all_low_dim_data, "state.leg.position", i, [0.0] * 12
                        ),
                        "observation.state.leg.velocity": get_nested_value(
                            all_low_dim_data, "state.leg.velocity", i, [0.0] * 12
                        ),
                        "imu.acc_xyz": get_nested_value(
                            all_low_dim_data, "imu.acc_xyz", i, [0.0] * 3
                        ),
                        "imu.gyro_xyz": get_nested_value(
                            all_low_dim_data, "imu.gyro_xyz", i, [0.0] * 3
                        ),
                        "imu.free_acc_xyz": get_nested_value(
                            all_low_dim_data, "imu.free_acc_xyz", i, [0.0] * 3
                        ),
                        "imu.quat_xyzw": get_nested_value(
                            all_low_dim_data, "imu.quat_xyzw", i, [0.0] * 4
                        ),
                    }

                    # æœ«ç«¯ç±»å‹
                    if USE_LEJU_CLAW:
                        frame.update(
                            {
                                "action.effector.position": get_nested_value(
                                    all_low_dim_data,
                                    "action.effector.position",
                                    i,
                                    [0.0] * 2,
                                ),
                                "observation.state.effector.position": get_nested_value(
                                    all_low_dim_data,
                                    "state.effector.position",
                                    i,
                                    [0.0] * 2,
                                ),
                            }
                        )
                    if USE_QIANGNAO:
                        frame.update(
                            {
                                "action.hand_left.position": get_nested_value(
                                    all_low_dim_data,
                                    "action.hand_left.position",
                                    i,
                                    [0.0] * 6,
                                ),
                                "action.hand_right.position": get_nested_value(
                                    all_low_dim_data,
                                    "action.hand_right.position",
                                    i,
                                    [0.0] * 6,
                                ),
                                "observation.state.hand_left.position": get_nested_value(
                                    all_low_dim_data,
                                    "state.hand_left.position",
                                    i,
                                    [0.0] * 6,
                                ),
                                "observation.state.hand_right.position": get_nested_value(
                                    all_low_dim_data,
                                    "state.hand_right.position",
                                    i,
                                    [0.0] * 6,
                                ),
                                "observation.state.hand_left.force_torque": get_nested_value(
                                    all_low_dim_data,
                                    "state.hand_left.force_torque",
                                    i,
                                    [0.0] * 6,
                                ),
                                "observation.state.hand_right.force_torque": get_nested_value(
                                    all_low_dim_data,
                                    "state.hand_right.force_torque",
                                    i,
                                    [0.0] * 6,
                                ),
                                "observation.state.hand_left.touch_matrix": get_nested_value(
                                    all_low_dim_data,
                                    "state.hand_left.touch_matrix",
                                    i,
                                    [0.0] * 360,
                                ),
                                "observation.state.hand_right.touch_matrix": get_nested_value(
                                    all_low_dim_data,
                                    "state.hand_right.touch_matrix",
                                    i,
                                    [0.0] * 360,
                                ),
                            }
                        )

                    # å¤–å‚ï¼ˆè‹¥å¯ç”¨ï¼‰
                    for cam_key, extrs in extrinsics_dict.items():
                        if extrs and len(extrs) > i:
                            rot = np.array(
                                extrs[i]["rotation_matrix"], dtype=np.float32
                            ).reshape(-1)
                            trans = np.array(
                                extrs[i]["translation_vector"], dtype=np.float32
                            ).reshape(-1)
                            frame[
                                f"observation.camera_params.rotation_matrix_flat.{cam_key}"
                            ] = rot
                            frame[
                                f"observation.camera_params.translation_vector.{cam_key}"
                            ] = trans

                    # å½©è‰²å›¾ï¼ˆå¦‚æœè§†é¢‘å•ç‹¬å­˜å‚¨ï¼Œè·³è¿‡å›¾åƒå¤„ç†ï¼‰
                    separate_video_storage = getattr(
                        raw_config, "separate_video_storage", False
                    )

                    if not separate_video_storage:
                        # åŸæœ‰é€»è¾‘ï¼šå›¾åƒç¼–ç åˆ°datasetä¸­
                        for camera, img_list in imgs_per_cam.items():
                            if i < len(img_list):
                                img_bytes = img_list[i]
                                img_np = cv2.imdecode(
                                    np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR
                                )
                                if img_np is None:
                                    raise ValueError(
                                        f"Failed to decode color image for camera {camera} at frame {i}"
                                    )
                                img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)
                                img_np = cv2.resize(
                                    img_np,
                                    (raw_config.resize.width, raw_config.resize.height),
                                )
                                frame[f"observation.images.{camera}"] = img_np

                    dataset.add_frame(frame, task=task)

                    if (
                        i % 800 == 0
                        and hasattr(dataset, "_wait_image_writer")
                        and dataset._wait_image_writer
                    ):
                        if dataset.image_writer.queue.qsize() > 500:
                            dataset._wait_image_writer()
                            gc.collect()

                # ä¿å­˜ä¸€æ‰¹ï¼ˆä½ç»´æ•°æ®ï¼‰
                _t_frame_loop_end = time.time()
                _t_save_episode_start = time.time()
                dataset.save_episode()
                _t_save_episode_end = time.time()

                # æ ¹æ®é…ç½®é€‰æ‹©è§†é¢‘å¤„ç†æ–¹å¼
                separate_video_storage = getattr(
                    raw_config, "separate_video_storage", False
                )

                _t_save_images_start = time.time()
                if separate_video_storage:
                    temp_video_dir = os.path.join(
                        "/tmp", "kuavo_video_temp", episode_uuid
                    )

                    # æµå¼ç¼–ç æ¨¡å¼ï¼šå½©è‰²å¸§ç›´æ¥å–‚å…¥ç¼–ç å™¨ï¼Œæ·±åº¦å¸§ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
                    if streaming_encoder is not None:
                        # å–‚å…¥å½©è‰²å¸§åˆ°æµå¼ç¼–ç å™¨
                        cam_stats = streaming_encoder.feed_batch(imgs_per_cam, batch_id)

                        # æ·±åº¦å¸§ä»éœ€ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•ï¼ˆffmpeg ç¼–ç ï¼‰
                        if imgs_per_cam_depth:
                            save_image_bytes_to_temp(
                                {}, imgs_per_cam_depth, temp_video_dir, batch_id
                            )
                    else:
                        # åŸæœ‰é€»è¾‘ï¼šä¿å­˜å›¾åƒå­—èŠ‚æµåˆ°ç‹¬ç«‹ä¸´æ—¶ç›®å½•
                        cam_stats = save_image_bytes_to_temp(
                            imgs_per_cam, imgs_per_cam_depth, temp_video_dir, batch_id
                        )

                        # å¦‚æœå¯ç”¨æµæ°´çº¿ç¼–ç ï¼Œç«‹å³æäº¤ç¼–ç ä»»åŠ¡
                        if pipeline_encoder is not None:
                            pipeline_encoder.submit_batch(batch_id)

                    _t_save_images_end = time.time()
                    # ç«‹å³é‡Šæ”¾å›¾åƒæ•°æ®å†…å­˜
                    del imgs_per_cam, imgs_per_cam_depth
                    gc.collect()
                    print(f"[MEMORY] æ‰¹æ¬¡{batch_id} å›¾åƒæ•°æ®å·²é‡Šæ”¾")

                else:
                    _t_save_images_end = None  # é separate_video_storage æ¨¡å¼ä¸è®¡æ—¶
                    # åŸæœ‰é€»è¾‘ï¼šæ·±åº¦è§†é¢‘ç¼–ç åˆ°batchç›®å½•
                    depth_dir = os.path.join(batch_root, "depth")
                    os.makedirs(depth_dir, exist_ok=True)
                    compressed_group = {
                        cam: imgs_per_cam_depth[cam]
                        for cam in cameras
                        if compressed.get(cam, None) is True
                    }
                    uncompressed_group = {
                        cam: imgs_per_cam_depth[cam]
                        for cam in cameras
                        if compressed.get(cam, None) is False
                    }

                    if compressed_group:
                        if raw_config.enhance_enabled:
                            save_depth_videos_enhanced_parallel(
                                compressed_group,
                                imgs_per_cam,
                                output_dir=depth_dir,
                                raw_config=raw_config,
                            )
                    if uncompressed_group:
                        save_depth_videos_16U_parallel(
                            uncompressed_group,
                            output_dir=depth_dir,
                            raw_config=raw_config,
                        )
                    move_and_rename_depth_videos(depth_dir, episode_idx=0)

                # ä¿å­˜å‚æ•°ï¼ˆcamera info ä¸ extrinsicsï¼‰
                if batch_id == 1:
                    parameters_dir = os.path.join(batch_root, "parameters")
                    os.makedirs(parameters_dir, exist_ok=True)
                    save_camera_info_to_json_new(
                        info_per_cam, distortion_model, output_dir=parameters_dir
                    )
                    save_camera_extrinsic_params(
                        cameras=cameras, output_dir=parameters_dir
                    )

                # ä¿å­˜ metadata.jsonï¼ˆæŒ‰æ‰¹æ¬¡ï¼‰
                try:
                    if metadata_json_DIR is not None and os.path.exists(metadata_json_DIR):
                        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ ¼å¼ï¼ˆåŒ…å« marks å­—æ®µï¼‰
                        with open(metadata_json_DIR, "r", encoding="utf-8") as f:
                            test_metadata = json.load(f)
                        is_new_format = "marks" in test_metadata and isinstance(test_metadata.get("marks"), list)
                        
                        # æ–°æ ¼å¼ä¸éœ€è¦ moment_json_DIRï¼Œæ—§æ ¼å¼éœ€è¦
                        if is_new_format:
                            # æ–°æ ¼å¼ï¼šåªéœ€è¦ metadata.json
                            merge_metadata_and_moment(
                                metadata_json_DIR,
                                None,  # moment_path åœ¨æ–°æ ¼å¼ä¸‹ä¸º None
                                os.path.join(batch_root, "metadata.json"),
                                episode_uuid,
                                raw_config,
                                bag_time_info=bag_time_info,
                                main_time_line_timestamps=main_ts,  # ç§’
                            )
                        elif moment_json_DIR is not None and os.path.exists(moment_json_DIR):
                            # æ—§æ ¼å¼ï¼šéœ€è¦ metadata.json + moments.json
                            merge_metadata_and_moment(
                                metadata_json_DIR,
                                moment_json_DIR,
                                os.path.join(batch_root, "metadata.json"),
                                episode_uuid,
                                raw_config,
                                bag_time_info=bag_time_info,
                                main_time_line_timestamps=main_ts,  # ç§’
                            )
                        else:
                            print(
                                f"[WARN] æ—§æ ¼å¼éœ€è¦ moments.jsonï¼Œä½†æœªæ‰¾åˆ°: moment_json_DIR={moment_json_DIR}"
                            )
                    else:
                        print(
                            f"[WARN] æœªç”Ÿæˆæ‰¹æ¬¡ metadata.jsonï¼Œmetadata_json_DIR={metadata_json_DIR}"
                        )
                except Exception as e:
                    print(f"[ERROR] åˆå¹¶ metadata å’Œ moment å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

                # é‡Šæ”¾æ‰¹æ¬¡å†…å­˜
                del dataset, info_per_cam, distortion_model

                # å¦‚æœæ²¡æœ‰åœ¨å‰é¢åˆ é™¤ï¼Œè¿™é‡Œåˆ é™¤å›¾åƒæ•°æ®
                if not separate_video_storage:
                    if "imgs_per_cam" in locals():
                        del imgs_per_cam
                    if "imgs_per_cam_depth" in locals():
                        del imgs_per_cam_depth

                del (
                    state,
                    action,
                    action_kuavo_arm_traj,
                    sensors_data_raw__joint_v,
                    state_joint_current_arr,
                    sensors_data_raw__imu_data,
                )
                del claw_state, claw_action, qiangnao_state, qiangnao_action
                del (
                    end_position,
                    end_orientation,
                    all_low_dim_data,
                )
                if batch_id == 1:
                    del head_extrinsics, left_extrinsics, right_extrinsics
                gc.collect()

                # ===== è®¡æ—¶æ±‡æ€» =====
                _t_batch_end = time.time()
                _t_total = _t_batch_end - _t_batch_start
                _t_extract = _t_extract_end - _t_extract_start
                _t_create = _t_create_dataset_end - _t_create_dataset_start
                _t_frames = _t_frame_loop_end - _t_frame_loop_start
                _t_save_ep = _t_save_episode_end - _t_save_episode_start
                _t_save_img = (
                    (_t_save_images_end - _t_save_images_start)
                    if _t_save_images_end
                    else 0
                )
                print(
                    f"[TIMING] Batch {batch_id}: ROSbagè¯»å–={_t_rosbag_read:.2f}s | "
                    f"æ•°æ®æå–={_t_extract:.2f}s | Datasetåˆ›å»º={_t_create:.2f}s | "
                    f"å¸§å¾ªç¯={_t_frames:.2f}s | Parquetä¿å­˜={_t_save_ep:.2f}s | "
                    f"å›¾åƒä¿å­˜={_t_save_img:.2f}s | æ‰¹æ¬¡æ€»è®¡={_t_total:.2f}s"
                )
                _t_prev_batch_end = time.time()  # æ›´æ–°ä¸ºä¸‹ä¸€æ‰¹å‡†å¤‡

        if separate_video_storage:
            return cam_stats
        else:
            return None

    # å¦‚æœå¯ç”¨æµæ°´çº¿ç¼–ç ï¼Œåˆ›å»ºç¼–ç å™¨
    # ç¯å¢ƒå˜é‡ä¼˜å…ˆäºé…ç½®æ–‡ä»¶
    pipeline_encoder = None
    env_pipeline = os.environ.get("USE_PIPELINE_ENCODING", "").lower()
    if env_pipeline in ("true", "1", "yes"):
        use_pipeline_encoding = True
        print("[CONFIG] æµæ°´çº¿ç¼–ç å·²é€šè¿‡ç¯å¢ƒå˜é‡å¯ç”¨ (USE_PIPELINE_ENCODING=true)")
    elif env_pipeline in ("false", "0", "no"):
        use_pipeline_encoding = False
    else:
        use_pipeline_encoding = getattr(raw_config, "use_pipeline_encoding", False)

    if use_pipeline_encoding and getattr(raw_config, "separate_video_storage", False):
        temp_video_dir = os.path.join("/tmp", "kuavo_video_temp", episode_uuid)
        segment_dir = os.path.join("/tmp", "kuavo_video_segments", episode_uuid)
        video_output_dir = os.path.join(base_root, episode_uuid, episode_uuid)

        pipeline_encoder = BatchSegmentEncoder(
            temp_base_dir=temp_video_dir,
            segment_base_dir=segment_dir,
            video_output_dir=video_output_dir,
            cameras=raw_config.default_camera_names,
            train_hz=raw_config.train_hz,
            uuid_str=episode_uuid,
            chunk_size=800,  # å›ºå®šæ‰¹æ¬¡å¤§å°
            max_workers=3,  # 3ä¸ªç›¸æœºï¼Œ3ä¸ªå·¥ä½œçº¿ç¨‹
        )

    # å¦‚æœå¯ç”¨æµå¼ç¼–ç ï¼Œåˆ›å»ºç¼–ç å™¨ï¼ˆä¼˜å…ˆçº§é«˜äº pipeline_encoderï¼‰
    streaming_encoder = None
    env_streaming = os.environ.get("USE_STREAMING_VIDEO", "").lower()
    if env_streaming in ("true", "1", "yes"):
        use_streaming_video = True
        print("[CONFIG] æµå¼è§†é¢‘ç¼–ç å·²é€šè¿‡ç¯å¢ƒå˜é‡å¯ç”¨ (USE_STREAMING_VIDEO=true)")
    elif env_streaming in ("false", "0", "no"):
        use_streaming_video = False
    else:
        use_streaming_video = getattr(raw_config, "use_streaming_video", False)

    if use_streaming_video and getattr(raw_config, "separate_video_storage", False):
        # æµå¼ç¼–ç ä¸æµæ°´çº¿ç¼–ç äº’æ–¥ï¼Œæµå¼ç¼–ç ä¼˜å…ˆ
        if pipeline_encoder is not None:
            print("[CONFIG] æµå¼ç¼–ç ä¸æµæ°´çº¿ç¼–ç äº’æ–¥ï¼Œä¼˜å…ˆä½¿ç”¨æµå¼ç¼–ç ")
            pipeline_encoder = None

        video_output_dir = os.path.join(base_root, episode_uuid, episode_uuid)
        queue_limit = int(
            os.environ.get(
                "VIDEO_QUEUE_LIMIT", getattr(raw_config, "video_queue_limit", 100)
            )
        )

        streaming_encoder = StreamingVideoEncoderManager(
            cameras=raw_config.default_camera_names,
            video_output_dir=video_output_dir,
            uuid_str=episode_uuid,
            train_hz=raw_config.train_hz,
            queue_limit=queue_limit,
        )

    # æ‰§è¡Œæµå¼å¡«å……ï¼ˆå¿«é€Ÿç”Ÿæˆlerobotæ•°æ®ï¼‰
    cam_stats = populate_dataset_stream(
        raw_config=raw_config,
        bag_files=processed_files,
        task=task,
        moment_json_dir=moment_json_DIR,
        base_root=base_root,
        metadata_json_dir=metadata_json_DIR,
        pipeline_encoder=pipeline_encoder,
        streaming_encoder=streaming_encoder,
    )

    print("[INFO] ========== ä¸»æ•°æ®å¤„ç†å®Œæˆ ==========")
    print(f"[INFO] LeRobotæ•°æ®å·²ä¿å­˜åˆ°: {base_root}")

    # ===== ä¼˜åŒ–: æå‰å¯åŠ¨è§†é¢‘ç¼–ç ï¼Œä¸åˆå¹¶å¹¶è¡Œ =====
    base_path = Path(base_root).resolve()
    output_dir = base_path / episode_uuid / episode_uuid
    encoding_thread = None

    if getattr(raw_config, "separate_video_storage", False):
        temp_video_dir = os.path.join("/tmp", "kuavo_video_temp", episode_uuid)
        video_output_dir = output_dir
        async_encoding = getattr(raw_config, "async_video_encoding", False)

        # æµå¼/æµæ°´çº¿ç¼–ç å™¨ç‰¹æ®Šå¤„ç†ï¼ˆå®ƒä»¬éœ€è¦åœ¨åˆå¹¶åfinalizeï¼‰
        if streaming_encoder is None and pipeline_encoder is None and async_encoding:
            # åŸæœ‰å¼‚æ­¥ç¼–ç : æå‰å¯åŠ¨ï¼Œä¸åˆå¹¶å¹¶è¡Œ
            import threading

            print("[VIDEO] ========== æå‰å¯åŠ¨è§†é¢‘ç¼–ç ï¼ˆä¸åˆå¹¶å¹¶è¡Œï¼‰==========")

            def async_encode():
                try:
                    encode_complete_videos_from_temp(
                        temp_video_dir,
                        video_output_dir,
                        episode_uuid,
                        raw_config,
                        use_depth=use_depth,
                    )
                except Exception as e:
                    print(f"[VIDEO] å¼‚æ­¥ç¼–ç å‡ºé”™: {e}")
                    import traceback

                    traceback.print_exc()

            encoding_thread = threading.Thread(target=async_encode, daemon=False)
            encoding_thread.start()
            print("[VIDEO] è§†é¢‘ç¼–ç å·²åœ¨åå°å¯åŠ¨")
            print(f"[VIDEO] è§†é¢‘å°†ä¿å­˜åˆ°: {video_output_dir}")

    # ===== åˆå¹¶æ‰¹æ¬¡æ•°æ®ï¼ˆä¸è§†é¢‘ç¼–ç å¹¶è¡Œï¼‰=====
    _t_merge_start = time.time()
    print("[INFO] å¼€å§‹åˆå¹¶æ‰¹æ¬¡æ•°æ®...")
    batch_dirs = get_batch_dirs(base_path)
    total_frames = merge_parquet_files(batch_dirs, output_dir)

    # å…ˆåˆå¹¶ç”Ÿæˆå…¨å±€ metadata.jsonï¼ˆä½¿ç”¨å„ batch çš„ metadata.jsonï¼‰
    try:
        merge_metadata(batch_dirs, output_dir, total_frames)
    except Exception as e:
        print(f"[WARN] åˆå¹¶ metadata.json å¤±è´¥: {e}")

    # å†åˆå¹¶ episodes.jsonl / info.json / tasks.jsonl / episodes_stats.jsonl ç­‰ meta æ–‡ä»¶
    # ä¼ å…¥çœŸå®ä¿å­˜çš„è§†é¢‘é«˜å®½ï¼Œç”¨äº info.json ä¸­ç›¸æœº shape
    video_h = None
    video_w = None
    if getattr(raw_config, "resize", None) is not None:
        video_h = getattr(raw_config.resize, "height", 480)
        video_w = getattr(raw_config.resize, "width", 848)
    merge_meta_files(
        batch_dirs, output_dir, total_frames, cam_stats,
        video_height=video_h, video_width=video_w,
    )
    _t_merge_end = time.time()
    print(f"[INFO] æ‰¹æ¬¡æ•°æ®åˆå¹¶å®Œæˆã€‚è€—æ—¶: {_t_merge_end - _t_merge_start:.2f}s")

    # åˆå¹¶ååˆ é™¤æ‰€æœ‰ batch æ–‡ä»¶å¤¹
    for d in base_path.iterdir():
        if d.is_dir() and d.name.startswith("batch_"):
            try:
                shutil.rmtree(d)
                print(f"[INFO] å·²åˆ é™¤æ‰¹æ¬¡æ–‡ä»¶å¤¹: {d}")
            except Exception as e:
                print(f"[WARN] åˆ é™¤æ‰¹æ¬¡æ–‡ä»¶å¤¹å¤±è´¥: {d}, é”™è¯¯: {e}")

    # ===== è§†é¢‘ç¼–ç åç»­å¤„ç† =====
    if getattr(raw_config, "separate_video_storage", False):
        temp_video_dir = os.path.join("/tmp", "kuavo_video_temp", episode_uuid)
        video_output_dir = output_dir

        if streaming_encoder is not None:
            # æµå¼ç¼–ç æ¨¡å¼ï¼šå½©è‰²è§†é¢‘å·²åœ¨æ‰¹å¤„ç†ä¸­ç¼–ç å®Œæˆï¼Œåªéœ€ finalize
            print("[VIDEO] ========== æµå¼ç¼–ç æ¨¡å¼ ==========")
            streaming_encoder.finalize()
            print(f"[VIDEO] å½©è‰²è§†é¢‘å·²ä¿å­˜åˆ°: {video_output_dir}")

            # æ·±åº¦è§†é¢‘å•ç‹¬å¤„ç†ï¼ˆä»ç„¶ä½¿ç”¨ ffmpegï¼‰
            if use_depth:
                depth_temp_dir = os.path.join(temp_video_dir, "depth")
                if os.path.exists(depth_temp_dir):
                    print("[VIDEO] å¼€å§‹ç¼–ç æ·±åº¦è§†é¢‘...")
                    depth_out_dir = os.path.join(video_output_dir, "depth", "chunk-000")
                    os.makedirs(depth_out_dir, exist_ok=True)
                    apply_denoise = False  # ä¿æŒåŸé€»è¾‘
                    depth_procs = []
                    for camera in os.listdir(depth_temp_dir):
                        camera_dir = os.path.join(depth_temp_dir, camera)
                        if not os.path.isdir(camera_dir):
                            continue
                        video_path = os.path.join(depth_out_dir, f"{camera}.mkv")
                        p = multiprocessing.Process(
                            target=_encode_depth_camera_worker,
                            args=(
                                camera_dir,
                                camera,
                                video_path,
                                raw_config.train_hz,
                                apply_denoise,
                            ),
                            daemon=False,
                        )
                        p.start()
                        depth_procs.append(p)
                    for p in depth_procs:
                        p.join()
                    print("[VIDEO] æ·±åº¦è§†é¢‘ç¼–ç å®Œæˆ")
                    # æ¸…ç†æ·±åº¦ä¸´æ—¶ç›®å½•
                    shutil.rmtree(depth_temp_dir, ignore_errors=True)

        elif pipeline_encoder is not None:
            # æµæ°´çº¿æ¨¡å¼ï¼šç­‰å¾…ç¼–ç å®Œæˆå¹¶æ‹¼æ¥
            print("[VIDEO] ========== æµæ°´çº¿ç¼–ç æ¨¡å¼ ==========")
            pipeline_encoder.finalize(use_depth=use_depth)
            print(f"[VIDEO] æ‰€æœ‰è§†é¢‘å·²ä¿å­˜åˆ°: {video_output_dir}")

        elif encoding_thread is not None:
            # å¼‚æ­¥ç¼–ç å·²æå‰å¯åŠ¨ï¼Œåªéœ€è¾“å‡ºçŠ¶æ€
            print("[INFO] ä¸»æµç¨‹å·²å®Œæˆï¼Œè§†é¢‘ç¼–ç åœ¨åå°ç»§ç»­...")

        else:
            # åŒæ­¥ç¼–ç ï¼ˆç­‰å¾…å®Œæˆï¼‰
            async_encoding = getattr(raw_config, "async_video_encoding", False)
            if not async_encoding:
                print("[VIDEO] å¼€å§‹åŒæ­¥ç¼–ç è§†é¢‘...")
                encode_complete_videos_from_temp(
                    temp_video_dir,
                    video_output_dir,
                    episode_uuid,
                    raw_config,
                    use_depth=use_depth,
                )
                print(f"[VIDEO] æ‰€æœ‰è§†é¢‘å·²ä¿å­˜åˆ°: {video_output_dir}")


if __name__ == "__main__":
    import argparse
    import json
    import time

    start = time.time()
    parser = argparse.ArgumentParser(description="Kuavo ROSbag to Lerobot Converter")
    parser.add_argument(
        "--bag_dir",
        default="/home/leju_kuavo/tmp/123/",
        type=str,
        required=False,
        help="Path to ROS bag",
    )
    # parser.add_argument("--bag_dir", default = "./testbag/task24_519_20250519_193043_0.bag", type=str, required=False, help="Path to ROS bag")
    parser.add_argument(
        "--moment_json_dir", type=str, required=False, help="Path to moment.json"
    )
    parser.add_argument(
        "--metadata_json_dir", type=str, required=False, help="Path to metadata.json"
    )
    parser.add_argument(
        "--output_dir",
        default="testoutput/",
        type=str,
        required=False,
        help="Path to output",
    )
    parser.add_argument(
        "--train_frequency",
        type=int,
        help="Training frequency (Hz), overrides config file setting",
    )

    parser.add_argument(
        "--only_arm",
        type=str,
        choices=["true", "false"],
        help="Use only arm data (true/false), overrides config file setting",
    )

    parser.add_argument(
        "--which_arm",
        type=str,
        choices=["left", "right", "both"],
        help="Which arm to use (left/right/both), overrides config file setting",
    )

    parser.add_argument(
        "--dex_dof_needed",
        type=int,
        help="Degrees of freedom needed for dexterous hand, overrides config file setting",
    )
    parser.add_argument(
        "--config",
        type=str,
        default="./kuavo/request.json",
        help="Path to config YAML file",
    )
    parser.add_argument(
        "--use_depth",
        action="store_true",
        help="å¦‚æœæŒ‡å®šï¼Œå¿½ç•¥æ‰€æœ‰ä¸ metadata.json / moments.json ç›¸å…³çš„è¾“å…¥ä¸è¾“å‡ºï¼ˆä¸è¯»å–ä¹Ÿä¸å†™å…¥ï¼‰",
    )
    args = parser.parse_args()

    # åŠ è½½é…ç½®æ–‡ä»¶
    config = load_config_from_json(args.config)
    # ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
    if args.train_frequency is not None:
        config.train_hz = args.train_frequency
        print(f"âœ… è¦†ç›–é…ç½®: train_hz = {args.train_frequency}")

    if args.only_arm is not None:
        config.only_arm = args.only_arm.lower() == "true"
        print(f"âœ… è¦†ç›–é…ç½®: only_arm = {config.only_arm}")

    if args.which_arm is not None:
        config.which_arm = args.which_arm
        print(f"âœ… è¦†ç›–é…ç½®: which_arm = {args.which_arm}")

    if args.dex_dof_needed is not None:
        config.dex_dof_needed = args.dex_dof_needed
        print(f"âœ… è¦†ç›–é…ç½®: dex_dof_needed = {args.dex_dof_needed}")
    # ä»é…ç½®è·å–å‚æ•°

    if args.bag_dir is not None:
        bag_DIR = args.bag_dir
    print(f"Bag directory: {bag_DIR}")
    moment_json_DIR = None
    metadata_json_DIR = None
    if args.moment_json_dir is not None:
        moment_json_DIR = args.moment_json_dir
    else:
        moment_json_DIR = os.path.join(bag_DIR, "moments.json")

    if args.metadata_json_dir is not None:
        metadata_json_DIR = args.metadata_json_dir
    else:
        metadata_json_DIR = os.path.join(bag_DIR, "metadata.json")
    if args.output_dir is not None:
        output_DIR = args.output_dir

    ID = config.id
    use_depth = args.use_depth
    bag_files = list_bag_files_auto(bag_DIR)
    port_kuavo_rosbag(
        raw_config=config,
        processed_files=bag_files,
        moment_json_DIR=moment_json_DIR,
        metadata_json_DIR=metadata_json_DIR,
        lerobot_dir=output_DIR,
        use_depth=use_depth,
    )
    end = time.time()
    print(f"[INFO] æ€»ç”¨æ—¶: {end - start:.2f} ç§’")
